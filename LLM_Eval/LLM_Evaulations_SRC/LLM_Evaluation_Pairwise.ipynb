{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267fab21-d152-4ab1-8769-be85bfe0d7b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM Evaluation \n",
    "\n",
    "This code uses llm as a judge to compare the generated content by two different llm models\n",
    "    \n",
    "<b> llm as a judge using user provided metrics:<br>\n",
    "- multimodal content coverage comparisions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830715c-0f3d-4cc1-b184-9f2512f71c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ab6b2b-8c59-4311-90df-5158cfaabd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import time\n",
    "import random\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "#from LLM_PairWiseEval_cls import PairwiseEvaluationClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c48cff-f90e-49bf-9b53-168c4d2553c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Data Sample for Multimodal Coverage Comparisions\n",
    "The assumption is that the generated content is in the form of json including the fields that are requested from llm models to be extracted from the content.<br>\n",
    "Because we did not have data in our environment, we make some sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e00a83-6ab8-43b1-9771-9b3c374cc77b",
   "metadata": {},
   "source": [
    "# Sample User Prompt\n",
    "This is basically the prompt text that will be used to generate the content for each video segment or image during batch/online content generation.\n",
    "Here, we used this prompt to generate the content of a sample video from 600s to 900s using two different models 'gemini-1.5-pro-002', 'gemini-1.5-flash-002'. The generated content is recorded in json format in output_model1.txt and output_model2.txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef82971-48ea-496b-ad1e-442c7aa88541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start=600\n",
    "end=900\n",
    "schema=\"\"\"{\n",
    "    \"description\": \"A structured schema to represent detailed information from a video or text analysis\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Category\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The category or general type of the content\"\n",
    "        },\n",
    "        \"DetailedDescriptionOfEventsAndConversations\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A detailed textual description of the events and conversations in the content\"\n",
    "        },\n",
    "        \"BrandsCompanyNamesLogos\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of brands, company names, or logos appearing or mentioned in the content\"\n",
    "        },\n",
    "        \"KeyLocationsAndScenes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of key locations and scenes appearing or mentioned in the content\"\n",
    "        },\n",
    "        \"KeyThemes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of key themes discussed or portrayed in the content\"\n",
    "        },\n",
    "        \"PeopleAppearingAndMentioned\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of people who appear or are mentioned in the content\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"Category\",\n",
    "        \"DetailedDescriptionOfEventsAndConversations\",\n",
    "        \"BrandsCompanyNamesLogos\",\n",
    "        \"KeyLocationsAndScenes\",\n",
    "        \"KeyThemes\",\n",
    "        \"PeopleAppearingAndMentioned\"\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "VAR_VIDEO_SEGMENT=f\"Your task is to provide a comprehensive description of this video from segment {start} seconds to {end} seconds.\\n\"\n",
    "VAR_INSTRUCTIONS= \"\"\"To complete the task you need to follow these steps:\\n\n",
    "                           No greetings, closing remarks, or additional comments. Begin immediately with the video analysis and provide only the requested information in the specified format.\\n\n",
    "                           Idenify all instances of visual product placement. Pay close attention to background details and items held by the characters. List each product placement with the following\n",
    "                            information: Brand name, product name (if applicable), and a brief description. Include information about product placement into the description generated for the video\\n\n",
    "                           Create a transcript of all the speeches, dialogs, narration.\\n\n",
    "                           Scrupulously examine each scene for any and all visible brand names, logos, and products. Even if a product appears briefly or in the background, it should be included.\\n\"\"\"\n",
    "\n",
    "VAR_CONSTRAINTS= \"\"\"Describe the video content objectively, avoiding any subjective opinions or assumptions.\\n\n",
    "                           Specify who is saying what. If a person talking can be seen, specify their name and/or occupation. If it is voice behind the scenes, then describe it as a narrator.\\n\n",
    "                           Be specific when describing. Include all the information that is shown or given.\\n\n",
    "                           Do not show timestamps.\\n\n",
    "                           If an unidentified person is shown in the video first, but then their name is mentioned later in the video, make sure to mention their name in the description from the start.\\n\n",
    "                           \"\"\"\n",
    "\n",
    "VAR_STRUCTURE= f\"\"\"Organize the description with the following properties, and give a valid json file with JSON schema.<JSONSchema>{json.dumps(schema)}</JSONSchema>:\n",
    "                       \\n**Category**\\n\n",
    "                       \\n**DetailedDescriptionOfEventsAndConversations**\\n\n",
    "                       \\n**BrandsCompanyNamesLogos**\\n\n",
    "                       \\n**KeyLocationsAndScenes**\\n\n",
    "                       \\n**KeyThemes**\\n\n",
    "                       \\n**PeopleAppearingAndMentioned**\\n \n",
    "                 \"\"\" \n",
    "\n",
    "VAR_CONDITIONS = \"\"\"Identify a video as one of these categories: News, TV Shows, Live Sport Events, News Analyses. \\n\n",
    "                       When describing the DetailedDescriptionOfEventsAndConversations, consider the following instructions for specific video types:\\n\n",
    "                       * **News:** Pay close attention to transitions, graphics, and on-screen text.\\n\n",
    "                       * **TV Shows:** Describe facial expressions, body language, appearances, and overall mood.\\n\n",
    "                       * **Live Sports Events:** Focus on key moments, like goals or fouls, and describe the overall flow and momentum of the game.\\n\n",
    "                       * **News Analyses:** Identify different perspectives, arguments, and supporting evidence.\\n\n",
    "                       Make sure to mention people's names in the DetailedDescriptionOfEventsAndConversations and in PeopleAppearingAndMentioned as well as any other information about them like their age, occupation, location, etc. \\n\"\"\"\n",
    "\n",
    "VAR_EXAMPLE = \"\"\"Follow this example for the format of the output:\\n\n",
    "              {\n",
    "                \"Category\": \"TV Show\",\n",
    "                \"DetailedDescriptionOfEventsAndConversations\": \"The video starts with a man sitting at a dining table, reading a letter. Two Fiji bottles are visible on the benchtop. He has short, light brown hair and a beard. His name is Harrison. The scene changes to Melissa. Melissa says: \\\"I'm Melissa, and I'm a hairdresser. I'm 41 years old, and I'm from Sydney.\\\"\",\n",
    "                \"BrandsCompanyNamesLogos\": [\"Lacoste\", \"Fiji\"],\n",
    "                \"KeyLocationsAndScenes\": [\"Apartment\"],\n",
    "                \"KeyThemes\": [\"Marriage\"],\n",
    "                \"PeopleAppearingAndMentioned\": [\n",
    "                \"Harrison, 32, Builder, NSW\",\n",
    "                \"Melissa, 41, Hairdresser, NSW\"\n",
    "                ]\n",
    "            }\n",
    "               \"\"\"\n",
    "  \n",
    "    \n",
    "video_description_prompt=VAR_VIDEO_SEGMENT+VAR_INSTRUCTIONS+VAR_CONSTRAINTS+VAR_STRUCTURE+VAR_CONDITIONS+VAR_EXAMPLE\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00eb83a4-070a-48f2-947a-379fc1ca24fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load pre-executed predictions\n",
    "with open('output_model1.txt', 'r') as file:\n",
    "    response1 = json.dumps(json.load(file))\n",
    "with open('output_model2.txt', 'r') as file:\n",
    "    response2 = json.dumps(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1111a86d-aa54-4bde-9912-7a6ee1d0b8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"content-generation-qa-quality\"\n",
    "file_path = 'PairWiseMultimodalContentEvaluationMetrics.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    multimodal_eval_prompt_metrics = json.load(file)\n",
    "\n",
    "    \n",
    "video_multimodal_content_evaluation_metric_promopt =multimodal_eval_prompt_metrics['video_multimodal_content_evaluation_metric_promopt']\n",
    "image_multimodal_content_evaluation_metric_promopt= multimodal_eval_prompt_metrics['image_multimodal_content_evaluation_metric_promopt']\n",
    "\n",
    "multimodal_evaluation_promt={'video_prompt': video_multimodal_content_evaluation_metric_promopt,'image_prompt':image_multimodal_content_evaluation_metric_promopt}\n",
    "\n",
    "\n",
    "generated_response = [\n",
    "    response1,\n",
    "    response2\n",
    "]\n",
    "llm_models=['gemini-1.5-pro-002', 'gemini-1.5-flash-002']\n",
    "\n",
    "items= pd.DataFrame(\n",
    "    {   'asset_id':\"MAAT2024_1_A_HBB.mp4\",\n",
    "        \"prompt_text_A\": video_description_prompt, #this should be set with the prompt_text when doing batch generation for model -A\n",
    "        \"prompt_text_B\": video_description_prompt, #this should be set with the prompt_text when doing batch generation for model -B\n",
    "        \"fileUri\":'gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4' , #this should be set to file uri when doing batch generation\n",
    "        \"description_A\": response1, #this should be set the generated content when doing batch generation for model-A\n",
    "        \"description_B\": response1, #this should be set the generated content when doing batch generation for model-B\n",
    "     \n",
    "        \"asset_type\": 'video/mp4', #this should be set to asset_type/mime_type when doing batch generation\n",
    "        \"startOffset_seconds\":[int(start)],\n",
    "        \"endOffset_seconds\":[int(end)],\n",
    "        \"modelVersion_A\":llm_models[0],#this should be set the model-A name\n",
    "        \"modelVersion_B\":llm_models[1],#this should be set the model-B name\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117dae90-26c7-4ed1-9424-463535f7d380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_id</th>\n",
       "      <th>prompt_text_A</th>\n",
       "      <th>prompt_text_B</th>\n",
       "      <th>fileUri</th>\n",
       "      <th>description_A</th>\n",
       "      <th>description_B</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>startOffset_seconds</th>\n",
       "      <th>endOffset_seconds</th>\n",
       "      <th>modelVersion_A</th>\n",
       "      <th>modelVersion_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAAT2024_1_A_HBB.mp4</td>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>gs://raw_nine_files/vlt_video_extract/MAAT/MAA...</td>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>video/mp4</td>\n",
       "      <td>600</td>\n",
       "      <td>900</td>\n",
       "      <td>gemini-1.5-pro-002</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               asset_id                                      prompt_text_A  \\\n",
       "0  MAAT2024_1_A_HBB.mp4  Your task is to provide a comprehensive descri...   \n",
       "\n",
       "                                       prompt_text_B  \\\n",
       "0  Your task is to provide a comprehensive descri...   \n",
       "\n",
       "                                             fileUri  \\\n",
       "0  gs://raw_nine_files/vlt_video_extract/MAAT/MAA...   \n",
       "\n",
       "                                       description_A  \\\n",
       "0  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...   \n",
       "\n",
       "                                       description_B asset_type  \\\n",
       "0  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...  video/mp4   \n",
       "\n",
       "   startOffset_seconds  endOffset_seconds      modelVersion_A  \\\n",
       "0                  600                900  gemini-1.5-pro-002   \n",
       "\n",
       "         modelVersion_B  \n",
       "0  gemini-1.5-flash-002  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a333e5f4-5ba0-4f61-9127-6aeaf73463e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "import uuid \n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "from datetime import datetime\n",
    "\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part as GenerativeModelPart,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "\n",
    "class PairwiseEvaluationClient:\n",
    "    \"\"\"Wrapper around Pairwise Evaluation Client.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project: str=None,\n",
    "        location: str = \"us-central1\",\n",
    "        items: pd.core.frame.DataFrame = None,\n",
    "        response_A_desc_column_name: str= 'description_A',\n",
    "        response_B_desc_column_name: str= 'description_B',\n",
    "        response_A_llm_model_column_name: str= None,\n",
    "        response_B_llm_model_column_name: str=None,\n",
    "        response_mediaType_column_name: str=None,\n",
    "        response_media_column_metadata : dict=None,\n",
    "        response_A_userPrompt_column_name: str=None,\n",
    "        response_B_userPrompt_column_name: str=None,\n",
    "        multimodal_evaluation_promt: dict=None,       \n",
    "        experiment_name: str=\"pairwise-evaluation-experiment\",\n",
    "       \n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initis the hyper parameters\n",
    "        \n",
    "        Args:\n",
    "         str project:  project id \n",
    "         str locations: project location         \n",
    "         Dataframe items: dataframe of AI-generated responses\n",
    "         str response_A_desc_column_name: the name of the column in the 'items' dataframe that includes the AI-generated response for model A\n",
    "         str response_B_desc_column_name: the name of the column in the 'items' dataframe that includes the AI-generated response for model B\n",
    "         str response_A_llm_model_column_name: the name of the column in the 'items' dataframe that includes the model A's name that is used for extracting AI-generated responses A\n",
    "         str response_B_llm_model_column_name: the name of the column in the 'items' dataframe that includes the model B's name that is used for extracting AI-generated responses B\n",
    "         str response_mediaType_column_name:  the name of the column in the 'items' dataframe that represent media type\n",
    "         str response_A_userPrompt_column_name: the name of the column in the 'items' dataframe that represent user prompt for model A using which the AI model generated the response A\n",
    "         str response_A_userPrompt_column_name: the name of the column in the 'items' dataframe that represent user prompt for model B using which the AI model generated the response B\n",
    "         dict response_media_column_metadata: dictionary including the name of fileuri, start and endoffset of the media if available\n",
    "                                              e.g. {'fileUri':'fileUri', 'startOffset':'startOffset_seconds','endOffset':'endOffset_seconds', 'mediaType':'mediaType'}           \n",
    "         dict multimodal_evaluation_promt: dictionary including prompts for multimodal content evaluations.\n",
    "                                           e.g. {\"video_prompt\":\"...\",\"image_prompt\":\"...\"}        \n",
    "         str experiment_name: name of the evaluation experiment\n",
    "        \"\"\"\n",
    "        \n",
    "        #set the parameters\n",
    "        self.location = location  \n",
    "        self.project = project   \n",
    "        self.items =items  \n",
    "        self.experiment_name=experiment_name      \n",
    "        self.multimodal_evaluation_promt=multimodal_evaluation_promt\n",
    "        self.response_A_userPrompt_column_name=response_A_userPrompt_column_name\n",
    "        self.response_B_userPrompt_column_name=response_B_userPrompt_column_name\n",
    "        self.response_A_llm_model_column_name=response_A_llm_model_column_name\n",
    "        self.response_B_llm_model_column_name=response_B_llm_model_column_name        \n",
    "        self.response_media_column_metadata=response_media_column_metadata\n",
    "        self.response_mediaType_column_name=response_mediaType_column_name\n",
    "        self.response_A_desc_column_name=response_A_desc_column_name\n",
    "        self.response_B_desc_column_name=response_B_desc_column_name\n",
    "      \n",
    "        self.run_experiment_name=self.experiment_name+\"-\"+ str(uuid.uuid4())\n",
    "\n",
    "         # Load the schema from PairWise_Schema.json\n",
    "        with open('PairWise_Schema.json') as config_file:\n",
    "            self.pairwise_schema = json.load(config_file)\n",
    "        \n",
    "        #initialize Vertex AI\n",
    "        vertexai.init(project=self.project, location= self.location )\n",
    "         \n",
    "\n",
    "    def set_evaluation_data(self):\n",
    "        \"\"\"\n",
    "        Prepare the input data as in a dataframe for evaluation\n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        eval_dataset = pd.DataFrame(\n",
    "                        {\n",
    "                            \"response_A\": self.items[self.response_A_desc_column_name].to_list(),\n",
    "                            \"response_B\": self.items[self.response_B_desc_column_name].to_list(),\n",
    "                                     \n",
    "                            **({\"mediaType\": self.items[self.response_mediaType_column_name].to_list()} if \n",
    "                               self.response_mediaType_column_name !=None else {}),\n",
    "                            **({\"multimodal_evaluation_promt\": [\n",
    "                                self.multimodal_evaluation_promt['video_prompt'] if 'video' in str(self.items[self.response_mediaType_column_name].to_list()[i]).lower() else \n",
    "                                self.multimodal_evaluation_promt['image_prompt'] if 'image' in str(self.items[self.response_mediaType_column_name].to_list()[i]).lower() else None\n",
    "                                for i in range(len(self.items))\n",
    "                            ]} if self.response_mediaType_column_name!=None and self.multimodal_evaluation_promt!=None else {}),\n",
    "                       \n",
    "                             **({\"instruction_A\": self.items[self.response_A_userPrompt_column_name].to_list()} if \n",
    "                               self.response_A_userPrompt_column_name !=None else {}),   \n",
    "                            **({\"instruction_B\": self.items[self.response_B_userPrompt_column_name].to_list()} if \n",
    "                               self.response_B_userPrompt_column_name !=None else {}),  \n",
    "                            \n",
    "                            \"reference\": [\n",
    "                                        json.dumps(\n",
    "                                            {\n",
    "                                                \"fileuri\": self.items[self.response_media_column_metadata['fileUri']].to_list()[i],\n",
    "                                                \"metadata\": {\n",
    "                                                    \"start_offset\": {\n",
    "                                                        \"seconds\": int(self.items[self.response_media_column_metadata['startOffset']].to_list()[i]),\n",
    "                                                        \"nanos\": 0,\n",
    "                                                    },\n",
    "                                                    \"end_offset\": {\n",
    "                                                        \"seconds\": int(self.items[self.response_media_column_metadata['endOffset']].to_list()[i]),\n",
    "                                                        \"nanos\": 0,\n",
    "                                                    },\n",
    "                                                } if self.response_media_column_metadata['startOffset'] in self.items.columns and \n",
    "                                                     self.response_media_column_metadata['endOffset'] in self.items.columns and \n",
    "                                                     'video' in str(self.items[self.response_mediaType_column_name].to_list()[i]).lower() \n",
    "                                                else {}\n",
    "                                            }\n",
    "                                        ) if self.response_media_column_metadata is not None and \n",
    "                                             self.response_media_column_metadata.get('fileUri') is not None \n",
    "                                          else \"{}\"\n",
    "                                \n",
    "                                        for i in range(len(self.items))\n",
    "                                    ],\n",
    "                            \"response_A_llm_model\": self.items[self.response_A_llm_model_column_name],\n",
    "                            \"response_B_llm_model\": self.items[self.response_B_llm_model_column_name],\n",
    "                            \"run_experiment_name\": [self.run_experiment_name] * len(self.items),\n",
    "                            \"run_experiment_date\": [datetime.today().strftime('%Y-%m-%d')] * len(self.items),\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        return eval_dataset\n",
    "    \n",
    "    def log_evaluations(self,result):\n",
    "        \"\"\"\n",
    "        Log the evaluation result into BigQuery, converting all columns to string type.\n",
    "\n",
    "        Args:\n",
    "            dataframe result : The evaluation result to be recorded into the database.\n",
    "        \"\"\"\n",
    "        import json\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud.exceptions import NotFound\n",
    "\n",
    "        # Load configuration from config.json\n",
    "        with open('config.json') as config_file:\n",
    "            config = json.load(config_file)\n",
    "\n",
    "        table_id = config['pairwise_eval_table']\n",
    "        dataset_id = config['eval_dataset']\n",
    "        project_id = config[\"project\"]\n",
    "        location_id = config[\"project_location\"]\n",
    "        table_full_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "        dataset_full_id = f\"{project_id}.{dataset_id}\"\n",
    "\n",
    "        # Remove unwanted characters from column names\n",
    "        result.columns = result.columns.str.replace(\"/\", \"_\").str.replace(',','')\n",
    "\n",
    "        # Convert all columns to string\n",
    "        result = result.astype(str)\n",
    "\n",
    "        # Convert DataFrame to list of dictionaries\n",
    "        data_as_dict = result.to_dict(orient='records')\n",
    "\n",
    "        # Initialize BigQuery Client\n",
    "        client = bigquery.Client()\n",
    "\n",
    "\n",
    "        try:\n",
    "            client.get_dataset(dataset_full_id)\n",
    "            print(f\"Dataset {dataset_full_id} exists.\")\n",
    "        except NotFound:\n",
    "            print(f\"Dataset {dataset_full_id} not found. Creating dataset...\")\n",
    "            dataset = bigquery.Dataset(dataset_full_id)\n",
    "            dataset.location = location_id\n",
    "            client.create_dataset(dataset)\n",
    "            print(f\"Dataset {dataset_full_id} created successfully.\")\n",
    "\n",
    "\n",
    "        # Ensure the dataset exists    \n",
    "        try:\n",
    "            # Fetch the existing table\n",
    "            table = client.get_table(table_full_id)\n",
    "            existing_schema = {field.name: field.field_type for field in table.schema}\n",
    "            print(f\"Table {table_full_id} exists. Checking schema...\")\n",
    "\n",
    "            # Identify new columns to be added\n",
    "            schema_changes = []\n",
    "            for col in result.columns:\n",
    "                if col not in existing_schema:\n",
    "                    schema_changes.append(bigquery.SchemaField(col, bigquery.enums.SqlTypeNames.STRING))\n",
    "\n",
    "            if schema_changes:\n",
    "                print(\"Altering schema to add new columns...\")\n",
    "                table.schema = table.schema + schema_changes\n",
    "                table = client.update_table(table, [\"schema\"])\n",
    "                print(f\"Schema updated successfully.\")\n",
    "\n",
    "        except NotFound:\n",
    "            print(f\"Table {table_full_id} not found. Creating table...\")\n",
    "            # Define schema as all string types\n",
    "            schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in result.columns]\n",
    "\n",
    "            # Create the table\n",
    "            table = bigquery.Table(table_full_id, schema=schema)\n",
    "            table = client.create_table(table)\n",
    "            print(f\"Table {table_full_id} created successfully.\")\n",
    "\n",
    "\n",
    "        # Insert rows into BigQuery\n",
    "        try:\n",
    "            errors = client.insert_rows_json(table_full_id, data_as_dict)\n",
    "            if not errors:\n",
    "                print(f\"Evaluations have successfully been loaded into {table_full_id}.\")\n",
    "            else:\n",
    "                print(\"Errors occurred while loading data:\")\n",
    "                for error in errors:\n",
    "                    print(error)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while inserting data: {e}\")\n",
    "    \n",
    "    \n",
    "    def get_autorater_response(self, metric_prompt: list, llm_model: str=\"gemini-1.5-pro\") -> dict:\n",
    "        \n",
    "        \"\"\"Extract evaluation metric on a AI-generated content using a AI-as-judge approach\n",
    "        \n",
    "        Args:\n",
    "        list metric_prompt: the input metric prompt parameters\n",
    "        str llm_model: evaluation model\n",
    "\n",
    "        Returns:\n",
    "        dict response_json: the evaluated metric in json format\n",
    "        \"\"\"\n",
    "            \n",
    "        # set evaluation metric schema\n",
    "        metric_response_schema = self.pairwise_schema \n",
    "\n",
    "        #define a generative model as an autorator\n",
    "        autorater = GenerativeModel(\n",
    "            llm_model,\n",
    "            generation_config=GenerationConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=metric_response_schema,\n",
    "            ),\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        #generate the rating metrics as per requested measures and metric in the prompt\n",
    "        response = autorater.generate_content(metric_prompt)\n",
    "\n",
    "        response_json = {}\n",
    "\n",
    "        if response.candidates and len(response.candidates) > 0:\n",
    "            candidate = response.candidates[0]\n",
    "            if (\n",
    "                candidate.content\n",
    "                and candidate.content.parts\n",
    "                and len(candidate.content.parts) > 0\n",
    "            ):\n",
    "                part = candidate.content.parts[0]\n",
    "                if part.text:\n",
    "                    response_json = json.loads(part.text)\n",
    "\n",
    "        return response_json\n",
    "\n",
    "    def custom_coverage_fn(self,instance):\n",
    "       \n",
    "        \"\"\"Extract evaluation metric on a AI-generated content using a AI-as-judge approach\n",
    "        \n",
    "        Args:\n",
    "        dict instance: an instance of predictions that should be evaluated\n",
    "       \n",
    "        Returns:\n",
    "        dict evaluation_response: scores and explanations related to the judgements for each requested metric\n",
    "        \"\"\"\n",
    "        \n",
    "        fileUri = json.loads(instance[\"reference\"])[\"fileuri\"]\n",
    "        eval_instruction_template =instance[\"multimodal_evaluation_promt\"]      \n",
    "        user_prompt_A_instruction= instance[\"instruction_A\"]\n",
    "        user_prompt_B_instruction= instance[\"instruction_B\"]\n",
    "        response_A = instance[\"response_A\"]\n",
    "        response_B = instance[\"response_B\"]\n",
    "        \n",
    "        evaluation_prompt=[]\n",
    "        # set the evaluation prompt\n",
    "        if 'video' in instance[\"mediaType\"]:   \n",
    "            evaluation_prompt = [\n",
    "                eval_instruction_template,       \n",
    "                \"VIDEO URI: \",\n",
    "                fileUri,\n",
    "                \"VIDEO METADATA: \",\n",
    "                json.dumps(json.loads(instance[\"reference\"])[\"metadata\"]),  \n",
    "                \"USER'S INPUT PROMPT MODEL A:\",\n",
    "                user_prompt_A_instruction,\n",
    "                \"USER'S INPUT PROMPT MODEL B:\",\n",
    "                user_prompt_B_instruction,\n",
    "                \"GENERATED RESPONSE MODEL A: \",\n",
    "                 response_A,\n",
    "                 \"GENERATED RESPONSE MODEL B: \",\n",
    "                 response_B,\n",
    "            ]\n",
    "        elif 'image' in instance[\"mediaType\"]:\n",
    "            # generate the evaluation prompt\n",
    "            evaluation_prompt = [\n",
    "                eval_instruction_template,       \n",
    "                \"IMAGE URI: \",\n",
    "                fileUri,   \n",
    "                \"USER'S INPUT PROMPT MODEL A:\",\n",
    "                user_prompt_A_instruction,\n",
    "                \"USER'S INPUT PROMPT MODEL B:\",\n",
    "                user_prompt_B_instruction,\n",
    "                \"GENERATED RESPONSE MODEL A: \",\n",
    "                 response_A,\n",
    "                 \"GENERATED RESPONSE MODEL B: \",\n",
    "                 response_B,\n",
    "            ]\n",
    "     \n",
    "        #generate evaluation response\n",
    "        evaluation_response = self.get_autorater_response(evaluation_prompt)\n",
    "        return evaluation_response\n",
    "\n",
    "    # Function to extract the score and explanation for each category\n",
    "    def flatten_evaluations(self,instance):\n",
    "        \"\"\"Flattens a dict column type in a dataframe series\n",
    "        \n",
    "        Args:\n",
    "        pandas.core.series.Series instance: an instance of predictions that should be evaluated\n",
    "       \n",
    "        Returns:\n",
    "        Dataframe flattened_data: flattened data\n",
    "        \"\"\" \n",
    "        flattened_data = {}\n",
    "        for key in self.pairwise_schema['required']:\n",
    "            flattened_data[f\"{key.lower().replace(' ', '_')}_score\"] = instance[key]['score']\n",
    "            flattened_data[f\"{key.lower().replace(' ', '_')}_explanation\"] = instance[key]['explanation']\n",
    "        \n",
    "        return flattened_data\n",
    "  \n",
    "    \n",
    "    def get_evaluations(self):\n",
    "        \"\"\"\n",
    "        Extracts the evaluation metricsusing:\n",
    "            1-user defined metrics and rating criteria\n",
    "            \n",
    "        \"\"\"\n",
    "        # set evaluation data\n",
    "        eval_dataset=self.set_evaluation_data()       \n",
    "            \n",
    "        #calculate coverage metrics\n",
    "        if self.multimodal_evaluation_promt:\n",
    "            #get evaluations\n",
    "            eval_dataset['custom_coverage']=eval_dataset.apply(self.custom_coverage_fn,axis=1)\n",
    "             \n",
    "            # Apply the function to flatten the 'custom_coverage' column and create new columns\n",
    "            flattened_df = eval_dataset['custom_coverage'].apply(self.flatten_evaluations)\n",
    "                                                                 \n",
    "            # Join the flattened columns to the original dataframe\n",
    "            eval_dataset = eval_dataset.join(pd.json_normalize(flattened_df))\n",
    "            eval_dataset = eval_dataset.drop(columns=[\"custom_coverage\"])\n",
    "            \n",
    "        eval_results=eval_dataset\n",
    "            \n",
    "        #log the statistics into bigquery\n",
    "        self.log_evaluations(eval_results)\n",
    "            \n",
    "        return eval_results \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c7977db-579f-49c9-8e37-9917249cbd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nine-quality-test.vlt_eval_statistics_schema exists.\n",
      "Table nine-quality-test.vlt_eval_statistics_schema.vlt_pairwise_eval_statistics not found. Creating table...\n",
      "Table nine-quality-test.vlt_eval_statistics_schema.vlt_pairwise_eval_statistics created successfully.\n",
      "Evaluations have successfully been loaded into nine-quality-test.vlt_eval_statistics_schema.vlt_pairwise_eval_statistics.\n"
     ]
    }
   ],
   "source": [
    "pairwise_evaluation_client=PairwiseEvaluationClient(project='nine-quality-test',\n",
    "                          location='us-central1',\n",
    "                          items=items,\n",
    "                          response_A_desc_column_name= \"description_A\",\n",
    "                          response_B_desc_column_name= \"description_B\",\n",
    "                          response_A_llm_model_column_name=\"modelVersion_A\",\n",
    "                          response_B_llm_model_column_name=\"modelVersion_B\",                        \n",
    "                         experiment_name=\"pairwise-evaluation-experiment\",    \n",
    "                         multimodal_evaluation_promt=multimodal_evaluation_promt, # prompt that will be used for video and image generated content evaluation comparisions\n",
    "                         response_A_userPrompt_column_name=\"prompt_text_A\", # name of the column in the 'items' data frame that includes user input prompt when generating content for model-A\n",
    "                         response_B_userPrompt_column_name=\"prompt_text_B\", # name of the column in the 'items' data frame that includes user input prompt when generating content for model-b\n",
    "                         response_media_column_metadata={'fileUri':'fileUri', 'startOffset':'startOffset_seconds','endOffset':'endOffset_seconds', 'mediaType':'asset_type'},   # name of metadata columns in the 'items' dataframe\n",
    "                         response_mediaType_column_name='asset_type'\n",
    "                             )\n",
    "evaluations=pairwise_evaluation_client.get_evaluations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38f6d8f-361c-4d29-ad9b-6f64c0022ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_A</th>\n",
       "      <th>response_B</th>\n",
       "      <th>mediaType</th>\n",
       "      <th>multimodal_evaluation_promt</th>\n",
       "      <th>instruction_A</th>\n",
       "      <th>instruction_B</th>\n",
       "      <th>reference</th>\n",
       "      <th>response_A_llm_model</th>\n",
       "      <th>response_B_llm_model</th>\n",
       "      <th>run_experiment_name</th>\n",
       "      <th>...</th>\n",
       "      <th>detailed_description_of_events_and_conversations_score</th>\n",
       "      <th>detailed_description_of_events_and_conversations_explanation</th>\n",
       "      <th>brands_companynames_and_logos_score</th>\n",
       "      <th>brands_companynames_and_logos_explanation</th>\n",
       "      <th>keylocations_and_scenes_score</th>\n",
       "      <th>keylocations_and_scenes_explanation</th>\n",
       "      <th>key_themes_score</th>\n",
       "      <th>key_themes_explanation</th>\n",
       "      <th>people_appearing_and_mentioned_score</th>\n",
       "      <th>people_appearing_and_mentioned_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>video/mp4</td>\n",
       "      <td># Instruction\\nYou are an expert evaluator. Yo...</td>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>{\"fileuri\": \"gs://raw_nine_files/vlt_video_ext...</td>\n",
       "      <td>gemini-1.5-pro-002</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>pairwise-evaluation-experiment-03b2e9a2-b118-4...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Both models provided similar and relatively de...</td>\n",
       "      <td>5</td>\n",
       "      <td>Model A is slightly better as it identifies \"F...</td>\n",
       "      <td>3</td>\n",
       "      <td>Both models captured the key locations accurat...</td>\n",
       "      <td>3</td>\n",
       "      <td>Both models accurately captured the key themes...</td>\n",
       "      <td>3</td>\n",
       "      <td>Both models accurately identified the people a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          response_A  \\\n",
       "0  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...   \n",
       "\n",
       "                                          response_B  mediaType  \\\n",
       "0  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...  video/mp4   \n",
       "\n",
       "                         multimodal_evaluation_promt  \\\n",
       "0  # Instruction\\nYou are an expert evaluator. Yo...   \n",
       "\n",
       "                                       instruction_A  \\\n",
       "0  Your task is to provide a comprehensive descri...   \n",
       "\n",
       "                                       instruction_B  \\\n",
       "0  Your task is to provide a comprehensive descri...   \n",
       "\n",
       "                                           reference response_A_llm_model  \\\n",
       "0  {\"fileuri\": \"gs://raw_nine_files/vlt_video_ext...   gemini-1.5-pro-002   \n",
       "\n",
       "   response_B_llm_model                                run_experiment_name  \\\n",
       "0  gemini-1.5-flash-002  pairwise-evaluation-experiment-03b2e9a2-b118-4...   \n",
       "\n",
       "   ... detailed_description_of_events_and_conversations_score  \\\n",
       "0  ...                                                  3       \n",
       "\n",
       "   detailed_description_of_events_and_conversations_explanation  \\\n",
       "0  Both models provided similar and relatively de...              \n",
       "\n",
       "  brands_companynames_and_logos_score  \\\n",
       "0                                   5   \n",
       "\n",
       "           brands_companynames_and_logos_explanation  \\\n",
       "0  Model A is slightly better as it identifies \"F...   \n",
       "\n",
       "  keylocations_and_scenes_score  \\\n",
       "0                             3   \n",
       "\n",
       "                 keylocations_and_scenes_explanation key_themes_score  \\\n",
       "0  Both models captured the key locations accurat...                3   \n",
       "\n",
       "                              key_themes_explanation  \\\n",
       "0  Both models accurately captured the key themes...   \n",
       "\n",
       "  people_appearing_and_mentioned_score  \\\n",
       "0                                    3   \n",
       "\n",
       "          people_appearing_and_mentioned_explanation  \n",
       "0  Both models accurately identified the people a...  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45139015-84f9-4f9b-b6b9-14ebf408f8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
