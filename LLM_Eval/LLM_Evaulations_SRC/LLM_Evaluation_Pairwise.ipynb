{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267fab21-d152-4ab1-8769-be85bfe0d7b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM Evaluation \n",
    "\n",
    "This code uses gcp evaluation service to evaluate the generated content by a generative AI API in terms of \n",
    "\n",
    "<b> built-in gcp evaluation service evaluation using user provided metrics-Use <i>PointWiseEvaluationMetrics.json</i> as a json file for the requested metrics and rating rubric:<br>\n",
    "- safety and sextural harmness\n",
    "- coherence and fluency\n",
    "- verbosity and repeatation<br>\n",
    "\n",
    "<b> mathematical metrics:<br>\n",
    "- perplexity\n",
    "- entropy<br>\n",
    "    \n",
    "<b> llm as a judge using user provided metrics:<br>\n",
    "- multimodal content coverage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830715c-0f3d-4cc1-b184-9f2512f71c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ab6b2b-8c59-4311-90df-5158cfaabd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import time\n",
    "import random\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from LLM_PointWiseEval_cls import PointWiseEvaluationClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf841a-47c8-4839-81fb-2111f1114298",
   "metadata": {},
   "source": [
    "### Load Prediction Data\n",
    "This block of code gets the executed predictions. The sample data for this test are stored in bigquery.\n",
    "\n",
    "Replace the code to load your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b85d2e8-3591-4bc2-af77-4e1708b38325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(table, dataset,project_id,filter_query=\"\"):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for text embeddings.\"\"\"\n",
    "  \n",
    "    sql = f\"\"\"  \n",
    "        WITH SEARCH_RESULT AS\n",
    "         (SELECT \n",
    "\n",
    "                        asset_id, \n",
    "                        content,\n",
    "                        headline,\n",
    "                        html_safe_text,\n",
    "                        description,\n",
    "                        startOffset_seconds,\n",
    "                        endOffset_seconds,\n",
    "                        fileUri,\n",
    "                        asset_type,\n",
    "                        first_published_timestamp,\n",
    "                        brand_type,\n",
    "                        primary_category_name,\n",
    "                        byline,\n",
    "                        image_license_type,\n",
    "                        publisher_type,\n",
    "                        photographer,\n",
    "                        date_published,\n",
    "                        dxcId,\n",
    "                        text_embedding_result ,\n",
    "                        byline[SAFE_OFFSET(0)].author_name ,  \n",
    "                        modelVersion,\n",
    "                        prompt_text,\n",
    "                        CAST(JSON_EXTRACT_SCALAR(media_jsonbody, '$.response.candidates[0].avgLogprobs') AS FLOAT64) AS  avgLogprobs\n",
    "                 FROM  `{dataset}.{table}` WHERE 1=1 and (LOWER(asset_type) LIKE '%video%' OR LOWER(asset_type) LIKE '%image%' ) {filter_query} \n",
    "        ),\n",
    "          IMAGE_CONTEXT AS (\n",
    "                   SELECT\n",
    "                          pd.asset_id,\n",
    "                          plain_text_column,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') AS image_id,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.caption') AS image_caption\n",
    "                        FROM\n",
    "                          (SELECT\n",
    "                              asset_id,\n",
    "                              plain_text_column,\n",
    "                              JSON_EXTRACT_ARRAY(article_body_json) AS article_body_json_array\n",
    "                            FROM\n",
    "                              `vlt_media_content_prelanding.vlt_article_content` -- change to vlt\n",
    "                            WHERE\n",
    "                              article_body_json IS NOT NULL\n",
    "                          ) pd,\n",
    "                          UNNEST(pd.article_body_json_array) AS entry -- Unnest the article body JSON array\n",
    "                        WHERE\n",
    "                          UPPER(JSON_EXTRACT_SCALAR(entry, '$.type')) = 'IMAGE' -- Filter to only 'IMAGE' type\n",
    "                          AND JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') IS NOT NULL -- Ensure there's an image ID\n",
    "                       \n",
    "          ) \n",
    "        \n",
    "        SELECT sr.*,    plain_text_column as image_context ,  image_caption\n",
    "        FROM SEARCH_RESULT   sr\n",
    "        LEFT JOIN IMAGE_CONTEXT imgcnxt\n",
    "        on REGEXP_REPLACE( sr.asset_id, r'\\..*', '') =imgcnxt.image_id\n",
    "    \"\"\"       \n",
    " \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "    output=[]\n",
    "    try:\n",
    "        # Fetch results\n",
    "        results = query_job.result()  \n",
    "        df = results.to_dataframe()\n",
    "       \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "            'startOffset_seconds', 'endOffset_seconds', 'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId','avgLogprobs', 'image_context','image_caption','modelVersion','prompt_text' ])\n",
    "        print(len(df))\n",
    "        # Sort by asset_id and startOffset_seconds to ensure proper order\n",
    "        df = df.sort_values(by=['asset_id', 'startOffset_seconds'])\n",
    "        \n",
    "     \n",
    "        # Aggregate descriptions for each asset_id, ordered by startOffset_seconds\n",
    "        # I dont want to aggregate different time-stamps\n",
    "        #df['description'] = df.groupby('asset_id')['description'].transform(lambda x: '\\n'.join(x))\n",
    "\n",
    "        # Aggregate and concatenate segments for each asset_id\n",
    "        df['time_lines'] = df.apply(\n",
    "            lambda row: f\"{{'startOffset_seconds': {row['startOffset_seconds']}, 'endOffset_seconds': {row['endOffset_seconds']}}}\", axis=1)\n",
    "            \n",
    "        # Now group by 'asset_id' and concatenate the strings in 'time_lines'\n",
    "        time_lines = df.groupby(['asset_id'])['time_lines'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "        \n",
    "        df.drop('time_lines', axis=1, inplace=True)\n",
    "        # Merge the time_lines into the original DataFrame\n",
    "        df = df.merge(time_lines, on=['asset_id'], how='left')\n",
    "    \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption','modelVersion','startOffset_seconds','endOffset_seconds','prompt_text' ])[['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption','modelVersion','startOffset_seconds','endOffset_seconds','prompt_text' ]]\n",
    "            \n",
    "        # Convert datetime to string using astype(str)\n",
    "        df['date_published'] = df['date_published'].astype(str)\n",
    "        df['first_published_timestamp'] = df['first_published_timestamp'].astype(str) \n",
    "        \n",
    "        #set the output\n",
    "        output = df#.to_dict(orient='records') \n",
    " \n",
    "    except Exception as e:\n",
    "        print('error'+str(e))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2127d60d-0381-4c2e-960b-aef542cf958b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568\n"
     ]
    }
   ],
   "source": [
    "dataset= \"vlt_media_embeddings_integration\"\n",
    "content_table=\"vlt_all_media_content_text_embeddings\"\n",
    "project_id='nine-quality-test'\n",
    "df=get_predictions(content_table, dataset,project_id,filter_query=\"\")\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55374f47-f05c-4e66-af6e-ac1f8c578887",
   "metadata": {},
   "source": [
    "### Pick Samples of Data\n",
    "Due to the costs, we just execute the evaluations on samples of data. \n",
    "\n",
    "The required columns for evaluations:\n",
    "\n",
    "- <b>AI-generated description:</b> if the final description should be composed of combinations of several columns, this should be done in advance as part of pre-processing and data preparation in previous step <br>\n",
    "- <b>AI-generated Average Log Probabilities:</b> will be used for calculating perplexity-set to None if not available.<br>\n",
    "- <b>LLM model version</b> <br>\n",
    "\n",
    "Replace the number of samples if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9097e9-2e8e-448d-9bcf-41fcfae4ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample=1#pick n random samples\n",
    "df = shuffle(df)\n",
    "items=df.sample(n_sample)\n",
    "items=items[['asset_id','description',\"avgLogprobs\",\"modelVersion\", 'prompt_text', 'fileUri', 'asset_type','startOffset_seconds','endOffset_seconds']]\n",
    "#set the respective column names for \n",
    "response_column_name='description'  #AI-generated description column name\n",
    "response_avgLogprobs='avgLogprobs' #AI-generated Average Log Probabilities column name\n",
    "response_modelVersion='modelVersion' # LLM model version column name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d218e46-7a00-4fb0-8d50-1a6e899d425a",
   "metadata": {},
   "source": [
    "### Load the user defined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c95b135-e309-43f5-aaf6-206e48e6671c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = \"content-generation-qa-quality\"\n",
    "file_path = 'PointWiseEvaluationMetrics.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    eval_metrics = json.load(file)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563a782-60e2-446d-8b51-23692b686d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pointwise_evaluation_client=PointWiseEvaluationClient(project='nine-quality-test',\n",
    "#                           location='us-central1',\n",
    "#                           items=items,\n",
    "#                           response_desc_column_name=response_column_name,\n",
    "#                           response_llm_model_column_name=response_modelVersion,\n",
    "#                           response_avgLogprobs_column_name=response_avgLogprobs,\n",
    "#                           eval_metrics=eval_metrics,\n",
    "#                          experiment_name=\"pointwise-evaluation-experiment\",    \n",
    "#                          evaluation_prompt= \"Evaluate the AI's contribution to a meaningful content generation. For rating and evaluationtion of the response on a 1-5 scale, use the given rubric criteria.\",\n",
    "#                          delete_experiment=True, # to save the costs, delete the evaluation experiment after the evaluation is finished\n",
    "#                          sys_metrics=True #calculate some mathematical metrics: entropy, perplexity\n",
    "#                          )\n",
    "# evaluations=pointwise_evaluation_client.get_evaluations()\n",
    " #evaluations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c48cff-f90e-49bf-9b53-168c4d2553c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Data Sample for Multimodal Coverage Evaluation\n",
    "The assumption is that the generated content is in the form of json including the fields that are requested from llm models to be extracted from the content.<br>\n",
    "Because we did not have data in our environment, we make some sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e00a83-6ab8-43b1-9771-9b3c374cc77b",
   "metadata": {},
   "source": [
    "# Sample User Prompt\n",
    "This is basically the prompt text that will be used to generate the content for each video segment or image during batch/online content generation.\n",
    "Here, we used this prompt to generate the content of a sample video from 600s to 900s using two different models 'gemini-1.5-pro-002', 'gemini-1.5-flash-002'. The generated content is recorded in json format in output_model1.txt and output_model2.txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef82971-48ea-496b-ad1e-442c7aa88541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start=600\n",
    "end=900\n",
    "schema=\"\"\"{\n",
    "    \"description\": \"A structured schema to represent detailed information from a video or text analysis\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Category\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The category or general type of the content\"\n",
    "        },\n",
    "        \"DetailedDescriptionOfEventsAndConversations\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A detailed textual description of the events and conversations in the content\"\n",
    "        },\n",
    "        \"BrandsCompanyNamesLogos\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of brands, company names, or logos appearing or mentioned in the content\"\n",
    "        },\n",
    "        \"KeyLocationsAndScenes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of key locations and scenes appearing or mentioned in the content\"\n",
    "        },\n",
    "        \"KeyThemes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of key themes discussed or portrayed in the content\"\n",
    "        },\n",
    "        \"PeopleAppearingAndMentioned\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of people who appear or are mentioned in the content\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"Category\",\n",
    "        \"DetailedDescriptionOfEventsAndConversations\",\n",
    "        \"BrandsCompanyNamesLogos\",\n",
    "        \"KeyLocationsAndScenes\",\n",
    "        \"KeyThemes\",\n",
    "        \"PeopleAppearingAndMentioned\"\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "VAR_VIDEO_SEGMENT=f\"Your task is to provide a comprehensive description of this video from segment {start} seconds to {end} seconds.\\n\"\n",
    "VAR_INSTRUCTIONS= \"\"\"To complete the task you need to follow these steps:\\n\n",
    "                           No greetings, closing remarks, or additional comments. Begin immediately with the video analysis and provide only the requested information in the specified format.\\n\n",
    "                           Idenify all instances of visual product placement. Pay close attention to background details and items held by the characters. List each product placement with the following\n",
    "                            information: Brand name, product name (if applicable), and a brief description. Include information about product placement into the description generated for the video\\n\n",
    "                           Create a transcript of all the speeches, dialogs, narration.\\n\n",
    "                           Scrupulously examine each scene for any and all visible brand names, logos, and products. Even if a product appears briefly or in the background, it should be included.\\n\"\"\"\n",
    "\n",
    "VAR_CONSTRAINTS= \"\"\"Describe the video content objectively, avoiding any subjective opinions or assumptions.\\n\n",
    "                           Specify who is saying what. If a person talking can be seen, specify their name and/or occupation. If it is voice behind the scenes, then describe it as a narrator.\\n\n",
    "                           Be specific when describing. Include all the information that is shown or given.\\n\n",
    "                           Do not show timestamps.\\n\n",
    "                           If an unidentified person is shown in the video first, but then their name is mentioned later in the video, make sure to mention their name in the description from the start.\\n\n",
    "                           \"\"\"\n",
    "\n",
    "VAR_STRUCTURE= f\"\"\"Organize the description with the following properties, and give a valid json file with JSON schema.<JSONSchema>{json.dumps(schema)}</JSONSchema>:\n",
    "                       \\n**Category**\\n\n",
    "                       \\n**DetailedDescriptionOfEventsAndConversations**\\n\n",
    "                       \\n**BrandsCompanyNamesLogos**\\n\n",
    "                       \\n**KeyLocationsAndScenes**\\n\n",
    "                       \\n**KeyThemes**\\n\n",
    "                       \\n**PeopleAppearingAndMentioned**\\n \n",
    "                 \"\"\" \n",
    "\n",
    "VAR_CONDITIONS = \"\"\"Identify a video as one of these categories: News, TV Shows, Live Sport Events, News Analyses. \\n\n",
    "                       When describing the DetailedDescriptionOfEventsAndConversations, consider the following instructions for specific video types:\\n\n",
    "                       * **News:** Pay close attention to transitions, graphics, and on-screen text.\\n\n",
    "                       * **TV Shows:** Describe facial expressions, body language, appearances, and overall mood.\\n\n",
    "                       * **Live Sports Events:** Focus on key moments, like goals or fouls, and describe the overall flow and momentum of the game.\\n\n",
    "                       * **News Analyses:** Identify different perspectives, arguments, and supporting evidence.\\n\n",
    "                       Make sure to mention people's names in the DetailedDescriptionOfEventsAndConversations and in PeopleAppearingAndMentioned as well as any other information about them like their age, occupation, location, etc. \\n\"\"\"\n",
    "\n",
    "VAR_EXAMPLE = \"\"\"Follow this example for the format of the output:\\n\n",
    "              {\n",
    "                \"Category\": \"TV Show\",\n",
    "                \"DetailedDescriptionOfEventsAndConversations\": \"The video starts with a man sitting at a dining table, reading a letter. Two Fiji bottles are visible on the benchtop. He has short, light brown hair and a beard. His name is Harrison. The scene changes to Melissa. Melissa says: \\\"I'm Melissa, and I'm a hairdresser. I'm 41 years old, and I'm from Sydney.\\\"\",\n",
    "                \"BrandsCompanyNamesLogos\": [\"Lacoste\", \"Fiji\"],\n",
    "                \"KeyLocationsAndScenes\": [\"Apartment\"],\n",
    "                \"KeyThemes\": [\"Marriage\"],\n",
    "                \"PeopleAppearingAndMentioned\": [\n",
    "                \"Harrison, 32, Builder, NSW\",\n",
    "                \"Melissa, 41, Hairdresser, NSW\"\n",
    "                ]\n",
    "            }\n",
    "               \"\"\"\n",
    "  \n",
    "    \n",
    "video_description_prompt=VAR_VIDEO_SEGMENT+VAR_INSTRUCTIONS+VAR_CONSTRAINTS+VAR_STRUCTURE+VAR_CONDITIONS+VAR_EXAMPLE\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00eb83a4-070a-48f2-947a-379fc1ca24fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load pre-executed predictions\n",
    "with open('output_model1.txt', 'r') as file:\n",
    "    response1 = json.dumps(json.load(file))\n",
    "with open('output_model2.txt', 'r') as file:\n",
    "    response2 = json.dumps(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1111a86d-aa54-4bde-9912-7a6ee1d0b8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"content-generation-qa-quality\"\n",
    "file_path = 'PointWiseMultimodalContentEvaluationMetrics.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    multimodal_eval_prompt_metrics = json.load(file)\n",
    "\n",
    "    \n",
    "video_multimodal_content_evaluation_metric_promopt =multimodal_eval_prompt_metrics['video_multimodal_content_evaluation_metric_promopt']\n",
    "image_multimodal_content_evaluation_metric_promopt= multimodal_eval_prompt_metrics['image_multimodal_content_evaluation_metric_promopt']\n",
    "\n",
    "multimodal_evaluation_promt={'video_prompt': video_multimodal_content_evaluation_metric_promopt,'image_prompt':image_multimodal_content_evaluation_metric_promopt}\n",
    "\n",
    "\n",
    "generated_response = [\n",
    "    response1,\n",
    "    response2\n",
    "]\n",
    "llm_models=['gemini-1.5-pro-002', 'gemini-1.5-flash-002']\n",
    "\n",
    "new_items= pd.DataFrame(\n",
    "    {   'asset_id':\"MAAT2024_1_A_HBB.mp4\",\n",
    "        \"prompt_text\": video_description_prompt, #this should be set with the prompt_text when doing batch generation\n",
    "        \"fileUri\":'gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4' , #this should be set to file uri when doing batch generation\n",
    "        \"description\": generated_response, #this should be set the generated content when doing batch generation\n",
    "        \"asset_type\": 'video/mp4', #this should be set to asset_type/mime_type when doing batch generation\n",
    "        \"startOffset_seconds\":[int(start)]*len(generated_response),\n",
    "        \"endOffset_seconds\":[int(end)]*len(generated_response),\n",
    "        \"modelVersion\":llm_models,\n",
    "        \"avgLogprobs\":[-0.25]*len(generated_response)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2133d346-be86-4716-97b6-b2d75e8bc500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#combine all samples\n",
    "items = pd.concat([items, new_items], ignore_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c7977db-579f-49c9-8e37-9917249cbd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-3b4bc6f0-1a7b-4f0b-8dab-a5fbd24bfedb\" href=\"#view-view-vertex-resource-3b4bc6f0-1a7b-4f0b-8dab-a5fbd24bfedb\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-3b4bc6f0-1a7b-4f0b-8dab-a5fbd24bfedb');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs?project=nine-quality-test');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs?project=nine-quality-test', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a to Experiment: pointwise-evaluation-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-3acf7dac-2178-41ec-855c-9afcdc84154f\" href=\"#view-view-vertex-resource-3acf7dac-2178-41ec-855c-9afcdc84154f\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-3acf7dac-2178-41ec-855c-9afcdc84154f');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a?project=nine-quality-test');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a?project=nine-quality-test', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:14<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:14.103694636 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment run pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a skipped backing tensorboard run deletion.\n",
      "To delete backing tensorboard run, execute the following:\n",
      "tensorboard_run_artifact = aiplatform.metadata.artifact.Artifact(artifact_name=f\"pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a-tb-run\")\n",
      "tensorboard_run_resource = aiplatform.TensorboardRun(tensorboard_run_artifact.metadata[\"resourceName\"])\n",
      "tensorboard_run_resource.delete()\n",
      "tensorboard_run_artifact.delete()\n",
      "Deleting Context : projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a\n",
      "Context deleted. . Resource name: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a\n",
      "Deleting Context resource: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a\n",
      "Delete Context backing LRO: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a/operations/2456246201209585664\n",
      "Context resource projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-54e57c50-3849-4e42-8858-c4d041da362a deleted.\n",
      "Deleting Context : projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment\n",
      "Context deleted. . Resource name: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment\n",
      "Deleting Context resource: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment\n",
      "Delete Context backing LRO: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment/operations/7565579978461413376\n",
      "Context resource projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment deleted.\n",
      "Dataset nine-quality-test.vlt_eval_statistics_schema exists.\n",
      "Table nine-quality-test.vlt_eval_statistics_schema.vlt_pointwise_eval_statistics exists. Checking schema...\n",
      "Evaluations have successfully been loaded into nine-quality-test.vlt_eval_statistics_schema.vlt_pointwise_eval_statistics.\n"
     ]
    }
   ],
   "source": [
    "pointwise_evaluation_client=PointWiseEvaluationClient(project='nine-quality-test',\n",
    "                          location='us-central1',\n",
    "                          items=items,\n",
    "                          response_desc_column_name=response_column_name,\n",
    "                          response_llm_model_column_name=response_modelVersion,\n",
    "                          response_avgLogprobs_column_name=response_avgLogprobs,\n",
    "                          eval_metrics=eval_metrics,\n",
    "                         experiment_name=\"pointwise-evaluation-experiment\",    \n",
    "                         evaluation_prompt= \"Evaluate the AI's contribution to a meaningful content generation. For rating and evaluationtion of the response on a 1-5 scale, use the given rubric criteria.\",\n",
    "                         delete_experiment=True, # to save the costs, delete the evaluation experiment after the evaluation is finished\n",
    "                         sys_metrics=True, #calculate some mathematical metrics: entropy, perplexity\n",
    "                         multimodal_evaluation_promt=multimodal_evaluation_promt, # prompt that will be used for video and image generated content evaluation\n",
    "                         response_userPrompt_column_name=\"prompt_text\", # name of the column in the 'items' data frame that includes user input prompt when generating content\n",
    "                         response_media_column_metadata={'fileUri':'fileUri', 'startOffset':'startOffset_seconds','endOffset':'endOffset_seconds', 'mediaType':'asset_type'},   # name of metadata columns in the 'items' dataframe\n",
    "                         response_mediaType_column_name='asset_type'\n",
    "                             )\n",
    "evaluations=pointwise_evaluation_client.get_evaluations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a333e5f4-5ba0-4f61-9127-6aeaf73463e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>evaluation_prompt</th>\n",
       "      <th>mediaType</th>\n",
       "      <th>avgLogprobs</th>\n",
       "      <th>multimodal_evaluation_promt</th>\n",
       "      <th>instruction</th>\n",
       "      <th>reference</th>\n",
       "      <th>response_llm_model</th>\n",
       "      <th>run_experiment_name</th>\n",
       "      <th>run_experiment_date</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>entropy</th>\n",
       "      <th>custom_coverage_score</th>\n",
       "      <th>custom_coverage_explanation</th>\n",
       "      <th>safety_explanation</th>\n",
       "      <th>safety_score</th>\n",
       "      <th>coherence and fluency_explanation</th>\n",
       "      <th>coherence and fluency_score</th>\n",
       "      <th>verbosity_explanation</th>\n",
       "      <th>verbosity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This close-up image captures Australian cricke...</td>\n",
       "      <td>Evaluate the AI's contribution to a meaningful...</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>-0.229052</td>\n",
       "      <td># Instruction\\nYou are an expert evaluator. Yo...</td>\n",
       "      <td>You are an assistant tasked with describing im...</td>\n",
       "      <td>{\"fileuri\": \"gs://nineshowcaseassets/IMAGES/0c...</td>\n",
       "      <td>gemini-1.5-pro-002@default</td>\n",
       "      <td>pointwise-evaluation-experiment-54e57c50-3849-...</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>1.257407</td>\n",
       "      <td>7.588403</td>\n",
       "      <td>5</td>\n",
       "      <td>The AI response has captured all the necessary...</td>\n",
       "      <td>The AI response provides a detailed and accura...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The AI's response demonstrates a comprehensive...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The AI response demonstrates excessive verbosi...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>Evaluate the AI's contribution to a meaningful...</td>\n",
       "      <td>video/mp4</td>\n",
       "      <td>-0.25</td>\n",
       "      <td># Instruction\\nYou are an expert evaluator. Yo...</td>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>{\"fileuri\": \"gs://raw_nine_files/vlt_video_ext...</td>\n",
       "      <td>gemini-1.5-pro-002</td>\n",
       "      <td>pointwise-evaluation-experiment-54e57c50-3849-...</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>1.284025</td>\n",
       "      <td>7.384875</td>\n",
       "      <td>4</td>\n",
       "      <td>The AI response captures most of the requested...</td>\n",
       "      <td>The AI-generated content is safe and appropria...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The AI response demonstrates good coherence an...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The response is verbose and contains redundant...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>Evaluate the AI's contribution to a meaningful...</td>\n",
       "      <td>video/mp4</td>\n",
       "      <td>-0.25</td>\n",
       "      <td># Instruction\\nYou are an expert evaluator. Yo...</td>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>{\"fileuri\": \"gs://raw_nine_files/vlt_video_ext...</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>pointwise-evaluation-experiment-54e57c50-3849-...</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>1.284025</td>\n",
       "      <td>6.755835</td>\n",
       "      <td>5</td>\n",
       "      <td>The response captures all the requested inform...</td>\n",
       "      <td>The AI-generated content is safe and appropria...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The AI response demonstrates good coherence an...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The AI-generated content is well-structured an...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  This close-up image captures Australian cricke...   \n",
       "1  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...   \n",
       "2  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...   \n",
       "\n",
       "                                   evaluation_prompt   mediaType avgLogprobs  \\\n",
       "0  Evaluate the AI's contribution to a meaningful...  image/jpeg   -0.229052   \n",
       "1  Evaluate the AI's contribution to a meaningful...   video/mp4       -0.25   \n",
       "2  Evaluate the AI's contribution to a meaningful...   video/mp4       -0.25   \n",
       "\n",
       "                         multimodal_evaluation_promt  \\\n",
       "0  # Instruction\\nYou are an expert evaluator. Yo...   \n",
       "1  # Instruction\\nYou are an expert evaluator. Yo...   \n",
       "2  # Instruction\\nYou are an expert evaluator. Yo...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  You are an assistant tasked with describing im...   \n",
       "1  Your task is to provide a comprehensive descri...   \n",
       "2  Your task is to provide a comprehensive descri...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  {\"fileuri\": \"gs://nineshowcaseassets/IMAGES/0c...   \n",
       "1  {\"fileuri\": \"gs://raw_nine_files/vlt_video_ext...   \n",
       "2  {\"fileuri\": \"gs://raw_nine_files/vlt_video_ext...   \n",
       "\n",
       "           response_llm_model  \\\n",
       "0  gemini-1.5-pro-002@default   \n",
       "1          gemini-1.5-pro-002   \n",
       "2        gemini-1.5-flash-002   \n",
       "\n",
       "                                 run_experiment_name run_experiment_date  \\\n",
       "0  pointwise-evaluation-experiment-54e57c50-3849-...          2025-01-24   \n",
       "1  pointwise-evaluation-experiment-54e57c50-3849-...          2025-01-24   \n",
       "2  pointwise-evaluation-experiment-54e57c50-3849-...          2025-01-24   \n",
       "\n",
       "  perplexity   entropy custom_coverage_score  \\\n",
       "0   1.257407  7.588403                     5   \n",
       "1   1.284025  7.384875                     4   \n",
       "2   1.284025  6.755835                     5   \n",
       "\n",
       "                         custom_coverage_explanation  \\\n",
       "0  The AI response has captured all the necessary...   \n",
       "1  The AI response captures most of the requested...   \n",
       "2  The response captures all the requested inform...   \n",
       "\n",
       "                                  safety_explanation  safety_score  \\\n",
       "0  The AI response provides a detailed and accura...           5.0   \n",
       "1  The AI-generated content is safe and appropria...           5.0   \n",
       "2  The AI-generated content is safe and appropria...           5.0   \n",
       "\n",
       "                   coherence and fluency_explanation  \\\n",
       "0  The AI's response demonstrates a comprehensive...   \n",
       "1  The AI response demonstrates good coherence an...   \n",
       "2  The AI response demonstrates good coherence an...   \n",
       "\n",
       "   coherence and fluency_score  \\\n",
       "0                          5.0   \n",
       "1                          4.0   \n",
       "2                          4.0   \n",
       "\n",
       "                               verbosity_explanation  verbosity_score  \n",
       "0  The AI response demonstrates excessive verbosi...              2.0  \n",
       "1  The response is verbose and contains redundant...              2.0  \n",
       "2  The AI-generated content is well-structured an...              4.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a9c9f-221e-4a59-b33a-a088635d62db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
