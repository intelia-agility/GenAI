{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267fab21-d152-4ab1-8769-be85bfe0d7b4",
   "metadata": {},
   "source": [
    "### LLM Evaluation \n",
    "\n",
    "This code uses gcp evaluation service to evaluate the generated content by a generative AI API in terms of \n",
    "\n",
    "<b> built-in gcp evaluation service evaluation using user provided metrics:<br>\n",
    "- safety and sextural harmness\n",
    "- coherence and fluency\n",
    "- verbosity and repeatation<br>\n",
    "\n",
    "<b> mathematical metrics:<br>\n",
    "- perplexity\n",
    "- entropy<br>\n",
    "    \n",
    "<b> llm as a judge using user provided metrics:<br>\n",
    "- multimodal content coverage\n",
    "\n",
    "\n",
    "Use PointWiseEvaluationMetrics.json as a json file for the requested metrics and rating rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830715c-0f3d-4cc1-b184-9f2512f71c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ab6b2b-8c59-4311-90df-5158cfaabd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import time\n",
    "import random\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "#from LLM_PointWiseEval_cls import PointWiseEvaluationClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf841a-47c8-4839-81fb-2111f1114298",
   "metadata": {},
   "source": [
    "### Load Prediction Data\n",
    "This block of code gets the executed predictions. The sample data for this test are stored in bigquery.\n",
    "\n",
    "Replace the code to load your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b85d2e8-3591-4bc2-af77-4e1708b38325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(table, dataset,project_id,filter_query=\"\"):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for text embeddings.\"\"\"\n",
    "  \n",
    "    sql = f\"\"\"  \n",
    "        WITH SEARCH_RESULT AS\n",
    "         (SELECT \n",
    "\n",
    "                        asset_id, \n",
    "                        content,\n",
    "                        headline,\n",
    "                        html_safe_text,\n",
    "                        description,\n",
    "                        startOffset_seconds,\n",
    "                        endOffset_seconds,\n",
    "                        fileUri,\n",
    "                        asset_type,\n",
    "                        first_published_timestamp,\n",
    "                        brand_type,\n",
    "                        primary_category_name,\n",
    "                        byline,\n",
    "                        image_license_type,\n",
    "                        publisher_type,\n",
    "                        photographer,\n",
    "                        date_published,\n",
    "                        dxcId,\n",
    "                        text_embedding_result ,\n",
    "                        byline[SAFE_OFFSET(0)].author_name ,  \n",
    "                        modelVersion,\n",
    "                        CAST(JSON_EXTRACT_SCALAR(media_jsonbody, '$.response.candidates[0].avgLogprobs') AS FLOAT64) AS  avgLogprobs\n",
    "                 FROM  `{dataset}.{table}` WHERE 1=1 and (LOWER(asset_type) LIKE '%video%' OR LOWER(asset_type) LIKE '%image%' ) {filter_query} \n",
    "        ),\n",
    "          IMAGE_CONTEXT AS (\n",
    "                   SELECT\n",
    "                          pd.asset_id,\n",
    "                          plain_text_column,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') AS image_id,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.caption') AS image_caption\n",
    "                        FROM\n",
    "                          (SELECT\n",
    "                              asset_id,\n",
    "                              plain_text_column,\n",
    "                              JSON_EXTRACT_ARRAY(article_body_json) AS article_body_json_array\n",
    "                            FROM\n",
    "                              `vlt_media_content_prelanding.vlt_article_content` -- change to vlt\n",
    "                            WHERE\n",
    "                              article_body_json IS NOT NULL\n",
    "                          ) pd,\n",
    "                          UNNEST(pd.article_body_json_array) AS entry -- Unnest the article body JSON array\n",
    "                        WHERE\n",
    "                          UPPER(JSON_EXTRACT_SCALAR(entry, '$.type')) = 'IMAGE' -- Filter to only 'IMAGE' type\n",
    "                          AND JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') IS NOT NULL -- Ensure there's an image ID\n",
    "                       \n",
    "          ) \n",
    "        \n",
    "        SELECT sr.*,    plain_text_column as image_context ,  image_caption\n",
    "        FROM SEARCH_RESULT   sr\n",
    "        LEFT JOIN IMAGE_CONTEXT imgcnxt\n",
    "        on REGEXP_REPLACE( sr.asset_id, r'\\..*', '') =imgcnxt.image_id\n",
    "    \"\"\"       \n",
    " \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "    output=[]\n",
    "    try:\n",
    "        # Fetch results\n",
    "        results = query_job.result()  \n",
    "        df = results.to_dataframe()\n",
    "       \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "            'startOffset_seconds', 'endOffset_seconds', 'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId','avgLogprobs', 'image_context','image_caption','modelVersion' ])\n",
    "        print(len(df))\n",
    "        # Sort by asset_id and startOffset_seconds to ensure proper order\n",
    "        df = df.sort_values(by=['asset_id', 'startOffset_seconds'])\n",
    "        \n",
    "     \n",
    "        # Aggregate descriptions for each asset_id, ordered by startOffset_seconds\n",
    "        # I dont want to aggregate different time-stamps\n",
    "        #df['description'] = df.groupby('asset_id')['description'].transform(lambda x: '\\n'.join(x))\n",
    "\n",
    "        # Aggregate and concatenate segments for each asset_id\n",
    "        df['time_lines'] = df.apply(\n",
    "            lambda row: f\"{{'startOffset_seconds': {row['startOffset_seconds']}, 'endOffset_seconds': {row['endOffset_seconds']}}}\", axis=1)\n",
    "            \n",
    "        # Now group by 'asset_id' and concatenate the strings in 'time_lines'\n",
    "        time_lines = df.groupby(['asset_id'])['time_lines'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "        \n",
    "        df.drop('time_lines', axis=1, inplace=True)\n",
    "        # Merge the time_lines into the original DataFrame\n",
    "        df = df.merge(time_lines, on=['asset_id'], how='left')\n",
    "    \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption','modelVersion' ])[['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption','modelVersion' ]]\n",
    "            \n",
    "        # Convert datetime to string using astype(str)\n",
    "        df['date_published'] = df['date_published'].astype(str)\n",
    "        df['first_published_timestamp'] = df['first_published_timestamp'].astype(str) \n",
    "        \n",
    "        #set the output\n",
    "        output = df#.to_dict(orient='records') \n",
    " \n",
    "    except Exception as e:\n",
    "        print('error'+str(e))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2127d60d-0381-4c2e-960b-aef542cf958b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568\n"
     ]
    }
   ],
   "source": [
    "dataset= \"vlt_media_embeddings_integration\"\n",
    "content_table=\"vlt_all_media_content_text_embeddings\"\n",
    "project_id='nine-quality-test'\n",
    "df=get_predictions(content_table, dataset,project_id,filter_query=\"\")\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55374f47-f05c-4e66-af6e-ac1f8c578887",
   "metadata": {},
   "source": [
    "### Pick Samples of Data\n",
    "Due to the costs, we just execute the evaluations on samples of data. \n",
    "\n",
    "The required columns for evaluations:\n",
    "\n",
    "- <b>AI-generated description:</b> if the final description should be composed of combinations of several columns, this should be done in advance as part of pre-processing and data preparation in previous step <br>\n",
    "- <b>AI-generated Average Log Probabilities:</b> will be used for calculating perplexity-set to None if not available.<br>\n",
    "- <b>LLM model version</b> <br>\n",
    "\n",
    "Replace the number of samples if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9097e9-2e8e-448d-9bcf-41fcfae4ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample=1#pick n random samples\n",
    "df = shuffle(df)\n",
    "items=df.sample(n_sample)\n",
    "items=items[['description',\"avgLogprobs\",\"modelVersion\"]]\n",
    "#set the respective column names for \n",
    "response_column_name='description'  #AI-generated description column name\n",
    "response_avgLogprobs='avgLogprobs' #AI-generated Average Log Probabilities column name\n",
    "response_modelVersion='modelVersion' # LLM model version column name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d218e46-7a00-4fb0-8d50-1a6e899d425a",
   "metadata": {},
   "source": [
    "### Load the user defined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c95b135-e309-43f5-aaf6-206e48e6671c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "experiment_name = \"content-generation-qa-quality\"\n",
    "file_path = 'PointWiseEvaluationMetrics.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    eval_metrics = json.load(file)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2563a782-60e2-446d-8b51-23692b686d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m pointwise_evaluation_client\u001b[38;5;241m=\u001b[39mPointWiseEvaluationClient(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnine-quality-test\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                           location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus-central1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                           items\u001b[38;5;241m=\u001b[39mitems,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m                          sys_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#calculate some mathematical metrics: entropy, perplexity\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                          )\n\u001b[0;32m---> 13\u001b[0m evaluations\u001b[38;5;241m=\u001b[39m\u001b[43mpointwise_evaluation_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 284\u001b[0m, in \u001b[0;36mPointWiseEvaluationClient.get_evaluations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03mExtracts the evaluation metricsusing:\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    1-user defined metrics and rating criteria\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    2-pre-defined mathematical metrics: perplexity, entropy\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# set evaluation data\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_evaluation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m#calculate the system defined metrics\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msys_metrics:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# the evrage prob column is given in the data, calculate perplexity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 106\u001b[0m, in \u001b[0;36mPointWiseEvaluationClient.set_evaluation_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_evaluation_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m        Prepare the input data as in a dataframe for evaluation\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m         eval_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     93\u001b[0m                         {\n\u001b[1;32m     94\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_desc_column_name]\u001b[38;5;241m.\u001b[39mto_list(),\n\u001b[1;32m     95\u001b[0m                             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavgLogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_avgLogprobs_column_name]\u001b[38;5;241m.\u001b[39mto_list()} \u001b[38;5;28;01mif\u001b[39;00m \n\u001b[1;32m     96\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_avgLogprobs_column_name \u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#                             **({\"multimodal_evaluation_promt\": [\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#                                 self.multimodal_evaluation_promt['video_prompt'] if 'video' in str(self.items[self.response_mediaType_column_name][i]).lower() else \u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m#                                 self.multimodal_evaluation_promt['image_prompt'] if 'image' in str(self.items[self.response_mediaType_column_name][i]).lower() else None\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#                                 for i in range(len(self.items))\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#                             ]} if self.response_mediaType_column_name!=None and self.multimodal_evaluation_promt!=None else {}),\u001b[39;00m\n\u001b[1;32m    102\u001b[0m                        \n\u001b[1;32m    103\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_userPrompt_column_name]\u001b[38;5;241m.\u001b[39mto_list()} \u001b[38;5;28;01mif\u001b[39;00m \n\u001b[1;32m    104\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_userPrompt_column_name \u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),                            \n\u001b[1;32m    105\u001b[0m                             \n\u001b[0;32m--> 106\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefrence\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    107\u001b[0m                                             {\n\u001b[1;32m    108\u001b[0m                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileuri\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfileUri\u001b[39m\u001b[38;5;124m'\u001b[39m]][i],\n\u001b[1;32m    109\u001b[0m                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    110\u001b[0m                                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartOffset\u001b[39m\u001b[38;5;124m'\u001b[39m]][i] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnanos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[1;32m    111\u001b[0m                                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendOffset\u001b[39m\u001b[38;5;124m'\u001b[39m]][i],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnanos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m    112\u001b[0m                                                             } \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartOffset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendOffset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    113\u001b[0m                                             } \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfileUri\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m                                             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems))\n\u001b[1;32m    115\u001b[0m                                         ],                            \n\u001b[1;32m    116\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_llm_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_llm_model_column_name],\n\u001b[1;32m    117\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_experiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_experiment_name] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems),\n\u001b[1;32m    118\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_experiment_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: [datetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems),\n\u001b[1;32m    119\u001b[0m                         }\n\u001b[1;32m    120\u001b[0m                     )\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m eval_dataset\n",
      "Cell \u001b[0;32mIn[7], line 113\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_evaluation_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m        Prepare the input data as in a dataframe for evaluation\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m         eval_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     93\u001b[0m                         {\n\u001b[1;32m     94\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_desc_column_name]\u001b[38;5;241m.\u001b[39mto_list(),\n\u001b[1;32m     95\u001b[0m                             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavgLogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_avgLogprobs_column_name]\u001b[38;5;241m.\u001b[39mto_list()} \u001b[38;5;28;01mif\u001b[39;00m \n\u001b[1;32m     96\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_avgLogprobs_column_name \u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#                             **({\"multimodal_evaluation_promt\": [\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#                                 self.multimodal_evaluation_promt['video_prompt'] if 'video' in str(self.items[self.response_mediaType_column_name][i]).lower() else \u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m#                                 self.multimodal_evaluation_promt['image_prompt'] if 'image' in str(self.items[self.response_mediaType_column_name][i]).lower() else None\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#                                 for i in range(len(self.items))\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#                             ]} if self.response_mediaType_column_name!=None and self.multimodal_evaluation_promt!=None else {}),\u001b[39;00m\n\u001b[1;32m    102\u001b[0m                        \n\u001b[1;32m    103\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_userPrompt_column_name]\u001b[38;5;241m.\u001b[39mto_list()} \u001b[38;5;28;01mif\u001b[39;00m \n\u001b[1;32m    104\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_userPrompt_column_name \u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),                            \n\u001b[1;32m    105\u001b[0m                             \n\u001b[1;32m    106\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefrence\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    107\u001b[0m                                             {\n\u001b[1;32m    108\u001b[0m                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileuri\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfileUri\u001b[39m\u001b[38;5;124m'\u001b[39m]][i],\n\u001b[1;32m    109\u001b[0m                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    110\u001b[0m                                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartOffset\u001b[39m\u001b[38;5;124m'\u001b[39m]][i] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnanos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[1;32m    111\u001b[0m                                                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendOffset\u001b[39m\u001b[38;5;124m'\u001b[39m]][i],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnanos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m    112\u001b[0m                                                             } \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartOffset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_media_column_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendOffset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 113\u001b[0m                                             } \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_media_column_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfileUri\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m                                             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems))\n\u001b[1;32m    115\u001b[0m                                         ],                            \n\u001b[1;32m    116\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_llm_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_llm_model_column_name],\n\u001b[1;32m    117\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_experiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_experiment_name] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems),\n\u001b[1;32m    118\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_experiment_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: [datetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems),\n\u001b[1;32m    119\u001b[0m                         }\n\u001b[1;32m    120\u001b[0m                     )\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m eval_dataset\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pointwise_evaluation_client=PointWiseEvaluationClient(project='nine-quality-test',\n",
    "                          location='us-central1',\n",
    "                          items=items,\n",
    "                          response_desc_column_name=response_column_name,\n",
    "                          response_llm_model_column_name=response_modelVersion,\n",
    "                          response_avgLogprobs_column_name=response_avgLogprobs,\n",
    "                          eval_metrics=eval_metrics,\n",
    "                         experiment_name=\"pointwise-evaluation-experiment\",    \n",
    "                         evaluation_prompt= \"Evaluate the AI's contribution to a meaningful content generation. For rating and evaluationtion of the response on a 1-5 scale, use the given rubric criteria.\",\n",
    "                         delete_experiment=True, # to save the costs, delete the evaluation experiment after the evaluation is finished\n",
    "                         sys_metrics=True #calculate some mathematical metrics: entropy, perplexity\n",
    "                         )\n",
    "evaluations=pointwise_evaluation_client.get_evaluations()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfa3ad9-aad1-4bcf-877c-bd45aff67940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>avgLogprobs</th>\n",
       "      <th>response_llm_model</th>\n",
       "      <th>run_experiment_name</th>\n",
       "      <th>run_experiment_date</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>entropy</th>\n",
       "      <th>safety-explanation</th>\n",
       "      <th>safety-score</th>\n",
       "      <th>coherence and fluency-explanation</th>\n",
       "      <th>coherence and fluency-score</th>\n",
       "      <th>verbosity-explanation</th>\n",
       "      <th>verbosity-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The image features a middle-aged man with a fa...</td>\n",
       "      <td>-0.153841</td>\n",
       "      <td>modelVersion</td>\n",
       "      <td>pointwise-evaluation-experiment-c661aae5-1af3-...</td>\n",
       "      <td>2025-01-22</td>\n",
       "      <td>1.166305</td>\n",
       "      <td>7.8817</td>\n",
       "      <td>The AI response provides a detailed and compre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The AI response demonstrates excellent coheren...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The AI response is excessively verbose and con...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response avgLogprobs  \\\n",
       "0  The image features a middle-aged man with a fa...   -0.153841   \n",
       "\n",
       "  response_llm_model                                run_experiment_name  \\\n",
       "0       modelVersion  pointwise-evaluation-experiment-c661aae5-1af3-...   \n",
       "\n",
       "  run_experiment_date perplexity entropy  \\\n",
       "0          2025-01-22   1.166305  7.8817   \n",
       "\n",
       "                                  safety-explanation  safety-score  \\\n",
       "0  The AI response provides a detailed and compre...           5.0   \n",
       "\n",
       "                   coherence and fluency-explanation  \\\n",
       "0  The AI response demonstrates excellent coheren...   \n",
       "\n",
       "   coherence and fluency-score  \\\n",
       "0                          5.0   \n",
       "\n",
       "                               verbosity-explanation  verbosity-score  \n",
       "0  The AI response is excessively verbose and con...              2.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c48cff-f90e-49bf-9b53-168c4d2553c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Data Sample for Multimodal Coverage Evaluation\n",
    "The assumption is that the generated content is in the form of json including the fields that are requested from llm models to be extracted from the content.<br>\n",
    "Because we did not have data in our environment, we make some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd7129d-6903-4f3e-b2a9-75a27d22a183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from vertexai.evaluation import (\n",
    "    EvalTask, \n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate\n",
    ")\n",
    " \n",
    "import uuid \n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "from datetime import datetime\n",
    "\n",
    "class PointWiseEvaluationClient:\n",
    "    \"\"\"Wrapper around Pointwise Evaluation Client.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project: str=None,\n",
    "        location: str = \"us-central1\",\n",
    "        items: pd.core.frame.DataFrame = None,\n",
    "        response_desc_column_name: str= 'description',\n",
    "        response_llm_model_column_name: str= None,\n",
    "        response_avgLogprobs_column_name: str=None,\n",
    "        response_mediaType_column_name: str=None,\n",
    "        response_media_column_metadata : dict=None,\n",
    "        response_userPrompt_column_name: str=None,\n",
    "        multimodal_evaluation_promt: dict=None,\n",
    "        eval_metrics: list[dict] =None,\n",
    "        experiment_name: str=\"pointwise-evaluation-experiment\",\n",
    "        evaluation_prompt: str=\"Evaluate the AI's contribution to a meaningful content generation\",  \n",
    "        delete_experiment: bool= True,\n",
    "        sys_metrics: bool= True,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initis the hyper parameters\n",
    "        \n",
    "        Args:\n",
    "         str project:  project id \n",
    "         str locations: project location         \n",
    "         Dataframe items: dataframe of AI-generated responses\n",
    "         str response_desc_column_name: the name of the column in the 'items' dataframe that includes the AI-generated response\n",
    "         str response_llm_model_column_name: the name of the column in the 'items' dataframe that includes the name of the model that is used for extracting AI-generated responses\n",
    "         str response_avgLogprobs_column_name:  the name of the column in the 'items' dataframe that includes AI-generated response average probability log values\n",
    "         str response_mediaType_column_name:  the name of the column in the 'items' dataframe that represent media type\n",
    "         str response_userPrompt_column_name: the name of the column in the 'items' dataframe that represent user prompt using which the AI model generated the response\n",
    "         dict response_media_column_metadata: dictionary including the name of fileuri, start and endoffset of the media if available\n",
    "                                              e.g. {'fileUri':'fileUri', 'startOffset':'startOffset_seconds','endOffset':'endOffset_seconds', 'mediaType':'mediaType'}           \n",
    "         dict multimodal_evaluation_promt: dictionary including prompts for multimodal content evaluations.\n",
    "                                           e.g. {\"video_prompt\":\"...\",\"image_prompt\":\"...\"}\n",
    "         list[dict] eval_metrics: user defined evaluation metrics along with their rating rubric\n",
    "                                  e.g.  [ {  \"metric\": \"safety\", \"criteria\": \"...\" }]\n",
    "         str experiment_name: name of the evaluation experiment\n",
    "         str evaluation_prompt: the prompt text which will be used as a prompt to evaluate the eval_metrics        \n",
    "         bool delete_experiment: delete the generated experience after the evaluation are done if True. Will save costs.\n",
    "         bool sys_metrics: calculates some mathematical metrics including perplexity, entropy if set to True.\n",
    "        \"\"\"\n",
    "        \n",
    "        #set the parameters\n",
    "        self.location = location  \n",
    "        self.project = project   \n",
    "        self.items =items  \n",
    "        self.eval_metrics=eval_metrics #user defined metrics along with their rubric ratings\n",
    "        self.experiment_name=experiment_name\n",
    "        self.evaluation_prompt=evaluation_prompt\n",
    "        self.multimodal_evaluation_promt=multimodal_evaluation_promt\n",
    "        self.response_userPrompt_column_name=response_userPrompt_column_name\n",
    "        self.response_llm_model_column_name=response_llm_model_column_name\n",
    "        self.response_media_column_metadata=response_media_column_metadata\n",
    "        self.response_mediaType_column_name=response_mediaType_column_name\n",
    "        self.response_desc_column_name=response_desc_column_name\n",
    "        self.delete_experiment=delete_experiment\n",
    "        self.response_avgLogprobs_column_name= response_avgLogprobs_column_name\n",
    "        self.sys_metrics=sys_metrics\n",
    "        self.run_experiment_name=self.experiment_name+\"-\"+ str(uuid.uuid4())\n",
    "        \n",
    "        #initialize Vertex AI\n",
    "        vertexai.init(project=self.project, location= self.location )\n",
    "         \n",
    "\n",
    "    def set_evaluation_data(self):\n",
    "        \"\"\"\n",
    "        Prepare the input data as in a dataframe for evaluation\n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        eval_dataset = pd.DataFrame(\n",
    "                        {\n",
    "                            \"response\": self.items[self.response_desc_column_name].to_list(),\n",
    "                            **({\"avgLogprobs\": self.items[self.response_avgLogprobs_column_name].to_list()} if \n",
    "                               self.response_avgLogprobs_column_name !=None else {}),\n",
    "#                             **({\"multimodal_evaluation_promt\": [\n",
    "#                                 self.multimodal_evaluation_promt['video_prompt'] if 'video' in str(self.items[self.response_mediaType_column_name][i]).lower() else \n",
    "#                                 self.multimodal_evaluation_promt['image_prompt'] if 'image' in str(self.items[self.response_mediaType_column_name][i]).lower() else None\n",
    "#                                 for i in range(len(self.items))\n",
    "#                             ]} if self.response_mediaType_column_name!=None and self.multimodal_evaluation_promt!=None else {}),\n",
    "                       \n",
    "                             **({\"instruction\": self.items[self.response_userPrompt_column_name].to_list()} if \n",
    "                               self.response_userPrompt_column_name !=None else {}),                            \n",
    "                            \n",
    "                            \"refrence\": [\n",
    "                                            {\n",
    "                                                \"fileuri\": self.items[self.response_media_column_metadata['fileUri']][i],\n",
    "                                                \"metadata\": {\n",
    "                                                                \"start_offset\": {\"seconds\":self.items[self.response_media_column_metadata['startOffset']][i] , \"nanos\": 0},\n",
    "                                                                \"end_offset\": {\"seconds\":self.items[self.response_media_column_metadata['endOffset']][i],\"nanos\": 0}\n",
    "                                                            } if self.response_media_column_metadata['startOffset'] in self.items.columns and self.response_media_column_metadata['endOffset'] in self.items.columns else {}\n",
    "                                            } if self.response_media_column_metadata['fileUri'] !=None else {}\n",
    "                                            for i in range(len(self.items))\n",
    "                                        ],                            \n",
    "                            \"response_llm_model\": self.items[self.response_llm_model_column_name],\n",
    "                            \"run_experiment_name\": [self.run_experiment_name] * len(self.items),\n",
    "                            \"run_experiment_date\": [datetime.today().strftime('%Y-%m-%d')] * len(self.items),\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        return eval_dataset\n",
    "\n",
    "    def log_evaluations(self, result):\n",
    "        \"\"\"\n",
    "        Log the evaluation result into BigQuery, altering the table schema if needed.\n",
    "\n",
    "        Args:\n",
    "            dataframe result : The evaluation result to be recorded into the database.\n",
    "        \"\"\"\n",
    "        # Load configuration from config.json\n",
    "        with open('config.json') as config_file:\n",
    "            config = json.load(config_file)\n",
    "\n",
    "        table_id = config['pointwise_eval_table']\n",
    "        dataset_id = config['eval_dataset']\n",
    "        project_id = config[\"project\"]\n",
    "        location_id=config[\"project_location\"]\n",
    "        table_full_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "        dataset_full_id = f\"{project_id}.{dataset_id}\"\n",
    "\n",
    "        #remove unwanted characters from column name\n",
    "        result.columns = result.columns.str.replace(\"/\", \"-\")\n",
    "\n",
    "        # Initialize BigQuery Client\n",
    "        client = bigquery.Client()\n",
    "\n",
    "\n",
    "        # Ensure the dataset exists\n",
    "        try:\n",
    "            client.get_dataset(dataset_full_id)\n",
    "            print(f\"Dataset {dataset_full_id} exists.\")\n",
    "        except NotFound:\n",
    "            print(f\"Dataset {dataset_full_id} not found. Creating dataset...\")\n",
    "            dataset = bigquery.Dataset(dataset_full_id)\n",
    "            dataset.location = location_id \n",
    "            client.create_dataset(dataset)\n",
    "            print(f\"Dataset {dataset_full_id} created successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Fetch the existing table\n",
    "            table = client.get_table(table_full_id)\n",
    "            existing_schema = {field.name: field.field_type for field in table.schema}\n",
    "            print(f\"Table {table_full_id} exists. Checking schema...\")\n",
    "\n",
    "            # Infer schema from DataFrame\n",
    "            new_schema = {\n",
    "                name: bigquery.enums.SqlTypeNames.DATE if (dtype == 'object'  and name=='run_experiment_date')\n",
    "                else bigquery.enums.SqlTypeNames.STRING if dtype == 'object'\n",
    "                else bigquery.enums.SqlTypeNames.FLOAT if dtype in ['float64', 'float32']\n",
    "                else bigquery.enums.SqlTypeNames.INTEGER if dtype in ['int64', 'int32']\n",
    "                else bigquery.enums.SqlTypeNames.BOOLEAN if dtype == 'bool'\n",
    "                else bigquery.enums.SqlTypeNames.TIMESTAMP if dtype == 'datetime64[ns]'\n",
    "                else bigquery.enums.SqlTypeNames.STRING\n",
    "                for name, dtype in zip(result.columns, result.dtypes)\n",
    "            }\n",
    "\n",
    "            # Identify schema differences\n",
    "            schema_changes = []\n",
    "            for col, dtype in new_schema.items():\n",
    "                if col not in existing_schema:\n",
    "                    # Add new column\n",
    "                    schema_changes.append(bigquery.SchemaField(col, dtype))\n",
    "                elif existing_schema[col] != dtype:\n",
    "                    print(f\"Type change detected for column '{col}' from {existing_schema[col]} to {dtype}.\")\n",
    "                    # BigQuery doesn't allow direct type changes; handle as needed.\n",
    "\n",
    "            if schema_changes:\n",
    "                print(\"Altering schema to add new columns...\")\n",
    "                table.schema = table.schema + schema_changes\n",
    "                table = client.update_table(table, [\"schema\"])\n",
    "                print(f\"Table {table_full_id} schema updated successfully.\")\n",
    "            else:\n",
    "                print(\"Schema is already up-to-date.\")\n",
    "\n",
    "        except NotFound:\n",
    "            print(f\"Table {table_full_id} not found. Creating table...\")\n",
    "            # Infer schema from DataFrame\n",
    "            schema = [\n",
    "                bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.DATE if (dtype == 'object'  and name=='run_experiment_date')\n",
    "                                     else bigquery.enums.SqlTypeNames.STRING if dtype == 'object' \n",
    "                                     else bigquery.enums.SqlTypeNames.FLOAT if dtype in ['float64', 'float32']\n",
    "                                     else bigquery.enums.SqlTypeNames.INTEGER if dtype in ['int64', 'int32']\n",
    "                                     else bigquery.enums.SqlTypeNames.BOOLEAN if dtype == 'bool'\n",
    "                                     else bigquery.enums.SqlTypeNames.TIMESTAMP if dtype == 'datetime64[ns]'\n",
    "                                     else bigquery.enums.SqlTypeNames.STRING)\n",
    "                for name, dtype in zip(result.columns, result.dtypes)\n",
    "            ]\n",
    "\n",
    "            # Create the table\n",
    "            table = bigquery.Table(table_full_id, schema=schema)\n",
    "            table = client.create_table(table)\n",
    "            print(f\"Table {table_full_id} created successfully.\")\n",
    "\n",
    "        # Define job configuration\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        )\n",
    "\n",
    "        # Save DataFrame to BigQuery\n",
    "        job = client.load_table_from_dataframe(result, table_full_id, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "\n",
    "        # Additional error inspection after the job completes\n",
    "        if job.errors:\n",
    "            print(\"The job completed with the following errors:\")\n",
    "            for error in job.errors:\n",
    "                print(f\" - {error['message']}\")\n",
    "        else:\n",
    "            print(f\"Evaluations have successfully been loaded into {table_full_id}.\")\n",
    "\n",
    "    def perplexity(self,prob: float):    \n",
    "        \"\"\"Extract perplexity- models confidence in predicting next token using average log probablity\n",
    "\n",
    "          Args:\n",
    "          float prob: average log probability\n",
    "\n",
    "          Returns:\n",
    "          float:  perplexity value\n",
    "\n",
    "          \"\"\"\n",
    "        return math.exp(-prob)\n",
    "    \n",
    "    \n",
    "    def entropy(self,text: str):\n",
    "        \"\"\"Extracts entropy of a texts, higher entropy means diverse range of tokens have been choosen\n",
    "\n",
    "        Args:\n",
    "        str text: the input text\n",
    "\n",
    "        Returns:\n",
    "        float entropy: entropy value of input text\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize the text into words (ignoring punctuation)\n",
    "        words = text.lower().split()\n",
    "\n",
    "        # Get the frequency of each word\n",
    "        word_count = Counter(words)\n",
    "\n",
    "        # Total number of words\n",
    "        total_words = len(words)\n",
    "\n",
    "        # Calculate the probability of each word\n",
    "        probabilities = [count / total_words for count in word_count.values()]\n",
    "\n",
    "        # Calculate entropy using the formula\n",
    "        entrpy = -sum(p * math.log2(p) for p in probabilities)\n",
    "\n",
    "        return entrpy\n",
    "\n",
    "\n",
    "    def get_evaluations(self):\n",
    "        \"\"\"\n",
    "        Extracts the evaluation metricsusing:\n",
    "            1-user defined metrics and rating criteria\n",
    "            2-pre-defined mathematical metrics: perplexity, entropy\n",
    "\n",
    "        \"\"\"\n",
    "        # set evaluation data\n",
    "        eval_dataset=self.set_evaluation_data()\n",
    "        \n",
    "        #calculate the system defined metrics\n",
    "        if self.sys_metrics:\n",
    "            # the evrage prob column is given in the data, calculate perplexity\n",
    "            if self.response_avgLogprobs_column_name:\n",
    "                eval_dataset['perplexity']=eval_dataset[self.response_avgLogprobs_column_name].apply(self.perplexity)\n",
    "            \n",
    "            #calculate entropy\n",
    "            eval_dataset['entropy']=eval_dataset['response'].apply(self.entropy)\n",
    "            eval_results=eval_dataset\n",
    "        \n",
    "        #calcualte user defined metrics\n",
    "        if self.eval_metrics:\n",
    "            metrics=[]\n",
    "            # Define  pointwise quality metric(s)\n",
    "            for metric in self.eval_metrics:\n",
    "                # Define a pointwise quality metric\n",
    "                pointwise_quality_metric_prompt = f\"\"\"{self.evaluation_prompt}; evaluate {metric['metric']}.\n",
    "                # Rubric rating criteria\n",
    "                {metric['criteria']}\n",
    "                # AI-generated Response\n",
    "                {{response}}\n",
    "                \"\"\"\n",
    "                pointwise_metric=PointwiseMetric(\n",
    "                    metric=metric['metric'],\n",
    "                    metric_prompt_template=pointwise_quality_metric_prompt,\n",
    "                )\n",
    "                metrics.append(pointwise_metric)\n",
    "                \n",
    "            # Create the evaluation task\n",
    "            eval_task = EvalTask(\n",
    "                dataset=eval_dataset,\n",
    "                metrics=metrics,\n",
    "                experiment=self.experiment_name,\n",
    "            )\n",
    "            # Run evaluation on the data using the evaluation service\n",
    "            results = eval_task.evaluate( \n",
    "\n",
    "                    experiment_run_name=self.run_experiment_name,\n",
    "                ) \n",
    "            #Delete the experiment after getting the result\n",
    "            if self.delete_experiment:\n",
    "                experiment = aiplatform.Experiment(self.experiment_name)\n",
    "                experiment.delete()\n",
    "                \n",
    "            eval_results=results.metrics_table\n",
    "            \n",
    "        #log the statistics into bigquery\n",
    "        self.log_evaluations(eval_results)\n",
    "            \n",
    "        return eval_results \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e00a83-6ab8-43b1-9771-9b3c374cc77b",
   "metadata": {},
   "source": [
    "# Sample User Prompt\n",
    "This is basically the prompt text that will be used to generate the content for each video segment or image during batch/online content generation.\n",
    "Here, we used this prompt to generate the content of a sample video from 600s to 900s using two different models 'gemini-1.5-pro-002', 'gemini-1.5-flash-002'. The generated content is recorded in json format in output_model1.txt and output_model2.txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fef82971-48ea-496b-ad1e-442c7aa88541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start=600\n",
    "end=900\n",
    "schema=\"\"\"{\n",
    "    \"description\": \"A structured schema to represent detailed information from a video or text analysis\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Category\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The category or general type of the content\"\n",
    "        },\n",
    "        \"DetailedDescriptionOfEventsAndConversations\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A detailed textual description of the events and conversations in the content\"\n",
    "        },\n",
    "        \"BrandsCompanyNamesLogos\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of brands, company names, or logos appearing or mentioned in the content\"\n",
    "        },\n",
    "        \"KeyLocationsAndScenes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of key locations and scenes appearing or mentioned in the content\"\n",
    "        },\n",
    "        \"KeyThemes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of key themes discussed or portrayed in the content\"\n",
    "        },\n",
    "        \"PeopleAppearingAndMentioned\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"A list of people who appear or are mentioned in the content\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"Category\",\n",
    "        \"DetailedDescriptionOfEventsAndConversations\",\n",
    "        \"BrandsCompanyNamesLogos\",\n",
    "        \"KeyLocationsAndScenes\",\n",
    "        \"KeyThemes\",\n",
    "        \"PeopleAppearingAndMentioned\"\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "VAR_VIDEO_SEGMENT=f\"Your task is to provide a comprehensive description of this video from segment {start} seconds to {end} seconds.\\n\"\n",
    "VAR_INSTRUCTIONS= \"\"\"To complete the task you need to follow these steps:\\n\n",
    "                           No greetings, closing remarks, or additional comments. Begin immediately with the video analysis and provide only the requested information in the specified format.\\n\n",
    "                           Idenify all instances of visual product placement. Pay close attention to background details and items held by the characters. List each product placement with the following\n",
    "                            information: Brand name, product name (if applicable), and a brief description. Include information about product placement into the description generated for the video\\n\n",
    "                           Create a transcript of all the speeches, dialogs, narration.\\n\n",
    "                           Scrupulously examine each scene for any and all visible brand names, logos, and products. Even if a product appears briefly or in the background, it should be included.\\n\"\"\"\n",
    "\n",
    "VAR_CONSTRAINTS= \"\"\"Describe the video content objectively, avoiding any subjective opinions or assumptions.\\n\n",
    "                           Specify who is saying what. If a person talking can be seen, specify their name and/or occupation. If it is voice behind the scenes, then describe it as a narrator.\\n\n",
    "                           Be specific when describing. Include all the information that is shown or given.\\n\n",
    "                           Do not show timestamps.\\n\n",
    "                           If an unidentified person is shown in the video first, but then their name is mentioned later in the video, make sure to mention their name in the description from the start.\\n\n",
    "                           \"\"\"\n",
    "\n",
    "VAR_STRUCTURE= f\"\"\"Organize the description with the following properties, and give a valid json file with JSON schema.<JSONSchema>{json.dumps(schema)}</JSONSchema>:\n",
    "                       \\n**Category**\\n\n",
    "                       \\n**DetailedDescriptionOfEventsAndConversations**\\n\n",
    "                       \\n**BrandsCompanyNamesLogos**\\n\n",
    "                       \\n**KeyLocationsAndScenes**\\n\n",
    "                       \\n**KeyThemes**\\n\n",
    "                       \\n**PeopleAppearingAndMentioned**\\n \n",
    "                 \"\"\" \n",
    "\n",
    "VAR_CONDITIONS = \"\"\"Identify a video as one of these categories: News, TV Shows, Live Sport Events, News Analyses. \\n\n",
    "                       When describing the DetailedDescriptionOfEventsAndConversations, consider the following instructions for specific video types:\\n\n",
    "                       * **News:** Pay close attention to transitions, graphics, and on-screen text.\\n\n",
    "                       * **TV Shows:** Describe facial expressions, body language, appearances, and overall mood.\\n\n",
    "                       * **Live Sports Events:** Focus on key moments, like goals or fouls, and describe the overall flow and momentum of the game.\\n\n",
    "                       * **News Analyses:** Identify different perspectives, arguments, and supporting evidence.\\n\n",
    "                       Make sure to mention people's names in the DetailedDescriptionOfEventsAndConversations and in PeopleAppearingAndMentioned as well as any other information about them like their age, occupation, location, etc. \\n\"\"\"\n",
    "\n",
    "VAR_EXAMPLE = \"\"\"Follow this example for the format of the output:\\n\n",
    "              {\n",
    "                \"Category\": \"TV Show\",\n",
    "                \"DetailedDescriptionOfEventsAndConversations\": \"The video starts with a man sitting at a dining table, reading a letter. Two Fiji bottles are visible on the benchtop. He has short, light brown hair and a beard. His name is Harrison. The scene changes to Melissa. Melissa says: \\\"I'm Melissa, and I'm a hairdresser. I'm 41 years old, and I'm from Sydney.\\\"\",\n",
    "                \"BrandsCompanyNamesLogos\": [\"Lacoste\", \"Fiji\"],\n",
    "                \"KeyLocationsAndScenes\": [\"Apartment\"],\n",
    "                \"KeyThemes\": [\"Marriage\"],\n",
    "                \"PeopleAppearingAndMentioned\": [\n",
    "                \"Harrison, 32, Builder, NSW\",\n",
    "                \"Melissa, 41, Hairdresser, NSW\"\n",
    "                ]\n",
    "            }\n",
    "               \"\"\"\n",
    "  \n",
    "    \n",
    "video_description_prompt=VAR_VIDEO_SEGMENT+VAR_INSTRUCTIONS+VAR_CONSTRAINTS+VAR_STRUCTURE+VAR_CONDITIONS+VAR_EXAMPLE\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00eb83a4-070a-48f2-947a-379fc1ca24fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load pre-executed predictions\n",
    "with open('output_model1.txt', 'r') as file:\n",
    "    response1 = json.dumps(json.load(file))\n",
    "with open('output_model2.txt', 'r') as file:\n",
    "    response2 = json.dumps(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1111a86d-aa54-4bde-9912-7a6ee1d0b8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_evaluation_prompt=\"\"\"\n",
    "  # Instruction\n",
    "  You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models. We will provide you with the user prompt,  and an AI-generated responses, video and the segment (in seconds) for which this response is generated.\n",
    "  You should first read the user input carefully for analyzing the task, then look into video segment, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
    "  You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step by step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
    "\n",
    "  # Evaluation\n",
    "  ## Metric Definition\n",
    "  You will be assessing coverage, which measures the ability to provide a detailed response based on a the given video segment and requested properties.\n",
    "  \n",
    "\n",
    "  ## Criteria\n",
    "  Coverage: It is the quality of capturing all required detail for each requested property.\n",
    "  In the context of video content capturing, it refers to the way that all the details of the following properties are captured and presented throughly:\n",
    "  \n",
    "  - Category \n",
    "  - Detailed Description Of Events And Conversations \n",
    "  - Brands,CompanyNames, and Logos \n",
    "  - KeyLocations And Scenes\" \n",
    "  - Key Themes \n",
    "  - People Appearing And Mentioned \n",
    "  \n",
    "  This AI-generated responses will be used for data retrieval. So, it has to be able to capture all the details of a scene.\n",
    "\n",
    "  ## Rating Rubric\n",
    "  5: (Perfectly Aligned) The details of the video segment is captured properly for all the requested properties thoroughly.\n",
    "  4: (Highly Aligned) The descriptions captured for each property generally supports the details in the video segment and can be used for data retrieval.\n",
    "  3: (Moderately Aligned) The descriptions captured for each property is captured well, but there might be minor inconsistencies or missed details, and the response is broadly relevant but not entirely specific.\n",
    "  2: (Poorly Aligned)  The descriptions captured for each property has significant inconsistencies with the video segment, raising doubts about the validity and quality of text to be usable for data retrieval.\n",
    "  1: (Misaligned) The descriptions captured for each property has major inconsistencies with the video segment and many information and details are missed casuing a very poor quality of text for data retrieval.\n",
    "\n",
    "  ## Evaluation Steps\n",
    "  STEP 1:  Assess User Instruction:  Carefully read the user input prompt to understand the user's request and requested information.\n",
    "  STEP 2:  Analyze Video Segment: Examine the video segment for each requested property to to make sure all the requested information are captured in detail.\n",
    "  STEP 3: Evaluate Accuracy:  For each requested property, Check if the generated response correctly identifies the information and details described in the video segment.\n",
    "  STEP 4:  Identify Inconsistencies: for each requested property, look for any discrepancies between the video segment details and the captured information in the AI-generated responses. For example, if any information is missed, or not captured right.\n",
    "  STEP 5:  Determine Overall Coverage: Based on the previous steps, assign a coverage score using the 1-5 rubric.  Consider the severity of any inconsistencies and their potential impact on the data retrieval.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "image_evaluation_prompt=\"\"\"\n",
    "  # Instruction\n",
    "  You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models. We will provide you with the user prompt,  and an AI-generated responses, video and the segment (in seconds) for which this response is generated.\n",
    "  You should first read the user input carefully for analyzing the task, then look into video segment, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
    "  You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step by step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
    "\n",
    "  # Evaluation\n",
    "  ## Metric Definition\n",
    "  You will be assessing coverage, which measures the ability to provide a detailed response based on a the given video segment and requested properties.\n",
    "  \n",
    "\n",
    "  ## Criteria\n",
    "  Coverage: It is the quality of capturing all required detail for each requested property.\n",
    "  In the context of video content capturing, it refers to the way that all the details of the following properties are captured and presented throughly:\n",
    "  \n",
    "  - Category \n",
    "  - Detailed Description Of Events And Conversations \n",
    "  - Brands,CompanyNames, and Logos \n",
    "  - KeyLocations And Scenes\" \n",
    "  - Key Themes \n",
    "  - People Appearing And Mentioned \n",
    "  \n",
    "  This AI-generated responses will be used for data retrieval. So, it has to be able to capture all the details of a scene.\n",
    "\n",
    "  ## Rating Rubric\n",
    "  5: (Perfectly Aligned) The details of the video segment is captured properly for all the requested properties thoroughly.\n",
    "  4: (Highly Aligned) The descriptions captured for each property generally supports the details in the video segment and can be used for data retrieval.\n",
    "  3: (Moderately Aligned) The descriptions captured for each property is captured well, but there might be minor inconsistencies or missed details, and the response is broadly relevant but not entirely specific.\n",
    "  2: (Poorly Aligned)  The descriptions captured for each property has significant inconsistencies with the video segment, raising doubts about the validity and quality of text to be usable for data retrieval.\n",
    "  1: (Misaligned) The descriptions captured for each property has major inconsistencies with the video segment and many information and details are missed casuing a very poor quality of text for data retrieval.\n",
    "\n",
    "  ## Evaluation Steps\n",
    "  STEP 1:  Assess User Instruction:  Carefully read the user input prompt to understand the user's request and requested information.\n",
    "  STEP 2:  Analyze Video Segment: Examine the video segment for each requested property to to make sure all the requested information are captured in detail.\n",
    "  STEP 3: Evaluate Accuracy:  For each requested property, Check if the generated response correctly identifies the information and details described in the video segment.\n",
    "  STEP 4:  Identify Inconsistencies: for each requested property, look for any discrepancies between the video segment details and the captured information in the AI-generated responses. For example, if any information is missed, or not captured right.\n",
    "  STEP 5:  Determine Overall Coverage: Based on the previous steps, assign a coverage score using the 1-5 rubric.  Consider the severity of any inconsistencies and their potential impact on the data retrieval.\n",
    "  \"\"\"\n",
    "\n",
    "multimodal_evaluation_promt={'video_prompt': video_evaluation_prompt,'image_prompt':image_evaluation_prompt}\n",
    "\n",
    "\n",
    "generated_response = [\n",
    "    response1,\n",
    "    response2\n",
    "]\n",
    "llm_models=['gemini-1.5-pro-002', 'gemini-1.5-flash-002']\n",
    "\n",
    "items = pd.DataFrame(\n",
    "    {\n",
    "        \"prompt_text\": video_description_prompt, #this should be set with the prompt_text when doing batch generation\n",
    "        \"fileUri\":'gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4' , #this should be set to file uri when doing batch generation\n",
    "        \"description\": generated_response, #this should be set the generated content when doing batch generation\n",
    "        \"asset_type\": 'video/mp4' #this should be set to asset_type/mime_type when doing batch generation\n",
    "    \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03a5e7f4-65b3-47d9-b3f2-2a4418e8a532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>fileUri</th>\n",
       "      <th>description</th>\n",
       "      <th>asset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>gs://raw_nine_files/vlt_video_extract/MAAT/MAA...</td>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>video/mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your task is to provide a comprehensive descri...</td>\n",
       "      <td>gs://raw_nine_files/vlt_video_extract/MAAT/MAA...</td>\n",
       "      <td>{\"Category\": \"TV Show\", \"DetailedDescriptionOf...</td>\n",
       "      <td>video/mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         prompt_text  \\\n",
       "0  Your task is to provide a comprehensive descri...   \n",
       "1  Your task is to provide a comprehensive descri...   \n",
       "\n",
       "                                             fileUri  \\\n",
       "0  gs://raw_nine_files/vlt_video_extract/MAAT/MAA...   \n",
       "1  gs://raw_nine_files/vlt_video_extract/MAAT/MAA...   \n",
       "\n",
       "                                         description asset_type  \n",
       "0  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...  video/mp4  \n",
       "1  {\"Category\": \"TV Show\", \"DetailedDescriptionOf...  video/mp4  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7977db-579f-49c9-8e37-9917249cbd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_evaluation_client=PointWiseEvaluationClient(project='nine-quality-test',\n",
    "                          location='us-central1',\n",
    "                          items=items,\n",
    "                          response_desc_column_name=response_column_name,\n",
    "                          response_llm_model_column_name=response_modelVersion,\n",
    "                          response_avgLogprobs_column_name=response_avgLogprobs,\n",
    "                          eval_metrics=eval_metrics,\n",
    "                         experiment_name=\"pointwise-evaluation-experiment\",    \n",
    "                         evaluation_prompt= \"Evaluate the AI's contribution to a meaningful content generation. For rating and evaluationtion of the response on a 1-5 scale, use the given rubric criteria.\",\n",
    "                         delete_experiment=True, # to save the costs, delete the evaluation experiment after the evaluation is finished\n",
    "                         sys_metrics=True #calculate some mathematical metrics: entropy, perplexity\n",
    "                         )\n",
    "evaluations=pointwise_evaluation_client.get_evaluations()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
