{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267fab21-d152-4ab1-8769-be85bfe0d7b4",
   "metadata": {},
   "source": [
    "### LLM Evaluation \n",
    "\n",
    "This code uses gcp evaluation service to evaluate the generated content by a generative AI API in terms of \n",
    "- safety and sextural harmness\n",
    "- coherence and fluency\n",
    "- verbosity and repeatation\n",
    "- perplexity\n",
    "- entropy\n",
    "\n",
    "\n",
    "Use PointWiseEvaluationMetrics.json as a json file for the requested metrics and rating rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf841a-47c8-4839-81fb-2111f1114298",
   "metadata": {},
   "source": [
    "### Get data from biquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85d2e8-3591-4bc2-af77-4e1708b38325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "def get_predictions(table, dataset,project_id,filter_query=\"\"):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for text embeddings.\"\"\"\n",
    "  \n",
    "    sql = f\"\"\"  \n",
    "        WITH SEARCH_RESULT AS\n",
    "         (SELECT \n",
    "\n",
    "                        asset_id, \n",
    "                        content,\n",
    "                        headline,\n",
    "                        html_safe_text,\n",
    "                        description,\n",
    "                        startOffset_seconds,\n",
    "                        endOffset_seconds,\n",
    "                        fileUri,\n",
    "                        asset_type,\n",
    "                        first_published_timestamp,\n",
    "                        brand_type,\n",
    "                        primary_category_name,\n",
    "                        byline,\n",
    "                        image_license_type,\n",
    "                        publisher_type,\n",
    "                        photographer,\n",
    "                        date_published,\n",
    "                        dxcId,\n",
    "                        text_embedding_result ,\n",
    "                        byline[SAFE_OFFSET(0)].author_name ,  \n",
    "                        modelVersion,\n",
    "                        CAST(JSON_EXTRACT_SCALAR(media_jsonbody, '$.response.candidates[0].avgLogprobs') AS FLOAT64) AS  avgLogprobs\n",
    "                 FROM  `{dataset}.{table}` WHERE 1=1 and (LOWER(asset_type) LIKE '%video%' OR LOWER(asset_type) LIKE '%image%' ) {filter_query} \n",
    "        ),\n",
    "          IMAGE_CONTEXT AS (\n",
    "                   SELECT\n",
    "                          pd.asset_id,\n",
    "                          plain_text_column,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') AS image_id,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.caption') AS image_caption\n",
    "                        FROM\n",
    "                          (SELECT\n",
    "                              asset_id,\n",
    "                              plain_text_column,\n",
    "                              JSON_EXTRACT_ARRAY(article_body_json) AS article_body_json_array\n",
    "                            FROM\n",
    "                              `vlt_media_content_prelanding.vlt_article_content` -- change to vlt\n",
    "                            WHERE\n",
    "                              article_body_json IS NOT NULL\n",
    "                          ) pd,\n",
    "                          UNNEST(pd.article_body_json_array) AS entry -- Unnest the article body JSON array\n",
    "                        WHERE\n",
    "                          UPPER(JSON_EXTRACT_SCALAR(entry, '$.type')) = 'IMAGE' -- Filter to only 'IMAGE' type\n",
    "                          AND JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') IS NOT NULL -- Ensure there's an image ID\n",
    "                       \n",
    "          ) \n",
    "        \n",
    "        SELECT sr.*,    plain_text_column as image_context ,  image_caption\n",
    "        FROM SEARCH_RESULT   sr\n",
    "        LEFT JOIN IMAGE_CONTEXT imgcnxt\n",
    "        on REGEXP_REPLACE( sr.asset_id, r'\\..*', '') =imgcnxt.image_id\n",
    "    \"\"\"       \n",
    " ##LOWER(asset_type) LIKE '%image%' OR \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "    output=[]\n",
    "    try:\n",
    "        # Fetch results\n",
    "        results = query_job.result()  \n",
    "        df = results.to_dataframe()\n",
    "       \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "            'startOffset_seconds', 'endOffset_seconds', 'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId','avgLogprobs', 'image_context','image_caption','modelVersion' ])\n",
    "        print(len(df))\n",
    "        # Sort by asset_id and startOffset_seconds to ensure proper order\n",
    "        df = df.sort_values(by=['asset_id', 'startOffset_seconds'])\n",
    "        \n",
    "     \n",
    "        # Aggregate descriptions for each asset_id, ordered by startOffset_seconds\n",
    "        # I dont want to aggregate different time-stamps\n",
    "        #df['description'] = df.groupby('asset_id')['description'].transform(lambda x: '\\n'.join(x))\n",
    "\n",
    "        # Aggregate and concatenate segments for each asset_id\n",
    "        df['time_lines'] = df.apply(\n",
    "            lambda row: f\"{{'startOffset_seconds': {row['startOffset_seconds']}, 'endOffset_seconds': {row['endOffset_seconds']}}}\", axis=1)\n",
    "            \n",
    "        # Now group by 'asset_id' and concatenate the strings in 'time_lines'\n",
    "        time_lines = df.groupby(['asset_id'])['time_lines'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "        \n",
    "        df.drop('time_lines', axis=1, inplace=True)\n",
    "        # Merge the time_lines into the original DataFrame\n",
    "        df = df.merge(time_lines, on=['asset_id'], how='left')\n",
    "    \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption','modelVersion' ])[['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption','modelVersion' ]]\n",
    "            \n",
    "        # Convert datetime to string using astype(str)\n",
    "        df['date_published'] = df['date_published'].astype(str)\n",
    "        df['first_published_timestamp'] = df['first_published_timestamp'].astype(str) \n",
    "        \n",
    "        #set the output\n",
    "        output = df#.to_dict(orient='records') \n",
    " \n",
    "    except Exception as e:\n",
    "        print('error'+str(e))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127d60d-0381-4c2e-960b-aef542cf958b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset= \"vlt_media_embeddings_integration\"\n",
    "content_table=\"vlt_all_media_content_text_embeddings\"\n",
    "project_id='nine-quality-test'\n",
    "df=get_predictions(content_table, dataset,project_id,filter_query=\"\")\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55374f47-f05c-4e66-af6e-ac1f8c578887",
   "metadata": {},
   "source": [
    "### Pick some samples- this is just to have some saving on the costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be9097e9-2e8e-448d-9bcf-41fcfae4ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick 3 random samples\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "items=df.sample(1)\n",
    "items=items[['description',\"avgLogprobs\",\"modelVersion\"]]\n",
    "#set the respective column names for response text and average probabilities logs\n",
    "response_column_name='description' \n",
    "response_avgLogprobs='avgLogprobs'\n",
    "response_modelVersion='modelVersion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8132eed8-14d8-4e34-a03b-039e1b1ff681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from vertexai.evaluation import (\n",
    "    EvalTask, \n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate\n",
    ")\n",
    " \n",
    "import uuid \n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "from datetime import datetime\n",
    "\n",
    "class PointWiseEvaluationClient:\n",
    "    \"\"\"Wrapper around Pointwise Evaluation Client.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project: str=None,\n",
    "        location: str = \"us-central1\",\n",
    "        items: pd.core.frame.DataFrame = None,\n",
    "        response_desc_column_name: str= 'description',\n",
    "        response_llm_model_column_name: str= None,\n",
    "        response_avgLogprobs_column_name: str=None,\n",
    "        eval_metrics: list[dict] =None,\n",
    "        experiment_name: str=\"pointwise-evaluation-experiment\",\n",
    "        evaluation_prompt: str=\"Evaluate the AI's contribution to a meaningful content generation\",\n",
    "        delete_experiment: bool= True,\n",
    "        sys_metrics: bool= True,\n",
    "       \n",
    "        \n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initis the hyper parameters\n",
    "        \n",
    "        Args:\n",
    "         str project:  project id \n",
    "         str locations: project location         \n",
    "         Dataframe items: dataframe of AI-generated responses\n",
    "         str response_desc_column_name: the name of the column in the 'items' dataframe that includes the AI-generated response\n",
    "         str response_llm_model_column_name: the name of the column in the 'items' dataframe that includes the name of the model that is used for extracting AI-generated responses\n",
    "         list[dict] eval_metrics: user defined evaluation metrics along with their rating rubric\n",
    "                                  e.g.  [ {  \"metric\": \"safety\", \"criteria\": \"...\" }]\n",
    "         str experiment_name: name of the evaluation experiment\n",
    "         str evaluation_prompt: the prompt text which will be used as a prompt to evaluate the eval_metrics        \n",
    "         bool delete_experiment: delete the generated experience after the evaluation are done if True. Will save costs.\n",
    "         bool sys_metrics: calculates some mathematical metrics including perplexity, entropy if set to True.\n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        self.location = location  \n",
    "        self.project = project   \n",
    "        self.items =items  \n",
    "        self.eval_metrics=eval_metrics #user defined metrics along with their rubric ratings\n",
    "        self.experiment_name=experiment_name\n",
    "        self.evaluation_prompt=evaluation_prompt\n",
    "        self.response_llm_model_column_name=response_llm_model_column_name\n",
    "        self.response_desc_column_name=response_desc_column_name\n",
    "        self.delete_experiment=delete_experiment\n",
    "        self.response_avgLogprobs_column_name= response_avgLogprobs_column_name\n",
    "        self.sys_metrics=sys_metrics\n",
    "        self.run_experiment_name=self.experiment_name+\"-\"+ str(uuid.uuid4())\n",
    "        \n",
    "        vertexai.init(project=self.project, location= self.location )\n",
    "       \n",
    "  \n",
    "\n",
    "    def set_evaluation_data(self):\n",
    "        \"\"\"\n",
    "        Sets the input data as in a dataframe for evaluation\n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        eval_dataset= pd.DataFrame(\n",
    "                                {\n",
    "                                   # \"instruction\": instructions,\n",
    "                                   # \"context\": contexts,\n",
    "                                    \"response\": self.items[self.response_desc_column_name].to_list(),\n",
    "                                    **({\"avgLogprobs\": self.items[self.response_avgLogprobs_column_name].to_list()} if self.response_avgLogprobs_column_name !=None else {}),\n",
    "                                    \"response_llm_model\":[self.response_llm_model_column_name]*len(self.items),\n",
    "                                    \"run_experiment_name\":[self.run_experiment_name]*len(self.items),\n",
    "                                    \"run_experiment_date\" :  pd.to_datetime( [datetime.today().date()]*len(self.items)).date,\n",
    "\n",
    "                                }\n",
    "                            )\n",
    "         \n",
    "        #eval_dataset['run_experiment_date'] = pd.to_datetime(eval_dataset['run_experiment_date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        return eval_dataset\n",
    "\n",
    "    def log_evaluations(self, result):\n",
    "        \"\"\"\n",
    "        Log the evaluation result into BigQuery, altering the table schema if needed.\n",
    "\n",
    "        Args:\n",
    "            dataframe result : The evaluation result to be recorded into the database.\n",
    "        \"\"\"\n",
    "        # Load configuration from config.json\n",
    "        with open('config.json') as config_file:\n",
    "            config = json.load(config_file)\n",
    "\n",
    "        table_id = config['pointwise_eval_table']\n",
    "        dataset_id = config['eval_dataset']\n",
    "        project_id = config[\"project\"]\n",
    "        location_id=config[\"project_location\"]\n",
    "        table_full_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "        dataset_full_id = f\"{project_id}.{dataset_id}\"\n",
    "\n",
    "        #remove unwanted characters from column name\n",
    "        result.columns = result.columns.str.replace(\"/\", \"-\")\n",
    "\n",
    "        # Initialize BigQuery Client\n",
    "        client = bigquery.Client()\n",
    "\n",
    "\n",
    "        # Ensure the dataset exists\n",
    "        try:\n",
    "            client.get_dataset(dataset_full_id)\n",
    "            print(f\"Dataset {dataset_full_id} exists.\")\n",
    "        except NotFound:\n",
    "            print(f\"Dataset {dataset_full_id} not found. Creating dataset...\")\n",
    "            dataset = bigquery.Dataset(dataset_full_id)\n",
    "            dataset.location = location_id \n",
    "            client.create_dataset(dataset)\n",
    "            print(f\"Dataset {dataset_full_id} created successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Fetch the existing table\n",
    "            table = client.get_table(table_full_id)\n",
    "            existing_schema = {field.name: field.field_type for field in table.schema}\n",
    "            print(f\"Table {table_full_id} exists. Checking schema...\")\n",
    "\n",
    "            # Infer schema from DataFrame\n",
    "            new_schema = {\n",
    "                name: bigquery.enums.SqlTypeNames.DATE if (dtype == 'object'  and name=='run_experiment_date')\n",
    "                else bigquery.enums.SqlTypeNames.STRING if dtype == 'object'\n",
    "                else bigquery.enums.SqlTypeNames.FLOAT if dtype in ['float64', 'float32']\n",
    "                else bigquery.enums.SqlTypeNames.INTEGER if dtype in ['int64', 'int32']\n",
    "                else bigquery.enums.SqlTypeNames.BOOLEAN if dtype == 'bool'\n",
    "                else bigquery.enums.SqlTypeNames.TIMESTAMP if dtype == 'datetime64[ns]'\n",
    "                else bigquery.enums.SqlTypeNames.STRING\n",
    "                for name, dtype in zip(result.columns, result.dtypes)\n",
    "            }\n",
    "\n",
    "            # Identify schema differences\n",
    "            schema_changes = []\n",
    "            for col, dtype in new_schema.items():\n",
    "                if col not in existing_schema:\n",
    "                    # Add new column\n",
    "                    schema_changes.append(bigquery.SchemaField(col, dtype))\n",
    "                elif existing_schema[col] != dtype:\n",
    "                    print(f\"Type change detected for column '{col}' from {existing_schema[col]} to {dtype}.\")\n",
    "                    # BigQuery doesn't allow direct type changes; handle as needed.\n",
    "\n",
    "            if schema_changes:\n",
    "                print(\"Altering schema to add new columns...\")\n",
    "                table.schema = table.schema + schema_changes\n",
    "                table = client.update_table(table, [\"schema\"])\n",
    "                print(f\"Table {table_full_id} schema updated successfully.\")\n",
    "            else:\n",
    "                print(\"Schema is already up-to-date.\")\n",
    "\n",
    "        except NotFound:\n",
    "            print(f\"Table {table_full_id} not found. Creating table...\")\n",
    "            # Infer schema from DataFrame\n",
    "            schema = [\n",
    "                bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.DATE if (dtype == 'object'  and name=='run_experiment_date')\n",
    "                                     else bigquery.enums.SqlTypeNames.STRING if dtype == 'object' \n",
    "                                     else bigquery.enums.SqlTypeNames.FLOAT if dtype in ['float64', 'float32']\n",
    "                                     else bigquery.enums.SqlTypeNames.INTEGER if dtype in ['int64', 'int32']\n",
    "                                     else bigquery.enums.SqlTypeNames.BOOLEAN if dtype == 'bool'\n",
    "                                     else bigquery.enums.SqlTypeNames.TIMESTAMP if dtype == 'datetime64[ns]'\n",
    "                                     else bigquery.enums.SqlTypeNames.STRING)\n",
    "                for name, dtype in zip(result.columns, result.dtypes)\n",
    "            ]\n",
    "\n",
    "            # Create the table\n",
    "            table = bigquery.Table(table_full_id, schema=schema)\n",
    "            table = client.create_table(table)\n",
    "            print(f\"Table {table_full_id} created successfully.\")\n",
    "\n",
    "        # Define job configuration\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        )\n",
    "\n",
    "        # Save DataFrame to BigQuery\n",
    "        job = client.load_table_from_dataframe(result, table_full_id, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "\n",
    "        # Additional error inspection after the job completes\n",
    "        if job.errors:\n",
    "            print(\"The job completed with the following errors:\")\n",
    "            for error in job.errors:\n",
    "                print(f\" - {error['message']}\")\n",
    "        else:\n",
    "            print(f\"Evaluations have successfully been loaded into {table_full_id}.\")\n",
    "\n",
    "    def perpelexity(prob: float):    \n",
    "        \"\"\"Extract perplexity- models confidence in predicting next token using average log probablity\n",
    "\n",
    "          Args:\n",
    "          float prob: average log probability\n",
    "\n",
    "          Returns:\n",
    "          float:  perplexity value\n",
    "\n",
    "          \"\"\"\n",
    "        return math.exp(-prob)\n",
    "    \n",
    "    \n",
    "    def entropy(text: str):\n",
    "        \"\"\"Extracts entropy of a texts, higher entropy means diverse range of tokens have been choosen\n",
    "\n",
    "        Args:\n",
    "        str text: the input text\n",
    "\n",
    "        Returns:\n",
    "        float entropy: entropy value of input text\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize the text into words (ignoring punctuation)\n",
    "        words = text.lower().split()\n",
    "\n",
    "        # Get the frequency of each word\n",
    "        word_count = Counter(words)\n",
    "\n",
    "        # Total number of words\n",
    "        total_words = len(words)\n",
    "\n",
    "        # Calculate the probability of each word\n",
    "        probabilities = [count / total_words for count in word_count.values()]\n",
    "\n",
    "        # Calculate entropy using the formula\n",
    "        entrpy = -sum(p * math.log2(p) for p in probabilities)\n",
    "\n",
    "        return entrpy\n",
    "\n",
    "\n",
    "    def get_evaluations(self):\n",
    "        \"\"\"\n",
    "        Get the evaluations using user defined rating criteria.\n",
    "\n",
    "        \"\"\"\n",
    "        # set evaluation data\n",
    "        eval_dataset=self.set_evaluation_data()\n",
    "        \n",
    "        #calculate the system defined metrics\n",
    "        if self.sys_metrics:\n",
    "            # the evrage prob column is given in the data, calculate perplexity\n",
    "            if self.response_avgLogprobs_column_name:\n",
    "                eval_dataset['perplexity']=eval_dataset[self.response_avgLogprobs_column_name].apply(perpelexity)\n",
    "            \n",
    "            #calculate entropy\n",
    "            eval_dataset['entropy']=eval_dataset[self.response_desc_column_name].apply(entropy)\n",
    "            eval_results=eval_dataset\n",
    "        \n",
    "        #calcualte user defined metrics\n",
    "        if self.eval_metrics:\n",
    "            metrics=[]\n",
    "            # Define  pointwise quality metric(s)\n",
    "            for metric in self.eval_metrics:\n",
    "                # Define a pointwise quality metric\n",
    "                pointwise_quality_metric_prompt = f\"\"\"{self.evaluation_prompt}; evaluate {metric['metric']}.\n",
    "                # Rubric rating criteria\n",
    "                {metric['criteria']}\n",
    "                # AI-generated Response\n",
    "                {{response}}\n",
    "                \"\"\"\n",
    "                pointwise_metric=PointwiseMetric(\n",
    "                    metric=metric['metric'],\n",
    "                    metric_prompt_template=pointwise_quality_metric_prompt,\n",
    "                )\n",
    "                metrics.append(pointwise_metric)\n",
    "\n",
    "\n",
    "\n",
    "            # Create the evaluation task\n",
    "            eval_task = EvalTask(\n",
    "                dataset=eval_dataset,\n",
    "                metrics=metrics,\n",
    "                experiment=self.experiment_name,\n",
    "            )\n",
    "            # Run evaluation on the data using the evaluation service\n",
    "            results = eval_task.evaluate( \n",
    "\n",
    "                    experiment_run_name=self.run_experiment_name,\n",
    "                ) \n",
    "            #Delete the experiment after getting the result\n",
    "            if self.delete_experiment:\n",
    "                experiment = aiplatform.Experiment(self.experiment_name)\n",
    "                experiment.delete()\n",
    "                \n",
    "            eval_results=results.metrics_table\n",
    "            \n",
    "        #log the statistics into bigquery\n",
    "        self.log_evaluations(eval_results)\n",
    "            \n",
    "        return eval_results \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d218e46-7a00-4fb0-8d50-1a6e899d425a",
   "metadata": {},
   "source": [
    "### Load the user defined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c95b135-e309-43f5-aaf6-206e48e6671c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "experiment_name = \"content-generation-qa-quality\"\n",
    "file_path = 'PointWiseEvaluationMetrics.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    eval_metrics = json.load(file)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c0b8d6c-e193-4418-9561-f15646528fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LLM_PointWiseEval_cls import PointWiseEvaluationClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2563a782-60e2-446d-8b51-23692b686d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PointWiseEvaluationClient.__init__() got an unexpected keyword argument 'response_desc_column_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pointwise_evaluation_client\u001b[38;5;241m=\u001b[39m\u001b[43mPointWiseEvaluationClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnine-quality-test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mus-central1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mitems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mresponse_desc_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mresponse_llm_model_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_modelVersion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mresponse_avgLogprobs_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_avgLogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpointwise-evaluation-experiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mevaluation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluate the AI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms contribution to a meaningful content generation. For rating and evaluationte of the response on a 1-5 scale, using the given rubric criteria.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdelete_experiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# to save the costs, delete the evaluation experiment after the evaluation is finished\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msys_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#calculate some mathematical metrics: entropy, perplexity\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m evaluations\u001b[38;5;241m=\u001b[39mpointwise_evaluation_client\u001b[38;5;241m.\u001b[39mget_evaluations()\n",
      "\u001b[0;31mTypeError\u001b[0m: PointWiseEvaluationClient.__init__() got an unexpected keyword argument 'response_desc_column_name'"
     ]
    }
   ],
   "source": [
    "pointwise_evaluation_client=PointWiseEvaluationClient(project='nine-quality-test',\n",
    "                          location='us-central1',\n",
    "                          items=items,\n",
    "                          response_desc_column_name=response_column_name,\n",
    "                          response_llm_model_column_name=response_modelVersion,\n",
    "                          response_avgLogprobs_column_name=response_avgLogprobs,\n",
    "                          eval_metrics=None,\n",
    "                         experiment_name=\"pointwise-evaluation-experiment\",    \n",
    "                         evaluation_prompt= \"Evaluate the AI's contribution to a meaningful content generation. For rating and evaluationte of the response on a 1-5 scale, using the given rubric criteria.\",\n",
    "                         delete_experiment=True, # to save the costs, delete the evaluation experiment after the evaluation is finished\n",
    "                         sys_metrics=True #calculate some mathematical metrics: entropy, perplexity\n",
    "                         )\n",
    "evaluations=pointwise_evaluation_client.get_evaluations()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa3ad9-aad1-4bcf-877c-bd45aff67940",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be984b87-6f61-4d68-ba9f-b7e29e6396c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]\n",
    "%pip install --upgrade --user bigframes -q\n",
    "%pip install --quiet --upgrade nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5bee4-2285-4291-a19b-4c6df1dc9441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4be2a-c689-470f-92b3-0edf282ebc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
