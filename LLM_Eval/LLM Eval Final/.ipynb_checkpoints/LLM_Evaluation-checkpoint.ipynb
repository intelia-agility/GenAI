{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267fab21-d152-4ab1-8769-be85bfe0d7b4",
   "metadata": {},
   "source": [
    "### LLM Evaluation \n",
    "\n",
    "This code uses gcp evaluation service to evaluate the generated content by a generative AI API in terms of \n",
    "- safety and sextural harmness\n",
    "- coherence and fluency\n",
    "- verbosity and repeatation\n",
    "\n",
    "\n",
    "Use PointWiseEvaluationMetrics.json as a json file for the requested metrics and rating rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf841a-47c8-4839-81fb-2111f1114298",
   "metadata": {},
   "source": [
    "### Get data from biquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b85d2e8-3591-4bc2-af77-4e1708b38325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "def get_predictions(table, dataset,project_id,filter_query=\"\"):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for text embeddings.\"\"\"\n",
    "  \n",
    "    sql = f\"\"\"  \n",
    "        WITH SEARCH_RESULT AS\n",
    "         (SELECT \n",
    "\n",
    "                        asset_id, \n",
    "                        content,\n",
    "                        headline,\n",
    "                        html_safe_text,\n",
    "                        description,\n",
    "                        startOffset_seconds,\n",
    "                        endOffset_seconds,\n",
    "                        fileUri,\n",
    "                        asset_type,\n",
    "                        first_published_timestamp,\n",
    "                        brand_type,\n",
    "                        primary_category_name,\n",
    "                        byline,\n",
    "                        image_license_type,\n",
    "                        publisher_type,\n",
    "                        photographer,\n",
    "                        date_published,\n",
    "                        dxcId,\n",
    "                        text_embedding_result ,\n",
    "                        byline[SAFE_OFFSET(0)].author_name ,                    \n",
    "                        CAST(JSON_EXTRACT_SCALAR(media_jsonbody, '$.response.candidates[0].avgLogprobs') AS FLOAT64) AS  avgLogprobs\n",
    "                 FROM  `{dataset}.{table}` WHERE 1=1 and (LOWER(asset_type) LIKE '%video%' OR LOWER(asset_type) LIKE '%image%' ) {filter_query} \n",
    "        ),\n",
    "          IMAGE_CONTEXT AS (\n",
    "                   SELECT\n",
    "                          pd.asset_id,\n",
    "                          plain_text_column,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') AS image_id,\n",
    "                          JSON_EXTRACT_SCALAR(entry, '$.image.caption') AS image_caption\n",
    "                        FROM\n",
    "                          (SELECT\n",
    "                              asset_id,\n",
    "                              plain_text_column,\n",
    "                              JSON_EXTRACT_ARRAY(article_body_json) AS article_body_json_array\n",
    "                            FROM\n",
    "                              `vlt_media_content_prelanding.vlt_article_content` -- change to vlt\n",
    "                            WHERE\n",
    "                              article_body_json IS NOT NULL\n",
    "                          ) pd,\n",
    "                          UNNEST(pd.article_body_json_array) AS entry -- Unnest the article body JSON array\n",
    "                        WHERE\n",
    "                          UPPER(JSON_EXTRACT_SCALAR(entry, '$.type')) = 'IMAGE' -- Filter to only 'IMAGE' type\n",
    "                          AND JSON_EXTRACT_SCALAR(entry, '$.image.mediaId') IS NOT NULL -- Ensure there's an image ID\n",
    "                       \n",
    "          ) \n",
    "        \n",
    "        SELECT sr.*,    plain_text_column as image_context ,  image_caption\n",
    "        FROM SEARCH_RESULT   sr\n",
    "        LEFT JOIN IMAGE_CONTEXT imgcnxt\n",
    "        on REGEXP_REPLACE( sr.asset_id, r'\\..*', '') =imgcnxt.image_id\n",
    "    \"\"\"       \n",
    " ##LOWER(asset_type) LIKE '%image%' OR \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "    output=[]\n",
    "    try:\n",
    "        # Fetch results\n",
    "        results = query_job.result()  \n",
    "        df = results.to_dataframe()\n",
    "       \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "            'startOffset_seconds', 'endOffset_seconds', 'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId','avgLogprobs', 'image_context','image_caption' ])\n",
    "        print(len(df))\n",
    "        # Sort by asset_id and startOffset_seconds to ensure proper order\n",
    "        df = df.sort_values(by=['asset_id', 'startOffset_seconds'])\n",
    "        \n",
    "     \n",
    "        # Aggregate descriptions for each asset_id, ordered by startOffset_seconds\n",
    "        # I dont want to aggregate different time-stamps\n",
    "        #df['description'] = df.groupby('asset_id')['description'].transform(lambda x: '\\n'.join(x))\n",
    "\n",
    "        # Aggregate and concatenate segments for each asset_id\n",
    "        df['time_lines'] = df.apply(\n",
    "            lambda row: f\"{{'startOffset_seconds': {row['startOffset_seconds']}, 'endOffset_seconds': {row['endOffset_seconds']}}}\", axis=1)\n",
    "            \n",
    "        # Now group by 'asset_id' and concatenate the strings in 'time_lines'\n",
    "        time_lines = df.groupby(['asset_id'])['time_lines'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "        \n",
    "        df.drop('time_lines', axis=1, inplace=True)\n",
    "        # Merge the time_lines into the original DataFrame\n",
    "        df = df.merge(time_lines, on=['asset_id'], how='left')\n",
    "    \n",
    "        #drop duplicates\n",
    "        df = df.drop_duplicates(subset=['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption' ])[['asset_id', 'headline', 'description',\n",
    "                'fileUri', 'asset_type',\n",
    "            'first_published_timestamp', 'brand_type', 'primary_category_name',\n",
    "            'author_name', 'image_license_type', 'publisher_type', 'photographer',\n",
    "            'date_published', 'dxcId',  'time_lines','avgLogprobs' ,'image_context','image_caption' ]]\n",
    "            \n",
    "        # Convert datetime to string using astype(str)\n",
    "        df['date_published'] = df['date_published'].astype(str)\n",
    "        df['first_published_timestamp'] = df['first_published_timestamp'].astype(str) \n",
    "        \n",
    "        #set the output\n",
    "        output = df#.to_dict(orient='records') \n",
    " \n",
    "    except Exception as e:\n",
    "        print('error'+str(e))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2127d60d-0381-4c2e-960b-aef542cf958b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568\n"
     ]
    }
   ],
   "source": [
    "dataset= \"vlt_media_embeddings_integration\"\n",
    "content_table=\"vlt_all_media_content_text_embeddings\"\n",
    "project_id='nine-quality-test'\n",
    "df=get_predictions(content_table, dataset,project_id,filter_query=\"\")\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55374f47-f05c-4e66-af6e-ac1f8c578887",
   "metadata": {},
   "source": [
    "### Pick some samples- this is just to have some saving on the costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9097e9-2e8e-448d-9bcf-41fcfae4ec93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>fileUri</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>first_published_timestamp</th>\n",
       "      <th>brand_type</th>\n",
       "      <th>primary_category_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>image_license_type</th>\n",
       "      <th>publisher_type</th>\n",
       "      <th>photographer</th>\n",
       "      <th>date_published</th>\n",
       "      <th>dxcId</th>\n",
       "      <th>time_lines</th>\n",
       "      <th>avgLogprobs</th>\n",
       "      <th>image_context</th>\n",
       "      <th>image_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>0b1dae83b0384856028b5efbcaf29a4866c12e4d.jpeg</td>\n",
       "      <td>None</td>\n",
       "      <td>The image features a man, likely in his 50s or...</td>\n",
       "      <td>gs://nineshowcaseassets/IMAGES/0b1dae83b038485...</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Royalty Free</td>\n",
       "      <td>The Age</td>\n",
       "      <td>Chris Johnson</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>0b1dae83b0384856028b5efbcaf29a4866c12e4d</td>\n",
       "      <td>{'startOffset_seconds': &lt;NA&gt;, 'endOffset_secon...</td>\n",
       "      <td>-0.185971</td>\n",
       "      <td>Swiss authorities have moved quickly to calm f...</td>\n",
       "      <td>The fall comes just one day after Credit Suiss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           asset_id headline  \\\n",
       "1175  0b1dae83b0384856028b5efbcaf29a4866c12e4d.jpeg     None   \n",
       "\n",
       "                                            description  \\\n",
       "1175  The image features a man, likely in his 50s or...   \n",
       "\n",
       "                                                fileUri  asset_type  \\\n",
       "1175  gs://nineshowcaseassets/IMAGES/0b1dae83b038485...  image/jpeg   \n",
       "\n",
       "     first_published_timestamp brand_type primary_category_name author_name  \\\n",
       "1175                       NaT       None                  None        None   \n",
       "\n",
       "     image_license_type publisher_type   photographer date_published  \\\n",
       "1175       Royalty Free        The Age  Chris Johnson     2023-01-23   \n",
       "\n",
       "                                         dxcId  \\\n",
       "1175  0b1dae83b0384856028b5efbcaf29a4866c12e4d   \n",
       "\n",
       "                                             time_lines  avgLogprobs  \\\n",
       "1175  {'startOffset_seconds': <NA>, 'endOffset_secon...    -0.185971   \n",
       "\n",
       "                                          image_context  \\\n",
       "1175  Swiss authorities have moved quickly to calm f...   \n",
       "\n",
       "                                          image_caption  \n",
       "1175  The fall comes just one day after Credit Suiss...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick 3 random samples\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "items=df.sample(1)\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c95b135-e309-43f5-aaf6-206e48e6671c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "experiment_name = \"content-generation-qa-quality\"\n",
    "file_path = 'PointWiseEvaluationMetrics.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    eval_metrics = json.load(file)\n",
    " \n",
    "#AI-generated Responses\n",
    "items=items['description'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0b8d6c-e193-4418-9561-f15646528fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LLM_PointWiseEval_cls import PointWiseEvaluationClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2563a782-60e2-446d-8b51-23692b686d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-319fa5da-a821-4659-bb51-5bba579d66b2\" href=\"#view-view-vertex-resource-319fa5da-a821-4659-bb51-5bba579d66b2\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-319fa5da-a821-4659-bb51-5bba579d66b2');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs?project=nine-quality-test');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs?project=nine-quality-test', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1 to Experiment: pointwise-evaluation-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-64d30219-312d-475a-abbc-5a67e507cfd8\" href=\"#view-view-vertex-resource-64d30219-312d-475a-abbc-5a67e507cfd8\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-64d30219-312d-475a-abbc-5a67e507cfd8');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1?project=nine-quality-test');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/pointwise-evaluation-experiment/runs/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1?project=nine-quality-test', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 3 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3 metric requests are successfully computed.\n",
      "Evaluation Took:5.387495030008722 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nine-quality-test.vlt_eval_statistics_schema exists.\n",
      "Table nine-quality-test.vlt_eval_statistics_schema.vlt_pointwise_eval_statistics exists. Checking schema...\n",
      "Type change detected for column 'run_experiment_date' from STRING to DATE.\n",
      "Schema is already up-to-date.\n",
      "Evaluations have successfully been loaded into nine-quality-test.vlt_eval_statistics_schema.vlt_pointwise_eval_statistics.\n",
      "Experiment run pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1 skipped backing tensorboard run deletion.\n",
      "To delete backing tensorboard run, execute the following:\n",
      "tensorboard_run_artifact = aiplatform.metadata.artifact.Artifact(artifact_name=f\"pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1-tb-run\")\n",
      "tensorboard_run_resource = aiplatform.TensorboardRun(tensorboard_run_artifact.metadata[\"resourceName\"])\n",
      "tensorboard_run_resource.delete()\n",
      "tensorboard_run_artifact.delete()\n",
      "Deleting Context : projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1\n",
      "Context deleted. . Resource name: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1\n",
      "Deleting Context resource: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1\n",
      "Delete Context backing LRO: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1/operations/379915249177853952\n",
      "Context resource projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-4926a293-ea26-4e64-8195-dc849555b0c1 deleted.\n",
      "Experiment run pointwise-evaluation-experiment-a242e306-791c-4f70-b8f9-7aa951a8f6ee skipped backing tensorboard run deletion.\n",
      "To delete backing tensorboard run, execute the following:\n",
      "tensorboard_run_artifact = aiplatform.metadata.artifact.Artifact(artifact_name=f\"pointwise-evaluation-experiment-pointwise-evaluation-experiment-a242e306-791c-4f70-b8f9-7aa951a8f6ee-tb-run\")\n",
      "tensorboard_run_resource = aiplatform.TensorboardRun(tensorboard_run_artifact.metadata[\"resourceName\"])\n",
      "tensorboard_run_resource.delete()\n",
      "tensorboard_run_artifact.delete()\n",
      "Deleting Context : projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-a242e306-791c-4f70-b8f9-7aa951a8f6ee\n",
      "Context deleted. . Resource name: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-a242e306-791c-4f70-b8f9-7aa951a8f6ee\n",
      "Deleting Context resource: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-a242e306-791c-4f70-b8f9-7aa951a8f6ee\n",
      "Delete Context backing LRO: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-a242e306-791c-4f70-b8f9-7aa951a8f6ee/operations/1664567042885287936\n",
      "Context resource projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment-pointwise-evaluation-experiment-a242e306-791c-4f70-b8f9-7aa951a8f6ee deleted.\n",
      "Deleting Context : projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment\n",
      "Context deleted. . Resource name: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment\n",
      "Deleting Context resource: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment\n",
      "Delete Context backing LRO: projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment/operations/3574093284890378240\n",
      "Context resource projects/494586852359/locations/us-central1/metadataStores/default/contexts/pointwise-evaluation-experiment deleted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>response_llm_model</th>\n",
       "      <th>run_experiment_name</th>\n",
       "      <th>run_experiment_date</th>\n",
       "      <th>safety-explanation</th>\n",
       "      <th>safety-score</th>\n",
       "      <th>coherence and fluency-explanation</th>\n",
       "      <th>coherence and fluency-score</th>\n",
       "      <th>verbosity-explanation</th>\n",
       "      <th>verbosity-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The image features a man, likely in his 50s or...</td>\n",
       "      <td>gemini-pro-1.5</td>\n",
       "      <td>pointwise-evaluation-experiment-4926a293-ea26-...</td>\n",
       "      <td>2025-01-21</td>\n",
       "      <td>The response is comprehensive, detailed, and s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The AI response provides a highly descriptive ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is excessively verbose, containin...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response response_llm_model  \\\n",
       "0  The image features a man, likely in his 50s or...     gemini-pro-1.5   \n",
       "\n",
       "                                 run_experiment_name run_experiment_date  \\\n",
       "0  pointwise-evaluation-experiment-4926a293-ea26-...          2025-01-21   \n",
       "\n",
       "                                  safety-explanation  safety-score  \\\n",
       "0  The response is comprehensive, detailed, and s...           5.0   \n",
       "\n",
       "                   coherence and fluency-explanation  \\\n",
       "0  The AI response provides a highly descriptive ...   \n",
       "\n",
       "   coherence and fluency-score  \\\n",
       "0                          5.0   \n",
       "\n",
       "                               verbosity-explanation  verbosity-score  \n",
       "0  The response is excessively verbose, containin...              2.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointwise_evaluation_client=PointWiseEvaluationClient(project='nine-quality-test',\n",
    "                          location='us-central1',\n",
    "                          items=items,\n",
    "                          response_llm_model='gemini-pro-1.5',\n",
    "                          eval_metrics=eval_metrics,\n",
    "                         experiment_name=\"pointwise-evaluation-experiment\",                        \n",
    "                         delete_experiment=True)\n",
    "evaluations=pointwise_evaluation_client.get_evaluations()\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be984b87-6f61-4d68-ba9f-b7e29e6396c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]\n",
    "%pip install --upgrade --user bigframes -q\n",
    "%pip install --quiet --upgrade nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5bee4-2285-4291-a19b-4c6df1dc9441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4be2a-c689-470f-92b3-0edf282ebc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
