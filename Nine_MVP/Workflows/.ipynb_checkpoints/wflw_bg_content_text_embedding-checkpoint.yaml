main:
  params: [input]

  steps:
    # Step 1: Retrieve the bucket name from an environment variable
    - init:
        assign:
            - PROJECT: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - LOCATION: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - CONTENT_METHOD: "batchPredictionJobs" 
            - LLM_API_ENDPOINT: ${"https://" + LOCATION + "-aiplatform.googleapis.com" + "/v1/projects/" + PROJECT + "/locations/" + LOCATION + "/"+CONTENT_METHOD}
            - TEXT_EMBEDDING_MODEL: "publishers/google/models/textembedding-gecko"
			
            - DATASET_ID: "langchain_dataset"
            - TABLE_ID: "chucked_data"
            - METADATA_COLUMNS: "id,content,path,media_type,test_metadata"
            - PAGE_CONTENT_COLUMNS: "content,test_metadata"
            - SOURCE_QUERY: "SELECT id,media_type,content,test_metadata, path  FROM `nine-quality-test.Nine_Quality_Test.test_long_artcles` ;" 
            - FUNC_CHUNK_BG_CONTENT: "https://us-central1-nine-quality-test.cloudfunctions.net/func_chunk_bq_content"
            #- SEPARATORS: "\n"
            - CHUNK_SIZE: 500
            - CHUNK_OVERLAP: 10
            - MAX_PROMPT_COUNT_LIMIT: 2 #30000 #Maximum number of requests per batch; currently, the API has a limit of 30,000 prompts
  
            - BATCH_OUTPUT: []
            
    
    #Step 1: Chunck content and load them into big query table

    - chunk_bq_content:
        call: http.post
        args:
            url: ${FUNC_CHUNK_BG_CONTENT}
            query:
                name: "CHUNK CONTENT"            
                project_id: ${PROJECT}
                dataset: ${DATASET_ID}
                table: ${TABLE_ID} 
                region:  ${LOCATION}              
                metadata_columns:  ${METADATA_COLUMNS}
                page_content_columns: ${PAGE_CONTENT_COLUMNS}
                source_query_str: ${SOURCE_QUERY}
                #separators: ${SEPARATORS}    #we use Langchain's defualt separators        
                chunk_size: ${CHUNK_SIZE}
                chunk_overlap: ${CHUNK_OVERLAP}
                max_prompt_count_limit: ${MAX_PROMPT_COUNT_LIMIT}
        result:  chunking_result_set

    #Step 2: Check if data is loaded into bigquery successfully
    # exit the workflow and return failure even if one load job is faile
    - check_bigquery_load:
        for:
           value: job_id
           in: ${chunking_result_set.body.jobs}
           steps:
                 #Step 3: Get job state
                - get_bigquery_load_job_status:
                    call:  googleapis.bigquery.v2.jobs.get 
                    args:
                        jobId: ${job_id}
                        projectId: ${PROJECT}
                        location : ${LOCATION}     
                    result: load_job_result
                
                 # Step 4: check the if load is done sucessfully for all jobs, otherwise exit
                - checkIfLoadDone: 
                        switch:
                              - condition: ${load_job_result.status.state=="DONE" and "errors" in load_job_result.status}
                                steps:
                                    - return_load_failure:
                                        assign:
                                            - out_result:
                                                output_table: ""
                                                state: "BIGQUERY LOAD FAILED- CHECK THE LOGS"                                          
                                            - BATCH_OUTPUT: ${list.concat(BATCH_OUTPUT,out_result)}
                                    - exit:
                                        return: ${BATCH_OUTPUT}

                              - condition:  ${load_job_result.status.state in ["PENDING", "RUNNING"] and  not( "errors" in load_job_result.status)}
                                steps:
                                    - bg_load_wait:
                                        call: sys.sleep
                                        args:
                                            seconds: 120
                                        next: get_bigquery_load_job_status
                                 
    
    - log_chunking:
        call: sys.log
        args:
          text: ${"Data Is Chunked and Loaded - " + "Count of Created Tables "+ string(chunking_result_set.body.count_of_tables)}
       
    #Step 5: Loop over created tables and send batch requests
    # Each table can only be up to 30,000 request
    - loop_over_request_tables:
        for:
           value: version
           range: [0,1] #${[0, chunking_result_set.body.count_of_tables-1]}
           steps:
                 #Step 6: Set the big query table 
                 
                - set_table_name:
                    assign:
                        - table_name: ${chunking_result_set.body.table_name_prefix+"_"+ string(version)}
                        - bigquery_table: ${"bq://"+PROJECT+"."+DATASET_ID+"."+table_name}
                        - job_name: ${DATASET_ID+"."+table_name}
                    
                # Step 7: Extract text embeddings by running batch prediction job
                - extract_embeddings:        
                            steps:
                                - ask_llm:
                                    call: http.post
                                    args:
                                        url: ${LLM_API_ENDPOINT}
                                        auth:
                                            type: OAuth2
                                        body:
                                        
                                            displayName: ${"PB_JOB_TEXT_EMBEDDING_"+job_name} 
                                            model: ${TEXT_EMBEDDING_MODEL}
                                            inputConfig: {
                                                    instancesFormat:"bigquery",
                                                    bigquerySource:{  
                                                            "inputUri" : "${bigquery_table}"
                                                            }
                                                        }
                                            
                                            outputConfig: {
                                                    predictionsFormat:"bigquery",
                                                    bigqueryDestination:{
                                                        outputUri: "${bigquery_table}"

                                                        }
                                                    }

                                    result: llm_response 

                - log_batch_prediction_job:
                    call: sys.log
                    args:
                        text: ${"Batch Prediction Job Is Created - " +llm_response.body.name}

                # Step 8: Get the job status 
                - get_job_status:    
                    call: googleapis.aiplatform.v1.projects.locations.batchPredictionJobs.get
                    args:
                        name:  ${llm_response.body.name}
                        region: ${LOCATION}     
                    
                    result: listResult
                    
                # Step 9: check the job status if succeeded or failed return
                - checkIfDone: 
                        switch:
                              - condition: ${listResult.state=="JOB_STATE_SUCCEEDED"}
                                steps:
                                    - set_success_out_put:
                                        assign:
                                            - out_result:
                                                output_table: "${listResult.outputInfo.bigqueryOutputTable}"
                                                state: "JOB_STATE_SUCCEEDED"                                          
                                            - BATCH_OUTPUT: ${list.concat(BATCH_OUTPUT,out_result)}
                                next: log_job_execution

                              - condition: ${listResult.state=="JOB_STATE_FAILED"}
                                steps:
                                    - set_fail_out_put:
                                        assign:
                                            - out_result:
                                                output_table: ""
                                                state: "JOB_STATE_FAILED"                                           
                                            - BATCH_OUTPUT: ${list.concat(BATCH_OUTPUT,out_result)} 
                                next: log_job_execution

                # Step 10: if job status is not  succeeded or failed, sleep for 120 seconds and check it again
                - wait:
                        call: sys.sleep
                        args:
                            seconds: 120
                        next: get_job_status

                - log_job_execution:
                        call: sys.log
                        args:
                           text: ${"Batch Prediction Is Finished - " + "Output bigquery "+ out_result.output_table+ " ," + "Job Execution Status "+ out_result.state}
    
    # Step 9: return the result
    - return_result:
        return: ${BATCH_OUTPUT}   

