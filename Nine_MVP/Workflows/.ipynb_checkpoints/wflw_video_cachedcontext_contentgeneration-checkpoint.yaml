### This is the workflow to extract multimodal embeddings from video

main:
  #params: [input]

  steps:
    # Step 1: Initialize variables
    - init:
        assign:
            - PROJECT: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - LOCATION: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - PROJECT_NUMBER : ${sys.get_env("GOOGLE_CLOUD_PROJECT_NUMBER") }
            - SOURCE_BUCKET: ${sys.get_env("VAR_SOURCE_BUCKET")}
            - DESTINATION_BUCKET: ${sys.get_env("VAR_DESTINATION_BUCKET")}

            - CONTENT_MODEL: "gemini-1.5-pro-002"  
            - CONTENT_METHOD: "generateContent" 
            - CONTENT_LLM_ENDPOINT: ${"https://"+  LOCATION +"-aiplatform.googleapis.com/v1beta1/projects/"  + PROJECT+"/locations/" + LOCATION + "/publishers/google/models/"+ CONTENT_MODEL + ":" + CONTENT_METHOD}
             
            - MEDIA_TYPES: "video/mp4" #comma separated string   
            - FUNC_GET_VIDEO_DURATION_ENDPOINT: ${"https://getvideodurationservice-"+PROJECT_NUMBER+".us-central1.run.app/get-video-duration"}
            
            - SOURCE_VIDEO_FOLDER: "VIDEOS"
            - SEGMENT_LENGTH: 600           
            - NO_OF_CONCURRENT_EXECUSTIONS: 10  
            - SLEEP_BETWEEN_VIDEOS: 30 
            - SLEEP_BETWEEN_SEGMENTS: 5
            - TEMPERATURE: 0.5
            - MAX_OUT_TOKENS: 8192
            - TOP_P: 0.8
            - TOP_K: 40


            - VIDEO_CONTENT_FOLDER: "video_batch_prediction_fldr_out" #main folder name 
            - VIDEO_CONTENT_FOLDER_JOB: ${"video_batch_prediction_"+  time.format( sys.now())} #this execution folder name
            - OUTPUT_FILE_NAME: "video_batch_prediction"
            - OUTPUT_DIRECTORY: ${"gs://"+DESTINATION_BUCKET+"/"+VIDEO_CONTENT_FOLDER+"/"+VIDEO_CONTENT_FOLDER_JOB}  
            
            - CACHE_EXPIRY_TIME: ${ time.format( sys.now()+  18000)}  #5 hours     
            - CACHE_MODEL : ${"projects/"+PROJECT+"/locations/"+LOCATION+"/"+ "publishers/google/models/gemini-1.5-pro-002"}
            - CACHED_CONTENT_ENDPOINT: ${"https://"+ LOCATION +"-aiplatform.googleapis.com/v1beta1/projects/"+ PROJECT+"/locations/"+LOCATION+"/cachedContents"}

            - METADATA_DATASET: "vlt_media_content_prelanding"
            - METADATA_TABLE: "vlt_video_metadata"
            - VIDEO_TABLE: "vlt_video_content"
            #- METADATA_ERROR_TABLE: "vlt_image_error_metadata"
            - FUNC_LOAD_MEDIA_METADATA: ${"https://"+LOCATION+"-"+PROJECT+".cloudfunctions.net/func_generate_media_metadata"}
            - OUTPUT_TABLE: ${METADATA_DATASET+"."+VIDEO_TABLE}
            - BATCH_OUTPUT: []
         
            #This will be added to the end of dynamic prompt. As prompt for each segment should be created dynamically.
            - VIDEO_PROMPT_DETAILS: " Focus on key themes, what people are talking about, names of people who appeared and who are discussed and key locations.\
                                       Highlight any brands, company names or logos that you see.\
                                       Give me at least 4000 words describing it. \ 
                                       Do not add any extra text to the description."



    #Step 1: Load media metadata into  big query table
    - load_media_metadata:
        call: http.post
        args:
            url: ${FUNC_LOAD_MEDIA_METADATA}
            query:
                name: "LOAD METADATA"            
                project_id: ${PROJECT}
                dataset: ${METADATA_DATASET}
                table: ${METADATA_TABLE}  # for video we separate metadata table and landing table
                video_landing_table: ${VIDEO_TABLE} 
                region:  ${LOCATION}
                source_bucket:  ${SOURCE_BUCKET}  
                source_folder:  ${SOURCE_VIDEO_FOLDER}  
                media_types:  ${MEDIA_TYPES}                 
              
        result:  metadata_load_result

    #Step 2: Check if data is loaded into bigquery successfully
    # exit the workflow and return failure even if one load job is faile
    - check_bigquery_load:
        for:
           value: job_id
           in: ${metadata_load_result.body.jobs}
           steps:
                 #Step 3: Get job state
                - get_bigquery_load_job_status:
                    call:  googleapis.bigquery.v2.jobs.get 
                    args:
                        jobId: ${job_id}
                        projectId: ${PROJECT}
                        location : ${LOCATION}     
                    result: load_job_result
                
                 # Step 4: check the if load is done sucessfully for all jobs, otherwise exit
                - checkIfLoadDone: 
                        switch:
                              - condition: ${load_job_result.status.state=="DONE" and "errors" in load_job_result.status}
                                steps:
                                    - return_load_failure:
                                        assign:
                                            - out_result:
                                                output_table: ""
                                                state: "BIGQUERY LOAD FAILED- CHECK THE LOGS"                                          
                                            - BATCH_OUTPUT: ${list.concat(BATCH_OUTPUT,out_result)}
                                    - exit:
                                        return: ${BATCH_OUTPUT}

                              - condition:  ${load_job_result.status.state in ["PENDING", "RUNNING"] and  not( "errors" in load_job_result.status)}
                                steps:
                                    - bg_load_wait:
                                        call: sys.sleep
                                        args:
                                            seconds: 30
                                        next: get_bigquery_load_job_status                                 
    
    - log_chunking:
        call: sys.log
        args:
          text: ${"Media Metadata Is Loaded - " + "Count of Created Tables "+ string(metadata_load_result.body.count_of_tables)}



    #Step 3: Get media count
    - get_media_count:
        call:  googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${PROJECT}
          body:
            query: ${"SELECT COUNT(*) AS media_count FROM `"+METADATA_DATASET+"."+METADATA_TABLE+"` LIMIT 1"}
            useLegacySql: false
        result: media_count_response

    #Step 4: Get media count
    - set_media_count:
        assign:
            - media_count: ${int(media_count_response.rows[0].f[0]["v"])-1}
            - media_count_response: null
            - metadata_load_result: null

    - log_media:
        call: sys.log
        args:
          text: ${"Media Count Loaded - " +  string(media_count+1)}


    #Step 5: Check media count. exit if no media
    - checkIfNoMedia: 
        switch:
            - condition: ${media_count<0}
              steps:
                    - exitNoMedia:
                        return: "No Media Found- Check The Media Folder"

    #Step 6: Extract video contents
    - extract_video_contents:
        parallel:
            concurrency_limit: ${NO_OF_CONCURRENT_EXECUSTIONS}
            for:
                value: idx
                range: ${[0, media_count]}    
              
                steps:
                    #Step 6-1: Set vars
                    - init_mdeia_metadata:
                        assign:
                            - metadata_map: {}

                    #Step 6-2: Get media info feom bigquery
                    - get_media_from_bq:
                            call:  googleapis.bigquery.v2.jobs.query
                            args:
                                projectId: ${PROJECT}
                                body:
                                    query: ${"SELECT name,mime_type,gcs_uri,media_name FROM `"+METADATA_DATASET+"."+METADATA_TABLE+"` WHERE idx="+idx+" LIMIT 1"}                      
                                    useLegacySql: false
                            result: bq_response 

                    - log_mediaInfoLoad:
                            call: sys.log
                            args:
                               text: ${"Media Info Loaded - "}

                    #Step 6-3: Set metadata info for current media
                    - set_metadata:
                            for: 
                                value: field
                                index: filed_idx
                                in: ${bq_response.schema.fields}
                                steps:
                                    - set_media_metadata:
                                            assign:                                            
                                                - media_map: 
                                                    "${field.name}" : ${bq_response.rows[0].f[filed_idx]["v"]}
                                                - metadata_map:  ${map.merge(metadata_map, media_map)} 

                    - log_mediaMetadataInfo:
                            call: sys.log
                            args:
                               text: ${"Media Metadata Is Set- "+metadata_map.media_name}  

                     #Step 6-4: Get video duration
                    - get_video_duration:
                        call: http.post
                        args:
                            url: ${FUNC_GET_VIDEO_DURATION_ENDPOINT}
                            headers:
                                Content-Type: "application/json"
                            body:
                                url: ${metadata_map.gcs_uri}

                        result:  video_duration_result

                    - log_video_duration:
                            call: sys.log
                            args:
                               text: ${"Video Duration Is Calcuated- "+metadata_map.media_name +"-"+ string(video_duration_result.body.duration)}  


                     #Step 6-5: Create video context cache
                    - create_media_context_cache:  
                                call: http.post
                                args:
                                    url: ${CACHED_CONTENT_ENDPOINT}
                                    auth:
                                        type: OAuth2
                                    body:

                                        "model":  ${CACHE_MODEL}
                                        "contents": [{
                                            "role": "user",
                                            "parts": [{
                                                "fileData": {
                                                        "mimeType": "${metadata_map.mime_type}",
                                                        "fileUri": "${metadata_map.gcs_uri}" 
                                                }

                                            }]
                                        },
                                        {
                                            "role": "model",
                                            "parts": [{
                                                "text": "Use this video to answer users questions."
                                            }]
                                        }]  
                                        # "ttl": {
                                        #         "seconds":"3600",
                                        #         "nanos":"0"
                                        #         } 
                                        expireTime: "${CACHE_EXPIRY_TIME}"
                                result: llm_cache_response  
                    
                    - log_context_cache:
                            call: sys.log
                            args:
                               text: ${"Context Cache Created - " + metadata_map.media_name} 


                    #Step 6-7: Initialize vars
                    - set_vars_for_video_loop:
                        assign:
                            - prev: 0
                            - video_duration: ${math.floor(video_duration_result.body.duration)} 
                            - cache_id: ${llm_cache_response.body.name}
                            - cached_video_duration: ${math.floor(llm_cache_response.body.usageMetadata.videoDurationSeconds)}
                            - llm_cache_response : null  


                    #Step 6-8: If video duration is exceeding context cache, set it to context cache videoDurationSeconds 
                    - check_video_duration:
                                 switch:
                                    - condition: ${video_duration > cached_video_duration}
                                      steps:
                                            - set_video_duration_to_cached_video_duration:  
                                                assign:
                                                    - video_duration : ${cached_video_duration}


                    #Step 7: Loop over segments and call multimodal embeddings for each
                    - loop_over_video_segments:
                        steps:
                            #Step 7-1: initialize loop variables 
                            - init_vars:
                                assign:                                  
                                      - startOffset: ${prev}
                                      - endOffset:  ${prev+SEGMENT_LENGTH}
                                      - video_prompt: ${"Describe this video for the time period " + string(startOffset)+ " seconds to "+ string(endOffset) +" seconds."+ VIDEO_PROMPT_DETAILS}
                                     
            
                            #Step 7-2: If end offset is exceeding video duration, set it to video duration
                            - check_endOffset:
                                 switch:
                                    - condition: ${endOffset >video_duration}
                                      steps:
                                            - set_endoffset_to_max_duration:  
                                                assign:
                                                    - endOffset : ${video_duration}

                            #Step 7-3: Generate content
                            - generate_content:
                                try:
                                    steps:
                                        #Step 7-3-1: Call content generation
                                        - ask_llm:
                                                call: http.post
                                                args:
                                                    url: ${CONTENT_LLM_ENDPOINT}
                                                    auth:
                                                        type: OAuth2
                                                    body:
                                                    
                                                        {
                                                                "contents": [
                                                                    {
                                                                    "role": user,
                                                                    "parts": [
                                                                        {
                                                                    
                                                                        
                                                                        "fileData": {
                                                                            "mimeType":  "${metadata_map.mime_type}",
                                                                            "fileUri": "${metadata_map.gcs_uri}"
                                                                        } 
                                                                         ,
                                                                        "videoMetadata": {
                                                                                        "startOffset": {
                                                                                        "seconds":  "${startOffset}",
                                                                                        "nanos": 0
                                                                                        },
                                                                                        "endOffset": {
                                                                                        "seconds": "${endOffset}",
                                                                                        "nanos": 0
                                                                                        }
                                                                                }

                                                                        },
                                                                        { "text": "${video_prompt}" }
                                                                    ]
                                                                    }
                                                                ] ,
                                                                
                                                                "safetySettings": [
                                                                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                                                                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                                                                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                                                                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                                                                ],
                                                                "generationConfig": {
                                                                    "temperature": "${TEMPERATURE}",
                                                                    "topP": "${TOP_P}",
                                                                    "topK": "${TOP_K}",                                              
                                                                    "maxOutputTokens": "${MAX_OUT_TOKENS}",
                                                                },

                                                                ,"cachedContent": "${cache_id}"
                                                            
                                                                }                                    
                                                result: llm_response 

                                        - log_llm:
                                                call: sys.log
                                                args:
                                                    text: ${"Content Generated "+metadata_map.media_name +"- From "+ string(startOffset) + " to "+ string(endOffset)   }

                                        # Step 7-3-2: Write the output into GCS
                                        - write_to_gcs:
                                                call: http.post
                                                args:
                                                    url: ${"https://storage.googleapis.com/upload/storage/v1/b/" + DESTINATION_BUCKET + "/o"}
                                                    auth:
                                                            type: OAuth2
                                                    query:
                                                            name: ${VIDEO_CONTENT_FOLDER+"/"+VIDEO_CONTENT_FOLDER_JOB+"/"+metadata_map.media_name+"/"+OUTPUT_FILE_NAME +".json"}                                   
                                                    body:
                                                            {"intervalSec": "${SEGMENT_LENGTH}",
                                                            "startOffsetSec": "${startOffset}",
                                                            "endOffsetSec": "${endOffset}",
                                                            "duration": "${video_duration}",
                                                            "gcs_uri": "${metadata_map.gcs_uri}",                                         
                                                            "llm_response": "${llm_response.body}"
                                                           }


                                        - log_wrtite_to_gcs:
                                                call: sys.log
                                                args:
                                                    text: ${"Data Is Written In to GCS For- " + metadata_map.media_name +"- From "+ string(startOffset) + " to "+ string(endOffset)} 
                                               
                                         # Step 7-3-4: Set record to insert write the output into bq as well                                    
                                        - set_row_to_insert:
                                            assign:                                         
                                                - bigquery_row:
                                                    gcs_uri: ${metadata_map.gcs_uri}
                                                    mime_type:  ${metadata_map.mime_type}
                                                    name: ${metadata_map.name}
                                                    media_name: ${metadata_map.media_name}
                                                    predictions: ${json.encode_to_string(llm_response.body)}                                                 
                                                    start_offset: ${startOffset}
                                                    end_offset: ${endOffset}
                                                    time: "${time.format(sys.now())}"
                                      
                                        # Step 7-3-5: Write the output into bq as well
                                        - load_data_into_bq:
                                            call: googleapis.bigquery.v2.tabledata.insertAll
                                            args:
                                                datasetId:  ${METADATA_DATASET}
                                                projectId: ${PROJECT}
                                                tableId: ${VIDEO_TABLE} 
                                                body:                                                    
                                                    rows:
                                                        json: ${bigquery_row}
                                            result: insertResult

                                                
                                        - log_wrtite_to_bq:
                                                call: sys.log
                                                args:
                                                    text: ${"Data Is Written In to Bigquery Table - " + VIDEO_TABLE +"-"+metadata_map.media_name+"- From "+ string(startOffset) + " to "+ string(endOffset)} 

                                except:
                                    as: error
                                    steps:
                                        
                                            - logerror:
                                                    call: sys.log
                                                    args:
                                                        text: ${error}

                                          
                                            # # Step 7-4: Set record to write the errors into bq as well
                                            - set_error_row_to_insert:
                                                assign:                                         
                                                    - bigquery_row:
                                                        gcs_uri: ${metadata_map.gcs_uri}
                                                        mime_type:  ${metadata_map.mime_type}
                                                        name: ${metadata_map.name}
                                                        media_name: ${metadata_map.media_name} 
                                                        start_offset: ${startOffset}
                                                        end_offset: ${endOffset}
                                                        error: ${json.encode_to_string(error)}
                                                        time: "${time.format(sys.now())}"
                                                    
                                            # # Step 7-5: Write the errors into bq as well
                                            - load_errors_into_bq:
                                                call: googleapis.bigquery.v2.tabledata.insertAll
                                                args:
                                                    datasetId:  ${METADATA_DATASET}
                                                    projectId: ${PROJECT}
                                                    tableId: ${VIDEO_TABLE} 
                                                    body:                                                    
                                                        rows:
                                                            json: ${bigquery_row}
                                                result: insertResult
                                                
                                            - log_wrtite_error_to_bq:
                                                    call: sys.log
                                                    args:
                                                        text: ${"Error Is Logged In to Bigquery Table - " + VIDEO_TABLE +"-"+metadata_map.media_name+"- From "+ string(startOffset) + " to "+ string(endOffset)}
                                   
                            # Step 7-5: Initialize vars to move to next segment
                            - assign_vars_move_next_segment:  
                                    assign:                                
                                      - prev: ${endOffset}


                            # Step 7-7: We can not constantly call the enpoint, wait after each call
                            - wait_between_segments:
                                    call: sys.sleep
                                    args:
                                        seconds: ${SLEEP_BETWEEN_SEGMENTS}

                            # Step 7-6: Check if there is any further segments left, if yes loop again 
                            - check_endOffset_move_next_segment:
                                 switch:
                                    - condition: ${endOffset <video_duration}
                                      next: loop_over_video_segments
                                      
                    - log_move_to_next_video:
                            call: sys.log
                            args:
                                text: ${"Embedding Is Calculated For - " + metadata_map.media_name}                     

                     # Step 8: We can not constantly call the enpoint, wait after each call
                    - wait_between_images:
                            call: sys.sleep
                            args:
                                seconds: ${SLEEP_BETWEEN_VIDEOS}

    # Step 9: Return output
    - return_result:
        return: [ {"status": "SUCCESS", "output_directory": "${OUTPUT_DIRECTORY}", "output_table":"${OUTPUT_TABLE}" }]