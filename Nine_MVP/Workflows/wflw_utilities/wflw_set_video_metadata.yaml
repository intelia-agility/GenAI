# This is the main workflow to generate video metadata

main:
  params: [input] 
  steps:
    # Step 1: initialize varibales
    - init:
        assign:
           
            - PROJECT: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - LOCATION: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - PROJECT_NUMBER : ${sys.get_env("GOOGLE_CLOUD_PROJECT_NUMBER") }

            - SOURCE_BUCKET: ${input.SOURCE_BUCKET}  
            - SOURCE_VIDEO_FOLDER: ${input.SOURCE_VIDEO_FOLDER}         
            - MEDIA_TYPES: ${input.MEDIA_TYPES}   

            - FUNC_LOAD_MEDIA_METADATA: ${"https://"+LOCATION+"-"+PROJECT+".cloudfunctions.net/func_generate_media_metadata"}
            - FUNC_GET_VIDEO_DURATION_ENDPOINT: ${"https://getvideodurationservice-"+PROJECT_NUMBER+".us-central1.run.app/get-video-duration"}

            - REMOTE_CONN_MODEL_GCS_ID:  ${input.REMOTE_CONN_MODEL_GCS_ID}    
            - METADATA_DATASET:  ${input.METADATA_DATASET}  
            - METADATA_TABLE: ${input.METADATA_TABLE}           
            - BATCH_OUTPUT: [] 
            #- NO_OF_CONCURRENT_EXECUSTIONS: 5 

            # - PROJECT: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            # - LOCATION: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            # - PROJECT_NUMBER : ${sys.get_env("GOOGLE_CLOUD_PROJECT_NUMBER") }

            # - SOURCE_BUCKET:  "raw_nine_files" #${sys.get_env("VAR_SOURCE_BUCKET")} 
            # - SOURCE_VIDEO_FOLDER: "vlt_video_extract"          
            # - MEDIA_TYPES: "video/mp4" #comma separated string of mime_types

            # - FUNC_LOAD_MEDIA_METADATA: ${"https://"+LOCATION+"-"+PROJECT+".cloudfunctions.net/func_generate_media_metadata"}
            # - FUNC_GET_VIDEO_DURATION_ENDPOINT: ${"https://getvideodurationservice-"+PROJECT_NUMBER+".us-central1.run.app/get-video-duration"}

            # - REMOTE_CONN_MODEL_GCS_ID: "vlt_remote_cnn_mdls_bglk" 
            # - METADATA_DATASET: "vlt_media_metadata_prelanding"
            # - METADATA_TABLE: "vlt_video_metadata"           
            # - BATCH_OUTPUT: [] 
            # - NO_OF_CONCURRENT_EXECUSTIONS: 10  


     #Step 1: Load media metadata into  big query table
    - load_media_metadata:
        call: http.post
        args:
            url: ${FUNC_LOAD_MEDIA_METADATA}
            query:
                name: "LOAD METADATA"            
                project_id: ${PROJECT}
                dataset: ${METADATA_DATASET}
                table: ${METADATA_TABLE}              
                region:  ${LOCATION}
                source_bucket:  ${SOURCE_BUCKET}  
                source_folder:  ${SOURCE_VIDEO_FOLDER}  
                media_types:  ${MEDIA_TYPES}                 
              
        result:  metadata_load_result

    #Step 2: Check if data is loaded into bigquery successfully
    # exit the workflow and return failure even if one load job is faile
    - check_bigquery_load:
        for:
           value: job_id
           in: ${metadata_load_result.body.jobs}
           steps:
               #Step 2-1: Get batch status
                - get_bigquery_load_job_status:
                    call:  googleapis.workflowexecutions.v1beta.projects.locations.workflows.executions.run
                    args:
                        workflow_id: "wflw_bg_job_check"
                        location: ${LOCATION}
                        project_id: ${PROJECT}
                        argument: {
                                            job_id:  "${job_id}",
                                            project: "${PROJECT}",
                                            location: "${LOCATION}" 
                                            
                                            }    
                    result: load_job_result

                # Step 2-2: check the if embeddings are extracted successfully
                # in the case of failure, exit
                - checkIfLoadDone: 
                    switch:
                        - condition: ${load_job_result.state =="JOB_FAILED"}
                          return: {"state": "Bigquery Metadata Load Failed- CHECK THE LOGS"    }
                    
    
    - log_chunking:
        call: sys.log
        args:
          text: ${"Media Metadata Is Loaded - " + "Count of Created Tables "+ string(metadata_load_result.body.count_of_tables)}


    #Step 3: Get media count
    - get_media_count:
        call:  googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${PROJECT}
          body:
            query: ${"SELECT COUNT(*) AS media_count FROM `"+METADATA_DATASET+"."+METADATA_TABLE+"` LIMIT 1"}
            useLegacySql: false
        result: media_count_response

    #Step 4: Get media count
    - set_media_count:
        assign:
            - media_count: ${int(media_count_response.rows[0].f[0]["v"])-1}
            - media_count_response: null
            - metadata_load_result: null

    - log_media:
        call: sys.log
        args:
          text: ${"Media Count Loaded - " +  string(media_count+1)}


    #Step 5: Check media count. exit if no media
    - checkIfNoMedia: 
        switch:
            - condition: ${media_count<0}
              steps:
                    - exitNoMedia:
                        return: "No Media Found- Check The Media Folder"

    #Step 6: Extract video contents
    - update_video_duration_metadata:
        #parallel: #parallel execution gives error when concurrent update hapens
            #concurrency_limit: ${NO_OF_CONCURRENT_EXECUSTIONS}
            for:
                value: idx
                range: ${[0, media_count]}    
              
                steps:
                    #Step 6-1: Set vars
                    - init_mdeia_metadata:
                        assign:
                            - metadata_map: {}

                    #Step 6-2: Get media info feom bigquery
                    - get_media_from_bq:
                            call:  googleapis.bigquery.v2.jobs.query
                            args:
                                projectId: ${PROJECT}
                                body:
                                    query: ${"SELECT gcs_uri,media_name FROM `"+METADATA_DATASET+"."+METADATA_TABLE+"` WHERE idx="+idx+" LIMIT 1"}                      
                                    useLegacySql: false
                            result: bq_response 

                    #Step 6-3: Set metadata info for current media
                    - set_metadata:
                            for: 
                                value: field
                                index: filed_idx
                                in: ${bq_response.schema.fields}
                                steps:
                                    - set_media_metadata:
                                            assign:                                            
                                                - media_map: 
                                                    "${field.name}" : ${bq_response.rows[0].f[filed_idx]["v"]}
                                                - metadata_map:  ${map.merge(metadata_map, media_map)} 

                     #Step 6-4: Get video duration
                    - get_video_duration:
                        call: http.post
                        args:
                            url: ${FUNC_GET_VIDEO_DURATION_ENDPOINT}
                            headers:
                                Content-Type: "application/json"
                            body:
                                url: ${metadata_map.gcs_uri}

                        result:  video_duration_result

                    - log_video_duration:
                            call: sys.log
                            args:
                               text: ${"Video Duration Is Calcuated- "+metadata_map.media_name +"-"+ string(video_duration_result.body.duration)} 
 
                    # Step 6-5: Update video's duration
                    - update_data_into_bq:
                            call: googleapis.bigquery.v2.jobs.insert
                            args:
                                projectId: ${PROJECT}
                                body:
                                    configuration:
                                        query:
                                            query:   ${"UPDATE `"+METADATA_DATASET+"."+ METADATA_TABLE+"` SET duration="+ math.floor(video_duration_result.body.duration)+"   WHERE gcs_uri = "+"'"+metadata_map.gcs_uri +"'"}
                                            useLegacySql: false  
                               
                            result: queryJob

                    #Step  6-6: Get update job status
                    - get_bigquery_update_job_status:
                        call:  googleapis.workflowexecutions.v1beta.projects.locations.workflows.executions.run
                        args:
                            workflow_id: "wflw_bg_job_check"
                            location: ${LOCATION}
                            project_id: ${PROJECT}
                            argument: {
                                                job_id:  "${queryJob.jobReference.jobId}",
                                                project: "${PROJECT}",
                                                location: "${LOCATION}" 
                                                
                                                }    
                        result: load_job_result

                    # Step 6-7: check the if embeddings are extracted successfully
                    # in the case of failure, exit
                    - checkIfUpdateDone: 
                        switch:
                            - condition: ${load_job_result.state =="JOB_FAILED"}
                              steps:
                                - set_failure_output:                                 
                                    raise:  "Bigquery Video Duration Update Failed- CHECK THE LOGS"

    - log_wrtite_to_bq:
        call: sys.log
        args:
            text: ${"VIDEO DURATIONS CALCULATED AND STORED- " + METADATA_TABLE } 

    - set_success_output:
            assign:
                - output:  {"state": "SUCCESS"}

    - return_result:
        return : ${output}