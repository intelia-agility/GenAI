# This is the main workflow to generate video contents in batch
#

main:
  steps:
    # Step 1: initialize varibales
    - init:
        assign:
            - PROJECT: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - LOCATION: "us-central1"
 
            
            - CONTENT_METHOD: "batchPredictionJobs" 
            - CONTENT_LLM_API_ENDPOINT: ${"https://" + LOCATION + "-aiplatform.googleapis.com" + "/v1/projects/" + PROJECT + "/locations/" + LOCATION + "/"+CONTENT_METHOD}
            - MODEL: "publishers/google/models/gemini-1.5-flash-002"
            - MEDIA_TYPES: ['video/mp4'] 
            - SOURCE_BUCKET: "raw_nine_files"
            - DESTINATION_BUCKET: "artifacts-nine-quality-test-embeddings"
            - FUNC_CREATE_GCS_BATCH_REQUEST: ${"https://"+LOCATION+"-"+PROJECT+".cloudfunctions.net/func_create_batchrequest_file"}
            - REQUEST_FILE_PREFIX: "video_request"
            - REQUEST_FOLDER: "video_batch_request_fldr"
            - BATCH_PREDICTION_FOLDER: "video_batch_prediction_fldr_out"
            
            - OUTPUT_SCHEMA:
                            {
                                "description": "A list of chapters",
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "startOffset": {
                                            "type": "integer"
                                        },
                                        "endOffset": {
                                            "type": "integer"
                                        },
                                        "Content": {
                                            "type": "string"
                                        }
                                    },
                                    "required": ["startOffset","endOffset","Content" ]
                                }
                            }

            - IMAGE_PROMPT_TEXT: ""

            - TEMPERATURE: 0.5
            - MAX_OUT_TOKENS: 2048
            - TOP_P: 0.8
            - TOP_K: 40

    # Step 2: List objects in the GCS bucket
    - list_files:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${SOURCE_BUCKET}
        result: list_result
 
   
    # Step 3: Filter only videos and create json batch requests  
    - create_batch_json_request:
        steps:
           - assignList:
              assign: 
                  - filetered_files: []
                  - batch_requests: []
           - loopList:
              for:
                  value: item
                  in: ${list_result.items}
                  steps:
                      - check_matching: 
                          switch:                        
                            - condition: ${item.contentType in MEDIA_TYPES}  
                              steps:
                                        - add_matching_item_to_filetered_list:
                                            assign:                                             
                                                - video_uri: ${"gs://"+SOURCE_BUCKET+"/"+item.name} 
                                                - video_uri_map: {"gcs_uri": "${video_uri}"}                                    
                                                - item: ${map.merge(item, video_uri_map)} 
                                                - intervals: ${string(120)}
                                                - startOffset: ${string(120)}
                                                - endOffset: ${string(240)}
                                                - json_schema: ${json.encode_to_string (OUTPUT_SCHEMA)}                                            
                                                - gimini_video_prompt: ${"You are an assistant tasked with describing videos for retrieval.These descriptions will be embedded and used to retrieve the raw video.\n
                                                                        Chapterize the video content by grouping the video content into chapters with intervals of "+intervals +" seconds and providing a concise detail for each chapter that is well optimized for retrieval.\n"}
                                                - gimini_video_prompt: ${gimini_video_prompt + "If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\n 
                                                                        Describe important scenes in the video concisely. If you are not sure about any info, please do not make it up. \n
                                                                        Only consider video from " +startOffset+ " seconds to "+ endOffset +" seconds. \n"}
                                                - gimini_video_prompt: ${gimini_video_prompt + "If it is the last chapter, set the endOffset to "+ endOffset +" instead.\n 
                                                                        For result, follow JSON schema.<JSONSchema>" +json_schema +"</JSONSchema>"}

                                                - json_item:
                                                    request:
                                                        contents: {
                                                                "parts": [
                                                                    {
                                                                    "fileData": {
                                                                        "fileUri": "gs://raw_nine_files/60MI23_33_A_HBB.mp4",
                                                                        "mimeType": "video/mp4"
                                                                    },
                                                                    "videoMetadata": {
                                                                        "endOffset": {
                                                                        "nanos": 0,
                                                                        "seconds": 240
                                                                        },
                                                                        "startOffset": {
                                                                        "nanos": 0,
                                                                        "seconds": 120
                                                                        }
                                                                    }

                                                                    },
                                                                    {
                                                                    "text": "${gimini_video_prompt}"
                                                                    } 
                                                                    # {
                                                                    # "videoMetadata": {
                                                                    #     "endOffset": {
                                                                    #     "nanos": 0,
                                                                    #     "seconds": 240
                                                                    #     },
                                                                    #     "startOffset": {
                                                                    #     "nanos": 0,
                                                                    #     "seconds": 120
                                                                    #     }
                                                                    # }
                                                                    # }
                                                                ],
                                                                "role": "user"
                                                                }                                                           
                                                    
                                                        generation_config:
                                                            temperature: ${TEMPERATURE}
                                                            max_output_tokens: ${MAX_OUT_TOKENS}
                                                            top_p: ${TOP_P}
                                                            top_k: ${TOP_K}
                                                        safety_settings: 
                                                            [   
                                                                {category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                                                                  threshold: "BLOCK_NONE"
                                                                },
                                                                {category: "HARM_CATEGORY_HATE_SPEECH",
                                                                  threshold: "BLOCK_NONE"
                                                                },
                                                                 {category: "HARM_CATEGORY_HARASSMENT",
                                                                  threshold: "BLOCK_NONE"
                                                                },
                                                                 {category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                                                                  threshold: "BLOCK_NONE"
                                                                }
                                                            ] 
                                               
                                                - batch_requests: ${list.concat(batch_requests, json_item)}
  
    - test:
        assign:
            - batch_requests: ${[batch_requests[0]]}
            
    # Step 4: Encode batch request as a json     
    - encode_batch_requests:
        assign:
          - encoded_request: ${json.encode(batch_requests)}     
    
    # Step 5: Create batch request file in gcs
    - genearte_batch_request_file:
        call: http.post
        args:
            url: ${FUNC_CREATE_GCS_BATCH_REQUEST}
            query:
                name: "CREATE BATCH REQUEST FILE"            
                bucket: ${DESTINATION_BUCKET}
                request_file_prefix: ${REQUEST_FILE_PREFIX}
                request_body:   ${ text.decode(encoded_request, "UTF-8")} 
                request_file_folder: ${REQUEST_FOLDER}
        result:  request_file_res

    # Step 6: Set request/response file name/location
    - set_request_response_locations:
        assign:
            - source_file: ${"gs://"+DESTINATION_BUCKET+"/"+request_file_res.body.request_file_name}
            - destination_folder: ${"gs://"+DESTINATION_BUCKET+"/"+BATCH_PREDICTION_FOLDER}

    # Step 7: Extract image contents by sending a batch prediction job                             
    - create_batch_request_job:        
                steps:
                    - ask_llm:
                        call: http.post
                        args:
                            url: ${CONTENT_LLM_API_ENDPOINT}
                            auth:
                                type: OAuth2
                            body:
                               
                                displayName: "BP_JOB_IMAGE_CONTENT_GENERATOR"
                                model: ${MODEL}
 
                                inputConfig: {
                                        instancesFormat:"jsonl",
                                        gcsSource:{  
                                                uris : "${source_file}"
                                                 }
                                            }
                                 
                                outputConfig: {
                                        predictionsFormat:"jsonl",
                                        gcsDestination:{
                                             outputUriPrefix: "${destination_folder}"
                                              }
                                          }
                                          
                        result: llm_response 

    # Step 8: Get the job status 
    - get_job_status:    
        call: googleapis.aiplatform.v1.projects.locations.batchPredictionJobs.get
        args:
            name:  ${llm_response.body.name}
            region: ${LOCATION}     
       
        result: listResult
    
    # Step 9: check the job status if succeeded or failed return
    - checkIfDone:
            switch:
              - condition: ${listResult.state=="JOB_STATE_SUCCEEDED"}
                return: {output_dir: "${listResult.outputInfo.gcsOutputDirectory}", state:"JOB_STATE_SUCCEEDED" } 

              - condition: ${listResult.state=="JOB_STATE_FAILED"}
                return:  {output_dir: "", state:"JOB_STATE_FAILED" } 

    # Step 10: if job status is not  succeeded or failed, sleep for 120 seconds and check it again
    - wait:
            call: sys.sleep
            args:
                seconds: 120
            next: get_job_status
 
    - return_result:
        return: ${batch_requests} 
 


 
