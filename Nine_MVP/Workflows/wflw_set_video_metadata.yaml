# This is the main workflow to generate video metadata

main:
  params: [input] 
  steps:
    # Step 1: initialize varibales
    - init:
        assign:
            # - PROJECT: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            # - LOCATION: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            # - PROJECT_NUMBER : ${sys.get_env("GOOGLE_CLOUD_PROJECT_NUMBER") }

            # - SOURCE_BUCKET:  "raw_nine_files" #${sys.get_env("VAR_SOURCE_BUCKET")} 
            # - SOURCE_VIDEO_FOLDER: "vlt_video_extract"          
            # - MEDIA_TYPES: "video/mp4" #comma separated string of mime_types

            # - FUNC_LOAD_MEDIA_METADATA: ${"https://"+LOCATION+"-"+PROJECT+".cloudfunctions.net/func_generate_media_metadata"}
            # - FUNC_GET_VIDEO_DURATION_ENDPOINT: ${"https://getvideodurationservice-"+PROJECT_NUMBER+".us-central1.run.app/get-video-duration"}

            # - REMOTE_CONN_MODEL_GCS_ID: "vlt_remote_cnn_mdls_bglk" 
            # - METADATA_DATASET: "vlt_media_metadata_prelanding"
            # - METADATA_TABLE: "vlt_video_metadata"           
            # - BATCH_OUTPUT: [] 
            # #- NO_OF_CONCURRENT_EXECUSTIONS: 5  

            - PROJECT: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - LOCATION: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - PROJECT_NUMBER : ${sys.get_env("GOOGLE_CLOUD_PROJECT_NUMBER") }

            - SOURCE_BUCKET: ${input.SOURCE_BUCKET}  
            - SOURCE_VIDEO_FOLDER: ${input.SOURCE_VIDEO_FOLDER}         
            - MEDIA_TYPES: ${input.MEDIA_TYPES}   

            - FUNC_LOAD_MEDIA_METADATA: ${"https://"+LOCATION+"-"+PROJECT+".cloudfunctions.net/func_generate_media_metadata"}
            - FUNC_GET_VIDEO_DURATION_ENDPOINT: ${"https://getvideodurationservice-"+PROJECT_NUMBER+".us-central1.run.app/get-video-duration"}

            - REMOTE_CONN_MODEL_GCS_ID:  ${input.REMOTE_CONN_MODEL_GCS_ID}    
            - METADATA_DATASET:  ${input.METADATA_DATASET}  
            - METADATA_TABLE: ${input.METADATA_TABLE}           
            - BATCH_OUTPUT: [] 
            #- NO_OF_CONCURRENT_EXECUSTIONS: 5  


     #Step 1: Load media metadata into  big query table
    - load_media_metadata:
        call: http.post
        args:
            url: ${FUNC_LOAD_MEDIA_METADATA}
            query:
                name: "LOAD METADATA"            
                project_id: ${PROJECT}
                dataset: ${METADATA_DATASET}
                table: ${METADATA_TABLE}              
                region:  ${LOCATION}
                source_bucket:  ${SOURCE_BUCKET}  
                source_folder:  ${SOURCE_VIDEO_FOLDER}  
                media_types:  ${MEDIA_TYPES}                 
              
        result:  metadata_load_result

    #Step 2: Check if data is loaded into bigquery successfully
    # exit the workflow and return failure even if one load job is faile
    - check_bigquery_load:
        for:
           value: job_id
           in: ${metadata_load_result.body.jobs}
           steps:
                 #Step 3: Get job state
                - get_bigquery_load_job_status:
                    call:  googleapis.bigquery.v2.jobs.get 
                    args:
                        jobId: ${job_id}
                        projectId: ${PROJECT}
                        location : ${LOCATION}     
                    result: load_job_result
                
                 # Step 4: check the if load is done sucessfully for all jobs, otherwise exit
                - checkIfLoadDone: 
                        switch:
                              - condition: ${load_job_result.status.state=="DONE" and "errors" in load_job_result.status}
                                steps:
                                    - return_load_failure:
                                        assign:
                                            - out_result:
                                                output_table: ""
                                                state: "BIGQUERY LOAD FAILED- CHECK THE LOGS"                                          
                                            - BATCH_OUTPUT: ${list.concat(BATCH_OUTPUT,out_result)}
                                    - exit:
                                        return: ${BATCH_OUTPUT}

                              - condition:  ${load_job_result.status.state in ["PENDING", "RUNNING"] and  not( "errors" in load_job_result.status)}
                                steps:
                                    - bg_load_wait:
                                        call: sys.sleep
                                        args:
                                            seconds: 30
                                        next: get_bigquery_load_job_status                                 
    
    - log_chunking:
        call: sys.log
        args:
          text: ${"Media Metadata Is Loaded - " + "Count of Created Tables "+ string(metadata_load_result.body.count_of_tables)}


    #Step 3: Get media count
    - get_media_count:
        call:  googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${PROJECT}
          body:
            query: ${"SELECT COUNT(*) AS media_count FROM `"+METADATA_DATASET+"."+METADATA_TABLE+"` LIMIT 1"}
            useLegacySql: false
        result: media_count_response

    #Step 4: Get media count
    - set_media_count:
        assign:
            - media_count: ${int(media_count_response.rows[0].f[0]["v"])-1}
            - media_count_response: null
            - metadata_load_result: null

    - log_media:
        call: sys.log
        args:
          text: ${"Media Count Loaded - " +  string(media_count+1)}


    #Step 5: Check media count. exit if no media
    - checkIfNoMedia: 
        switch:
            - condition: ${media_count<0}
              steps:
                    - exitNoMedia:
                        return: "No Media Found- Check The Media Folder"

    #Step 6: Extract video contents
    - update_video_duration_metadata:
        #parallel: #parallel execution gives error when concurrent update hapens
            #concurrency_limit: ${NO_OF_CONCURRENT_EXECUSTIONS}
            for:
                value: idx
                range: ${[0, media_count]}    
              
                steps:
                    #Step 6-1: Set vars
                    - init_mdeia_metadata:
                        assign:
                            - metadata_map: {}

                    #Step 6-2: Get media info feom bigquery
                    - get_media_from_bq:
                            call:  googleapis.bigquery.v2.jobs.query
                            args:
                                projectId: ${PROJECT}
                                body:
                                    query: ${"SELECT gcs_uri,media_name FROM `"+METADATA_DATASET+"."+METADATA_TABLE+"` WHERE idx="+idx+" LIMIT 1"}                      
                                    useLegacySql: false
                            result: bq_response 

                    #Step 6-3: Set metadata info for current media
                    - set_metadata:
                            for: 
                                value: field
                                index: filed_idx
                                in: ${bq_response.schema.fields}
                                steps:
                                    - set_media_metadata:
                                            assign:                                            
                                                - media_map: 
                                                    "${field.name}" : ${bq_response.rows[0].f[filed_idx]["v"]}
                                                - metadata_map:  ${map.merge(metadata_map, media_map)} 

                     #Step 6-4: Get video duration
                    - get_video_duration:
                        call: http.post
                        args:
                            url: ${FUNC_GET_VIDEO_DURATION_ENDPOINT}
                            headers:
                                Content-Type: "application/json"
                            body:
                                url: ${metadata_map.gcs_uri}

                        result:  video_duration_result

                    - log_video_duration:
                            call: sys.log
                            args:
                               text: ${"Video Duration Is Calcuated- "+metadata_map.media_name +"-"+ string(video_duration_result.body.duration)}  
 
                    # Step 7-4: Write the output into bq as well
                    - load_data_into_bq:
                            call: googleapis.bigquery.v2.jobs.query 
                            args:
                                projectId: ${PROJECT}
                                body:
                                               
                                    query: ${"UPDATE `"+METADATA_DATASET+"."+ METADATA_TABLE+"` SET duration="+ math.floor(video_duration_result.body.duration)+"   WHERE gcs_uri = "+"'"+metadata_map.gcs_uri +"'"}
                                    useLegacySql: false                                      
                                #result: queryJob

                    # - wait:
                    #         call: sys.sleep
                    #         args:
                    #             seconds: 10

                                        
    - log_wrtite_to_bq:
        call: sys.log
        args:
            text: ${"VIDEO DURATIONS CALCULATED AND STORED- " + METADATA_TABLE } 

    - return_result:
        return : {"state": "SUCCESS"}