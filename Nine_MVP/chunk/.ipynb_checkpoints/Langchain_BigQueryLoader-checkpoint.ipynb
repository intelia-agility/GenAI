{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835b64-eab5-4b12-8cb8-946b554ab1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8b4a0-9336-4ca8-8483-4ba688f4628c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db12b4e8-a141-4be8-897b-23d9cfc1d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.147.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-google-vertexai \"langchain-google-community[featurestore]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccbd0b78-00e8-47a4-aa70-0b9eba58f44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import google.cloud.bigquery as bq\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader,BigQueryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "from datetime import datetime\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "857e7fd4-aeae-445d-af46-0e1dfa8f6e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_710571/1683864506.py:8: LangChainDeprecationWarning: The class `BigQueryLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import BigQueryLoader``.\n",
      "  loader = BigQueryLoader(\n"
     ]
    }
   ],
   "source": [
    "# Define our query\n",
    "query = \"\"\"\n",
    "SELECT id,media_type,content,test_metadata \n",
    "FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "loader = BigQueryLoader(\n",
    "    query, metadata_columns=[\"id\"], page_content_columns=[\"content\",\"media_type\",\"test_metadata\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da00fa47-16dc-46c4-a501-291698ff1508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "\n",
    "\n",
    "store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08949b5-f302-40d7-b2a9-3a73db018bd7",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/rag-with-bigquery-and-langchain-in-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ea0c737-71e6-4126-a6cb-7ab71393705f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "documents.extend(loader.load())\n",
    " \n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5#,\n",
    "   # separators=[\"\\n\\n\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# get current processing time to add it to metadata, datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Add chunk number to metadata\n",
    "chunk_idx=0\n",
    "prev=doc_splits[0].metadata[\"id\"]\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"process_time\"]=now\n",
    "    if prev==split.metadata[\"id\"]:\n",
    "       split.metadata[\"chunk\"] = chunk_idx      \n",
    "    else:\n",
    "        chunk_idx=0\n",
    "        split.metadata[\"chunk\"] = chunk_idx\n",
    "        prev=split.metadata[\"id\"]\n",
    "    chunk_idx +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f1cca1e-efc9-4f5d-ad4e-80d0e3b4d42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ") \n",
    "\n",
    "_=bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e9714ce-acba-4d90-818f-c3d97fce3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load():\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "    \n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "    chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\n\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     project_id= os.environ.get(\"PROJECT_ID\") \n",
    "#     dataset= os.environ.get(\"DATASET\")  \n",
    "#     table= os.environ.get(\"TABLE\") \n",
    "#     region= os.environ.get(\"REGION\") \n",
    "#     metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "#     page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "#     source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "#     separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "#     chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "#     chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "#     project_id= 'nine-quality-test' \n",
    "#     dataset= 'my_langchain_dataset'\n",
    "#     table= 'doc_and_vectors'\n",
    "#     region= 'us-central1'\n",
    "#     metadata_columns= \"id\".split(\",\")\n",
    "#     page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "#     source_query_str= \"\"\"\n",
    "#     SELECT id,media_type,content,test_metadata \n",
    "#     FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "#     \"\"\"\n",
    "#     separators= \"\\n\\n\"\n",
    "#     chunk_size= 25\n",
    "#     chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "#     message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "#                         metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "#                         source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "#                        chunk_overlap=chunk_overlap)\n",
    "#     return(message)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0424463-a90c-42a9-bb30-99f3a8eded81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b05dbe9-3eda-409d-9821-526491f7b66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "\n",
    "def list_sub_directories(bucket_name, prefix):\n",
    "    \"\"\"Returns a list of sub-directories within the given bucket.\"\"\"\n",
    "    service = googleapiclient.discovery.build('storage', 'v1')\n",
    "\n",
    "    req = service.objects().list(bucket=bucket_name, prefix=prefix, delimiter='/')\n",
    "    \n",
    "    res = req.execute()\n",
    "    \n",
    "    return res['prefixes']\n",
    "\n",
    "# For the example (gs://abc/xyz), bucket_name is 'abc' and the prefix would be 'xyz/'\n",
    "#print(list_sub_directories(bucket_name='raw_nine_files/sub_dir', prefix=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46029a1-d72b-4ee2-a594-848a5bfa4c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023/', '2024/']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_sub_directories(bucket_name=bucket, prefix=prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee98b19e-ae8d-423a-8756-102822b0c081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/1/a/\n",
      "2023/1/b/\n",
      "2023/2/c/\n",
      "2024/3/d/\n"
     ]
    }
   ],
   "source": [
    "bucket='raw_nine_files'\n",
    "prefix=''\n",
    "l=[]\n",
    "for main in list(list_sub_directories(bucket_name=bucket, prefix=prefix)):\n",
    "    for dircty in list(list_sub_directories(bucket_name=bucket, prefix=main )):\n",
    "       for sub_dircty in list(list_sub_directories(bucket_name=bucket, prefix=dircty )):\n",
    "           print(sub_dircty)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "f6048d2c-dd54-46b8-b181-ade334045925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_gcs_objects(bucket_name):\n",
    "    \"\"\"List all objects in a Google Cloud Storage bucket and print their metadata.\"\"\"\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=\"vlt_video_extract\")\n",
    "\n",
    "    print(f\"Objects in bucket '{bucket_name}':\")\n",
    "    for blob in blobs:\n",
    "        print(f\"Name: {blob.name}\")\n",
    "        print(f\"Size: {blob.size} bytes\")\n",
    "        print(f\"Content Type: {blob.content_type}\")\n",
    "        print(f\"Media Link: {blob.public_url}\")  # Get public URL if the object is publicly accessible\n",
    "        print(f\"Created: {blob.time_created}\")\n",
    "        print(f\"Updated: {blob.updated}\")\n",
    "        print(f\"Updated: {blob.self_link}\")\n",
    "        print(f\"Updated: {blob.get}\")\n",
    "        #print(dir(blob))\n",
    "        \n",
    "        print(\"------\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f197a8-25b7-416a-a2a6-f04a6fd5da4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5634.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_duration(\"raw_nine_files\",\"vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced9a6e-a9a9-4f8e-9e39-5d410c5e7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ffcfb4b138e4ddd2456a59167c84bf4021642dff6ad6de28e57d52e-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/2024%2F3%2Fd%2Fscreenshot_directrycontent.png?jk=AdvOr8tEXuKMrGk7hapfYy4ZPHk1yEDCUJKOHuCrx660bHpMtnBM9ujMQYZYlGC3uD9x2FfhMgvBZNcuZcVX6z301E7Zm2RjL3pJkSIu8paY66HCL6H6SLrZ4R1eCjkaxareY7JnsKrMof1amj819hbOcCMgyGfuRm74dLtW501kSwMiUwyd_6yYs6EeoeG38UVmQBiioRHQgCSMdDBu3Ej3hAwXvcIPMfCIhqvjQD7uD1OFecUTJx40xk_o_Yiy-uMA5XNsHRBiPtZ3KnUphFw5BIxArUX3DDMPFpEySCHtPIb67btv3H-FXW8MvV_1HBkw3V-xlDt3lKggVN4WRq3IeS6cF0xFD25jpDSYA-jBKL6f_U0wprpd3918-ldVEE77DysLlD7EnhIn_KErSpWi9qYWnsvqftvC3R-4iJUkehhUjtUiefJrA0uEJsZoSEeevzM_0jwtMY2Rb4LCzroTIYLDwRc4n45ILr13Rev_ATTsSx6ccGzxhfRClVaHrBRY7udek7EXIPJAx8m8MkyQCfKScX4GoWQqmPhn3hNd0LuW44r5xpO-ljrOzsBndeswe0gtwM53ns9ObCMznjoJo3DeGKFxKxsIUxmoSRaLP4bpvz7YgZEIqvrS8Mt1ZxHEXG98_meWlYyPd3A2ir7G7ZLQcxiIBm7-IKms2rmapPc3u5bM69KhjQNR93HYRxCXGzQnIhR8HYXzj9P1_6INOw3UVkBdIYX-jEFsNhkzHHTEIqrmDW_76xQ_wSrr_dOh3Oi8Ri12DXaXksMFBvbJhUGmgETi0QKfqd8EI-ReT3BbZVChbAIJ1RNUe-wd7guT8mKdpxTicWK0KpMnGaU3DL8tlY9GZwpFyRDmJRHwBP7iqVjU4M4jF2KyHwdL589yyxlheVn-fmItOag0vPxkysf4pX0qL52WNP3ch15tN2D8LePFp4S__rQwXD-vdsciDDXBH3DOeLeKWR4-h6lVkYxi6XtHVbLgOE26lurqe1Q0XQQcJYA1usDNdJUNYwuW1qYGiB7VH6PDYSrSHuVj2YB3bmsGxpxOf2vxrhhJZzIhKU3Yxy-BX-tXp0AK8C4oowhu66feVTx6dIl_d3xQlOinsq5kEbsxvb9aSSKPkuNkfFLTbksJhW2-QWHnv4Bwtf7uC6zMLxVfocy-iCZAMTkGa9icvrHJeT3htPvpTQYbHq_kOeTVZUj3Hw-LRcaUQnxQfOTLVWmGXBAyWhWr28l5zblWxYxGMXMP&isca=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b19e03-d39e-40e9-be2e-4aa2ec427834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Duration: 5634.0\n"
     ]
    }
   ],
   "source": [
    "#video_url = \"https://storage.cloud.google.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\"\n",
    "video_url=\"https://ffba87f3998dc58bc084d165a1de6442fbc82dae22183352ca8b1bc-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4?jk=AdvOr8stvY3hRXD9psznNOnR68vfY46qQuSRwB7uO7udZdeU2LGGz-_54PmfT-yobQGxdyq1OJd1dnjB-KHuYc64Ky3hZ45dZPi3LocJ9kOpF2E7EAfzzbh1tnu855pcL63SAw-jIOe4AKQdfRQcXxO-KhuhEkUxHY9jthAzHPVv7wYH5zPsrKLuKIZzlfqJ-kh2KYLi-HlC9SDa9FtEPZbHXSuJZvlUXZK9exW_8Qnxlo5EtNLP386be6siB0rndlMgdKeHPTcO3r7RpXPlfeFm6hQuWDpe-vAW6wRz-BJzlnXOc1-cdXQ7_2Hz85JrdQ6SO-7Sf9fG7Nhk-wSLWGf_9hzaD4AaaVqhCJqmojJGQ1NID5iLry10QGX6r2Ce23Rp0P0dicWb0ePc6BygOVOPM2NZZWyHQO0OJB63aaXsz5MAFTA7dU0oQsz36-d4NzwcEiIfb4Sv86r5uowgeS_pBBOJrv0Y3NlmF1BJ_Eq5bN-q10S0wJ040NMQNs-fWfDKSSCBd-gWovKWk16r4DLSOvt10-EVfNWs8qHEnI9VJB_3E03XxhrONQxX2VqQoCnPMYcAob1lbfezAefR9dpy6xoXvcoB_GajqVoBBLlF6lk8cX69YxsL_gdSzNVJIbRMwVYfdipLKhXxiipUJM3GuB-OlD9OwKjpbYK2hGCxN0GR_pY5mrRREka8FAsxMpZvmEXEgqwYKwMPRCHmtfRj4uAyubVUPP-WCBEQQ_0r8VG1IBFqhLLrM4rqrWwxM5Vejjo6NPS6QP2Va5sOfptdcyLqTXpaNBesHkrO_hyC4ClOtwkhx1-x8rTPHat8VkUHUcPGHug2aVHp8l-zzTs-puLcF4N24hfkD06MVqbUWslkKHXTUiEbRiIOgg854ObQ976NsZI8GPeDjIIbCYzCZyDGUR9lYcJ98oQlopCEtPZsKrwUE-nV40WvQ2ABXGrW3szWrOlaOPQDwmwCTTSgWvlyxWlqI8txdIV3B1pccav3bkOiqlz7e2BcBc6rxaxu_P4Xy9_ZBGggkFO7dIiBizs0O6kkWoFib5985-bXbEvXNdeClyhuTSG9JML5-EiQqMvfm0_uW69Y9LtAUw9QwlMg6AP9nZytgw1aPBTDJlU6nZHM4ARrS7-Tv9QV3eUIGANS466_mAPP-M5wR3sWIy8fq7LhUCWmVxOqCt-iMgiqP6ZgyyRd6AgY1wAk-lshrySf1uDZV5xgeQrp8WhfNS19HNMexHSqreqPYIs&isca=1\"\n",
    "try:\n",
    "    clip = VideoFileClip(video_url)\n",
    "    print(\"Video Duration:\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba45b01-3702-4224-a881-cedccbd6daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from moviepy.editor import VideoFileClip\n",
    "import io\n",
    "\n",
    " \n",
    "# Initialize the storage client\n",
    "client = storage.Client( )\n",
    "\n",
    "# Set your bucket and file names\n",
    "bucket_name = 'raw_nine_files'\n",
    "object_name = 'vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4'\n",
    "\n",
    "# Create a file-like object from GCS\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(object_name)\n",
    "\n",
    "# Download the blob as a byte stream\n",
    "video_stream = io.BytesIO()\n",
    "blob.download_to_file(video_stream)\n",
    "video_stream.seek(0)  # Reset stream position to the beginning\n",
    "\n",
    "# Use moviepy to get the video length from the stream\n",
    "try:\n",
    "    clip = VideoFileClip(video_stream)\n",
    "    print(\"Video Duration (seconds):\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2da52ee7-4a76-4d33-81bf-81e624583f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rows have been added.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load(project_id: str= None, dataset: str= None, table: str= None, region: str =None,\\\n",
    "                        metadata_columns: list[str]=None, page_content_columns: list[str]= None, \\\n",
    "                        source_query_str: str= None, separators:  list[str]=None, chunk_size: int=None, \\\n",
    "                       chunk_overlap: int=0):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "   \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= 1000 if str(os.environ.get(\"CHUNK_SIZE\"))==\"None\" else int(str(os.environ.get(\"CHUNK_SIZE\")))  \n",
    "    chunk_overlap= 0 if str(os.environ.get(\"CHUNK_OVERLAP\"))==\"None\" else int(str(os.environ.get(\"CHUNK_OVERLAP\")))\n",
    " \n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\\\n\\\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "    message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "                        metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "                        source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "                       chunk_overlap=chunk_overlap)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "ff0ec9a2-e279-4f9e-bd31-64294f9d6a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "       \n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Truncate the table if exist\n",
    "        query = f\"TRUNCATE TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' truncated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error truncating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "        #add partitioning\n",
    "        # table.time_partitioning = bigquery.TimePartitioning(\n",
    "        #     type_=bigquery.TimePartitioningType.DAY,\n",
    "        #     field=\"request_id\",  # This field will be used for partitioning\n",
    "        # )\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5bc05941-78d7-45ae-bc59-77841f2b803f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''\n",
    " \n",
    "    try:\n",
    "        project_id=  request_args['project_id']\n",
    "        dataset_id=  request_args['dataset']\n",
    "        table= request_args['table']\n",
    "        region= request_args['region']\n",
    "        metadata_columns= str(request_args['metadata_columns']).split(',') \n",
    "        page_content_columns= str(request_args['page_content_columns']).split(',') \n",
    "        source_query_str= request_args['source_query_str']\n",
    "        separators= None if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "        chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "        chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap']))  \n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= 'chunked_data'\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [ col.strip() for col in  \"id,content,path,media_type,test_metadata\".split(\",\")]\n",
    "            page_content_columns= [col.strip() for col in \"content,test_metadata\".split(',') ]\n",
    "            source_query_str= \"\"\"\n",
    "            SELECT id,media_type,content,test_metadata, path\n",
    "            FROM `nine-quality-test.Nine_Quality_Test.test_long_artcles` ;\n",
    "            \"\"\"\n",
    "            separators= \"\\n\"\n",
    "            chunk_size= 500\n",
    "            chunk_overlap= 10\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    request_date=datetime.today().strftime('%Y_%m_%d')\n",
    "    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=30000 #maximum number of requests in a batch\n",
    " \n",
    "    max_index=2\n",
    "    record_count=0\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"id\"]\n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = request_date+'_'+str(version)\n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"id\": split.metadata[\"id\"], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\":  split.page_content,\n",
    "                                   \"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   \"media_type\": split.metadata[\"media_type\"],\n",
    "                                   \"path\": split.metadata[\"path\"],\n",
    "                                   \"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{dataset_id}_{request_id}\"\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]\n",
    "                print(job.result())\n",
    "    \n",
    "    return {'status':'SUCCESS', 'record_count':record_count, 'count_of_tables':version+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6bc54925-7ff7-47a9-863f-8bc6f142cd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BigQueryLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchunk_bq_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[176], line 39\u001b[0m, in \u001b[0;36mchunk_bq_content\u001b[0;34m(request_args)\u001b[0m\n\u001b[1;32m     33\u001b[0m         chunk_overlap\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m#return {'record_count':0, 'status':'ERROR- Set required input parameters'}\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m  \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mBigQueryLoader\u001b[49m(\n\u001b[1;32m     40\u001b[0m     query\u001b[38;5;241m=\u001b[39msource_query_str, project\u001b[38;5;241m=\u001b[39mproject_id, metadata_columns\u001b[38;5;241m=\u001b[39mmetadata_columns, page_content_columns\u001b[38;5;241m=\u001b[39mpage_content_columns\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m documents\u001b[38;5;241m.\u001b[39mextend(loader\u001b[38;5;241m.\u001b[39mload())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BigQueryLoader' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_bq_content('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90c12451-fb58-4edd-9d53-8dde1eaa087c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320504668865953792\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "import datetime\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "\n",
    " \n",
    "system_instruction = \"\"\"\n",
    "You are an expert researcher. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at these research papers, and answer the following questions.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(days=2),\n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)\n",
    "# Example response:\n",
    "# 1234567890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df716674-c8cd-4880-91f2-df9ff80f4963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cf3d063-c54f-42ed-ace5-ada7a184ec39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/494586852359/locations/us-central1/cachedContents/1320504668865953792',\n",
       " 'model': 'projects/nine-quality-test/locations/us-central1/publishers/google/models/gemini-1.5-pro-002',\n",
       " 'createTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'updateTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'expireTime': '2024-10-12T00:05:18.129147Z',\n",
       " 'displayName': 'example-cache'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_content.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40be2737-2aaa-4a80-9b25-cd49ee49a3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'createTime': 'llm_response.body.createTime',\n",
       "  'expireTime': 'llm_response.body.expireTime',\n",
       "  'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4',\n",
       "  'model': 'llm_response.body.model',\n",
       "  'name': 'llm_response.body.name',\n",
       "  'updateTime': 'llm_response.body.updateTime',\n",
       "  'usageMetadata': 'llm_response.body.usageMetadata'},\n",
       " {'createTime': 'llm_response.body.createTime',\n",
       "  'expireTime': 'llm_response.body.expireTime',\n",
       "  'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4',\n",
       "  'model': 'llm_response.body.model',\n",
       "  'name': 'llm_response.body.name',\n",
       "  'updateTime': 'llm_response.body.updateTime',\n",
       "  'usageMetadata': 'llm_response.body.usageMetadata'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= data['items']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1db7b32-ad04-468a-809b-96daefab45b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l=list(filter(lambda element: element['gcs_uri'] == 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b0cc048-04d5-499e-b4e8-966969bc39b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][\"gcs_uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0db5932-f361-467b-b6b2-9ef846dad71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n",
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-10-12T23:45:42.158262Z'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import the Google Cloud client library and JSON library\n",
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('raw_nine_files')\n",
    "blob = bucket.blob('test111.json')\n",
    "\n",
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for x in data['items']:\n",
    "    print(x)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "data[\"cache_expiry_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76829048-51c8-4427-8ca1-475e0553df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for item in enumerate( data.items):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae3296-e97c-46ae-8ad2-2647a3c92c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": {\n",
    "                                            \"contents\": {\n",
    "                                                \"parts\": [\n",
    "                                                    {\n",
    "                                                        \"fileData\": {\n",
    "                                                            \"fileUri\": \"gs://raw_nine_files/60MI23_33_A_HBB.mp4\",\n",
    "                                                            \"mimeType\": \"video/mp4\"\n",
    "                                                        },\n",
    "                                                        \"videoMetadata\": {\n",
    "                                                            \"endOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 240\n",
    "                                                            },\n",
    "                                                            \"startOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 120\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                        \"text\": \"You are an assistant tasked with describing videos for retrieval.These descriptions will be embedded and used to retrieve the raw video.\\n Chapterize the video content by grouping the video content into chapters with intervals of 120 seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\nIf there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\n Describe important scenes in the video concisely. If you are not sure about any info, please do not make it up. \\n Only consider video from 120 seconds to 240 seconds. \\nIf it is the last chapter, set the endOffset to 240 instead.\\n For result, follow JSON schema.<JSONSchema>{\\\"description\\\":\\\"A list of chapters\\\",\\\"items\\\":{\\\"properties\\\":{\\\"Content\\\":{\\\"type\\\":\\\"string\\\"},\\\"endOffset\\\":{\\\"type\\\":\\\"integer\\\"},\\\"startOffset\\\":{\\\"type\\\":\\\"integer\\\"}},\\\"required\\\":[\\\"startOffset\\\",\\\"endOffset\\\",\\\"Content\\\"],\\\"type\\\":\\\"object\\\"},\\\"type\\\":\\\"array\\\"}</JSONSchema>\"\n",
    "                                                    }\n",
    "                                                ],\n",
    "                                                \"role\": \"user\"\n",
    "                                            },\n",
    "                                            \"generation_config\": {\n",
    "                                                \"max_output_tokens\": 2048,\n",
    "                                                \"temperature\": 0.5,\n",
    "                                                \"top_k\": 40,\n",
    "                                                \"top_p\": 0.8\n",
    "                                            },\n",
    "                                            \"safety_settings\": [\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        }\n",
    "                                    }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcba9a1d-5832-4e71-83bc-7b13d46c491a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7373483305540255744\n",
      "<vertexai.caching._caching.CachedContent object at 0x7fcf8c5ed3c0>: {\n",
      "  \"name\": \"projects/494586852359/locations/us-central1/cachedContents/7373483305540255744\",\n",
      "  \"model\": \"projects/nine-quality-test/locations/us-central1/publishers/google/models/gemini-1.5-pro-002\",\n",
      "  \"createTime\": \"2024-10-16T05:34:44.833168Z\",\n",
      "  \"updateTime\": \"2024-10-16T05:34:44.833168Z\",\n",
      "  \"expireTime\": \"2024-10-16T07:34:07.699361Z\"\n",
      "}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(l)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m---> 16\u001b[0m cached_content \u001b[38;5;241m=\u001b[39m \u001b[43mcaching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCachedContent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_content_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#cached_content.delete()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/caching/_caching.py:141\u001b[0m, in \u001b[0;36mCachedContent.__init__\u001b[0;34m(self, cached_content_name)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cached_content_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Represents a cached content resource.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    This resource can be used with vertexai.generative_models.GenerativeModel\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m            \"456\".\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_content_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gca_resource(cached_content_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:552\u001b[0m, in \u001b[0;36mVertexAiResourceNoun.__init__\u001b[0;34m(self, project, location, credentials, resource_name)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes class with project, location, and api_client.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    resource_name(str): A fully-qualified resource name or ID.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resource_name:\n\u001b[0;32m--> 552\u001b[0m     project, location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_validate_project_location\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject \u001b[38;5;241m=\u001b[39m project \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mproject\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation \u001b[38;5;241m=\u001b[39m location \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mlocation\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:654\u001b[0m, in \u001b[0;36mVertexAiResourceNoun._get_and_validate_project_location\u001b[0;34m(self, resource_name, project, location)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_validate_project_location\u001b[39m(\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    638\u001b[0m     resource_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    639\u001b[0m     project: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    640\u001b[0m     location: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    641\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    643\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate the project and location for the resource.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m        RuntimeError: If location is different from resource location\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m     fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_resource_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fields:\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m project, location\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:613\u001b[0m, in \u001b[0;36mVertexAiResourceNoun._parse_resource_name\u001b[0;34m(cls, resource_name)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03mParses resource name into its component segments.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    Dictionary of component segments.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m# gets the underlying wrapped gapic client class\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gapic_client_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_resource_name_method\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1beta1/services/gen_ai_cache_service/client.py:212\u001b[0m, in \u001b[0;36mGenAiCacheServiceClient.parse_cached_content_path\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_cached_content_path\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parses a cached_content path into its component segments.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^projects/(?P<project>.+?)/locations/(?P<location>.+?)/cachedContents/(?P<cached_content>.+?)$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroupdict() \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/re.py:190\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "# cache_id = \"your-cache-id\"\n",
    " \n",
    "# cached_content = caching.CachedContent(cached_content_name=cache_id)\n",
    "# cached_content.delete()\n",
    "\n",
    "for x in caching.CachedContent.list():\n",
    "     l=x.name.split(\"/\")[-1]\n",
    "     print(l)\n",
    "     print(x)\n",
    "     cached_content = caching.CachedContent(cached_content_name=x)\n",
    "     #cached_content.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46dedaab-b53c-4f9b-9e17-8037bf254d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VideoRequest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_video_duration\u001b[39m(request: \u001b[43mVideoRequest\u001b[49m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" generates video length\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m   \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m          \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     fs \u001b[38;5;241m=\u001b[39m gcsfs\u001b[38;5;241m.\u001b[39mGCSFileSystem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VideoRequest' is not defined"
     ]
    }
   ],
   "source": [
    "def get_video_duration(request: VideoRequest):\n",
    "    \"\"\" generates video length\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7b74fb8-6815-4022-bdfc-bd13da79aaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import gcsfs\n",
    "from pymediainfo import MediaInfo\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the input data model\n",
    "class VideoRequest(BaseModel):\n",
    "    url: str  # Input: video URL\n",
    "\n",
    "# Define the output data model\n",
    "class VideoResponse(BaseModel):\n",
    "    duration: float  # Output: Duration in seconds\n",
    "\n",
    "@app.post(\"/get-video-duration\", response_model=VideoResponse)\n",
    "async def get_video_duration(request: VideoRequest):\n",
    "    \"\"\"\n",
    "    get video duration of a given gcs url\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break\n",
    " \n",
    "\n",
    "    return VideoResponse(duration=duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c1998d85-7777-4975-8454-6ebe00317d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                      \n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None, video_metadata_file: str=\"\"):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for video and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "    \n",
    "\n",
    "    # Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    blob = bucket.blob(video_metadata_file)\n",
    "\n",
    "    # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "    data = json.loads(blob.download_as_string(client=None))\n",
    "    video_metadata=data['items']\n",
    "    \n",
    "\n",
    "    segments_to_process=120 #segments duration\n",
    "    intervals=120#intervals\n",
    "    video_start =0 #where from video to start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for blob in blobs:                         \n",
    "        if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         metadata=list(filter(lambda element: element['gcs_uri'] ==gcsuri, video_metadata))[0]\n",
    "                         video_duration=metadata[\"videoOriginalDurationSecond\"]\n",
    "                         cache_id=metadata[\"name\"]\n",
    "                         prev=video_start\n",
    "                         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                                offset={'start':prev, 'end':val}\n",
    "                                \n",
    "                                startOffset=offset['start']\n",
    "                                endOffset=offset['end']\n",
    "                                if endOffset>=video_duration:\n",
    "                                     endOffset=video_duration\n",
    "                                print(offset)\n",
    "                                prev=val \n",
    "                                segment_prompt= f\"Only consider video from {startOffset} seconds to {endOffset} seconds. Ignore analyzing the rest of video.\\n\" \n",
    "                                if index==0:\n",
    "                                    request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                                    rf= open(request_file.name, \"a\") \n",
    "                                \n",
    "                                 \n",
    "                                request_list=[\n",
    "                                       json.dumps(\n",
    "                                              {\n",
    "                                                \"request\": \n",
    "                                                     {\n",
    "                                                     \"cached_content\": cache_id,\n",
    "                                                      \"contents\":  {\n",
    "                                                                        \"parts\": [\n",
    "                                                                            {\n",
    "                                                                            \"fileData\":  {\"fileUri\": gcsuri, \"mimeType\": mimeType},\n",
    "                                                                            \"videoMetadata\": {\n",
    "                                                                                \"endOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": endOffset\n",
    "                                                                                },\n",
    "                                                                                \"startOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": startOffset\n",
    "                                                                                }\n",
    "                                                                            }\n",
    "\n",
    "                                                                            },\n",
    "                                                                            {\n",
    "                                                                            \"text\": prompt_text +\"\\n\"+ segment_prompt\n",
    "                                                                            } \n",
    "\n",
    "                                                                        ],\n",
    "                                                                        \"role\": \"user\"\n",
    "                                                                        }\n",
    "                                                          , \n",
    "                                                          \"generation_config\": \n",
    "                                                               {\"max_output_tokens\": max_output_tokens, \n",
    "                                                                \"temperature\":temperature, \n",
    "                                                                 \"top_k\": top_k, \n",
    "                                                                 \"top_p\": top_p\n",
    "                                                                }\n",
    "                                                          , \n",
    "                                                          \"safety_settings\": \n",
    "                                                           [\n",
    "                                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                             \"threshold\": \"BLOCK_NONE\"\n",
    "                                                             }\n",
    "                                                           , \n",
    "                                                           {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           },\n",
    "                                                           {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           }, \n",
    "                                                           {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                            \"threshold\": \"BLOCK_NONE\"\n",
    "                                                            }\n",
    "\n",
    "\n",
    "                                                           ]\n",
    "                                                     }\n",
    "                                              }\n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "                                rf.writelines(request_list)\n",
    "                                rf.flush()\n",
    "\n",
    "                                if index==(max_index-1):\n",
    "                                        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                        rf.close() \n",
    "                                        versions.append(version)                               \n",
    "                                        index=0\n",
    "                                        version +=1\n",
    "                                        request_list=[]\n",
    "                                        rf=None\n",
    "\n",
    "                                else:                               \n",
    "                                        index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "@functions_framework.http\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "#     request_json = request.get_json(silent=True)\n",
    "#     request_args = request.args\n",
    "\n",
    "#     dest_bucket_name =request_args['destination_bucket']\n",
    "#     source_bucket_name =request_args['source_bucket']\n",
    "#     source_folder_name=request_args['source_folder']\n",
    "#     request_file_prefix =request_args['request_file_prefix']\n",
    "#     request_file_folder =request_args['request_file_folder']\n",
    "#     prompt_text= request_args['prompt_text']\n",
    "#     media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    " \n",
    "#     request_content= request_args['request_content']\n",
    "\n",
    "#     if request_args and 'temperature' in request_args:\n",
    "#         temperature= request_args['temperature']\n",
    "#     else:\n",
    "#       temperature=0.5\n",
    "\n",
    "#     if request_args and 'max_output_tokens' in request_args:\n",
    "#        max_output_tokens= request_args['max_output_tokens'] \n",
    "#     else:\n",
    "#          max_output_tokens=2048\n",
    "\n",
    "#     if request_args and 'top_p' in request_args:\n",
    "#         top_p= request_args['top_p']\n",
    "#     else:\n",
    "#          top_p=0.5\n",
    "\n",
    "#     if request_args and 'top_k' in request_args:\n",
    "#         top_k= request_args['top_k']\n",
    "#     else:\n",
    "#          top_k=50\n",
    "\n",
    "#     if request_args and 'max_request_per_file' in request_args:\n",
    "#         max_request_per_file= request_args['max_request_per_file']\n",
    "#     else:\n",
    "#       max_request_per_file=30000\n",
    "    \n",
    "    dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "    source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "    request_file_prefix = \"video_request\"#request_args['request_file_prefix']\n",
    "    request_file_folder =  \"video_batch_request_fldr\"#request_args['request_file_folder']\n",
    "    prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "    media_types= ['video/mp4']#list(request_args['media_type'])\n",
    "    request_content= \"video\"#request_args['request_content']\n",
    "    source_folder_name=\"vlt_video_extract/OTHERS\"\n",
    "    temperature=0.5\n",
    "    max_output_tokens=2048\n",
    "    top_p=50\n",
    "    top_k=0.5\n",
    "    max_request_per_file=30000\n",
    "    video_metadata_file=\"vlt_video_metadata_fldr/vlt_video_metadata.json\"\n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file\n",
    "                                      )  \n",
    "    if  request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file,\n",
    "                                     video_metadata_file=video_metadata_file\n",
    "                                      ) \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "28e800a6-f02c-49ef-a0fd-9c91330edbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 120}\n",
      "{'start': 120, 'end': 240}\n",
      "{'start': 240, 'end': 360}\n",
      "{'start': 360, 'end': 480}\n",
      "{'start': 480, 'end': 600}\n",
      "{'start': 600, 'end': 720}\n",
      "{'start': 720, 'end': 840}\n",
      "{'start': 840, 'end': 960}\n",
      "{'start': 960, 'end': 1080}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 1}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db570928-ad71-4027-8cc3-13db077eef24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None, video_metadata_file: str=\"\"):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for video and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str source_bucket_name: name of the source  gcs bucket to read files from\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str source_folder_name: name of the source folder name to read files from\n",
    "            str request_file_prefix: prefix of the request file name\n",
    "            list mime_types: list of accepted mime_types\n",
    "            str prompt_text: prompt for Gimini\n",
    "            float temperature: Gimini temprature\n",
    "            float top_p: Gimini top_p\n",
    "            float top_k: Gimini top_k\n",
    "            int max_request_per_file: max number of requests per batch\n",
    "            int max_output_tokens: Gimini max_output_tokens          \n",
    "            str video_metadata_file: name of video metadata\n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]    \n",
    "\n",
    "    # Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    blob = bucket.blob(video_metadata_file)\n",
    "\n",
    "    # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "    data = json.loads(blob.download_as_string(client=None))\n",
    "    video_metadata=data['items']\n",
    "    \n",
    "\n",
    "    segments_to_process=120 #segments duration\n",
    "    intervals=120#intervals\n",
    "    video_start =0 #where from video to start\n",
    "\n",
    "    for blob in blobs:                         \n",
    "        if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         metadata=list(filter(lambda element: element['gcs_uri'] ==gcsuri, video_metadata))[0]\n",
    "                         video_duration=metadata[\"videoOriginalDurationSecond\"]\n",
    "                         cache_id=metadata[\"name\"]\n",
    "\n",
    "                         prev=video_start\n",
    "                         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                                offset={'start':prev, 'end':val}\n",
    "                                \n",
    "                                startOffset=offset['start']\n",
    "                                endOffset=offset['end']\n",
    "                                if endOffset>=video_duration:\n",
    "                                     endOffset=video_duration\n",
    "                                print(offset)\n",
    "                                prev=val \n",
    "                                segment_prompt= \"Only consider video from\" + str(startOffset)+\" seconds to \"+ str(endOffset)+\" seconds. Ignore analyzing the rest of video.\\n\" \n",
    "                                if index==0:\n",
    "                                    request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                                    rf= open(request_file.name, \"a\") \n",
    "                                \n",
    "                                 \n",
    "                                request_list=[\n",
    "                                       json.dumps(\n",
    "                                              {\n",
    "                                                \"request\": \n",
    "                                                     {\n",
    "                                                      \"cached_content\": cache_id,\n",
    "                                                      \"contents\":  {\n",
    "                                                                        \"parts\": [\n",
    "                                                                            {\n",
    "                                                                            \"fileData\":  {\"fileUri\": gcsuri, \"mimeType\": mimeType},\n",
    "                                                                            \"videoMetadata\": {\n",
    "                                                                                \"endOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": endOffset\n",
    "                                                                                },\n",
    "                                                                                \"startOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": startOffset\n",
    "                                                                                }\n",
    "                                                                            }\n",
    "\n",
    "                                                                            },\n",
    "                                                                            {\n",
    "                                                                            \"text\": prompt_text +\"\\n\"+ segment_prompt\n",
    "                                                                            } \n",
    "\n",
    "                                                                        ],\n",
    "                                                                        \"role\": \"user\"\n",
    "                                                                        }\n",
    "                                                          , \n",
    "                                                          \"generation_config\": \n",
    "                                                               {\"max_output_tokens\": max_output_tokens, \n",
    "                                                                \"temperature\":temperature, \n",
    "                                                                 \"top_k\": top_k, \n",
    "                                                                 \"top_p\": top_p\n",
    "                                                                }\n",
    "                                                          , \n",
    "                                                          \"safety_settings\": \n",
    "                                                           [\n",
    "                                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                             \"threshold\": \"BLOCK_NONE\"\n",
    "                                                             }\n",
    "                                                           , \n",
    "                                                           {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           },\n",
    "                                                           {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           }, \n",
    "                                                           {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                            \"threshold\": \"BLOCK_NONE\"\n",
    "                                                            }\n",
    "\n",
    "\n",
    "                                                           ]\n",
    "                                                     }\n",
    "                                              }\n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "                                rf.writelines(request_list)\n",
    "                                rf.flush()\n",
    "\n",
    "                                if index==(max_index-1):\n",
    "                                        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                        rf.close() \n",
    "                                        versions.append(version)                               \n",
    "                                        index=0\n",
    "                                        version +=1\n",
    "                                        request_list=[]\n",
    "                                        rf=None\n",
    "\n",
    "                                else:                               \n",
    "                                        index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for videos and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str source_bucket_name: name of the source  gcs bucket to read files from\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str source_folder_name: name of the source folder name to read files from\n",
    "            str request_file_prefix: prefix of the request file name\n",
    "            list mime_types: list of accepted mime_types\n",
    "            str prompt_text: prompt for Gimini\n",
    "            float temperature: Gimini temprature\n",
    "            float top_p: Gimini top_p\n",
    "            float top_k: Gimini top_k\n",
    "            int max_request_per_file: max number of requests per batch\n",
    "            int max_output_tokens: Gimini max_output_tokens          \n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"           \n",
    "         \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "@functions_framework.http\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request_json = request.get_json(silent=True)\n",
    "        request_args = request.args\n",
    "\n",
    "        dest_bucket_name =request_args['destination_bucket']\n",
    "        source_bucket_name =request_args['source_bucket']\n",
    "        source_folder_name=request_args['source_folder']\n",
    "        request_file_prefix =request_args['request_file_prefix']\n",
    "        request_file_folder =request_args['request_file_folder']\n",
    "        prompt_text= request_args['prompt_text']\n",
    "        media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "\n",
    "        request_content= request_args['request_content']\n",
    "\n",
    "        if request_args and 'video_metadata_file' in request_args:\n",
    "            video_metadata_file= request_args['video_metadata_file']\n",
    "        else:\n",
    "          video_metadata_file=\"\"\n",
    "\n",
    "        if request_args and 'temperature' in request_args:\n",
    "            temperature= request_args['temperature']\n",
    "        else:\n",
    "          temperature=1\n",
    "\n",
    "        if request_args and 'max_output_tokens' in request_args:\n",
    "           max_output_tokens= request_args['max_output_tokens'] \n",
    "        else:\n",
    "             max_output_tokens=8192\n",
    "\n",
    "        if request_args and 'top_p' in request_args:\n",
    "            top_p= request_args['top_p']\n",
    "        else:\n",
    "             top_p=0.95\n",
    "\n",
    "        if request_args and 'top_k' in request_args:\n",
    "            top_k= request_args['top_k']\n",
    "        else:\n",
    "             top_k=40\n",
    "\n",
    "        if request_args and 'max_request_per_file' in request_args:\n",
    "            max_request_per_file= request_args['max_request_per_file']\n",
    "        else:\n",
    "          max_request_per_file=30000\n",
    "    except:\n",
    "            dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "            source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "            request_file_prefix = \"video_request\"#request_args['request_file_prefix']\n",
    "            request_file_folder =  \"video_batch_request_fldr\"#request_args['request_file_folder']\n",
    "            prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "            media_types= ['video/mp4']#list(request_args['media_type'])\n",
    "            request_content= \"video\"#request_args['request_content']\n",
    "            source_folder_name=\"vlt_video_extract/OTHERS\"\n",
    "            temperature=0.5\n",
    "            max_output_tokens=2048\n",
    "            top_p=50\n",
    "            top_k=0.5\n",
    "            max_request_per_file=30000\n",
    "            video_metadata_file=\"vlt_video_metadata_fldr/vlt_video_metadata.json\"\n",
    "\n",
    " \n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file\n",
    "                                      )  \n",
    "    if  request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file,\n",
    "                                     video_metadata_file=video_metadata_file\n",
    "                                      ) \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b17d6b7-db52-48ef-b9b3-504fdeb11bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 120}\n",
      "{'start': 120, 'end': 240}\n",
      "{'start': 240, 'end': 360}\n",
      "{'start': 360, 'end': 480}\n",
      "{'start': 480, 'end': 600}\n",
      "{'start': 600, 'end': 720}\n",
      "{'start': 720, 'end': 840}\n",
      "{'start': 840, 'end': 960}\n",
      "{'start': 960, 'end': 1080}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 1}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6f51d-1fc9-4b4a-ba40-1f6b196b8927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7323099284709048320\n"
     ]
    }
   ],
   "source": [
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    \n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ae371a-c0db-4f4c-86f6-87c86ff0eb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first paper, \"Gemini: A Family of Highly Capable Multimodal Models\", introduces Gemini 1.0, Google's family of multimodal models. It details their architecture, training, capabilities, and responsible deployment.  Gemini 1.0 models come in different sizes (Ultra, Pro, Nano) for various applications.  The paper highlights Gemini Ultra's state-of-the-art performance on many benchmarks, exceeding human expert performance on MMLU and achieving strong results in multimodal reasoning (like MMMU) and coding tasks.  The paper emphasizes responsible AI development, outlining impact assessments, safety mitigations, and evaluations to address potential harms.\n",
      "\n",
      "The second paper, \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\", introduces Gemini 1.5 Pro, an evolution focusing on significantly expanded context length and efficiency.  It details the model's architecture as a mixture-of-experts model and the infrastructure advancements enabling its capabilities.  The paper emphasizes Gemini 1.5 Pro's near-perfect recall on long-context retrieval across modalities (text, video, audio) up to millions of tokens. It highlights improvements over Gemini 1.0 on core benchmarks while requiring less compute, and showcases surprising new capabilities like in-context language learning for low-resource languages with limited data.  Responsible development remains a focus, outlining updated impact assessment, model mitigation, and safety evaluations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "# cache_id = \"your-cache-id\"\n",
    "\n",
    " \n",
    "cached_content = caching.CachedContent(cached_content_name=\"7323099284709048320\")\n",
    "\n",
    "model = GenerativeModel.from_cached_content(cached_content=cached_content)\n",
    "\n",
    "response = model.generate_content(\"What are the papers about?\")\n",
    "\n",
    "print(response.text)\n",
    "# Example response:\n",
    "# The provided text is about a new family of multimodal models called Gemini, developed by Google.\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc92a70a-f78b-40e1-84d0-82165a547135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GenerativeModel' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mdir\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GenerativeModel' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802f1da-660c-4c80-84b8-8e6eec3961fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " url: ${MM_LLM_ENDPOINT}\n",
    "                                auth:\n",
    "                                    type: OAuth2\n",
    "                                body:\n",
    "                                    \"instances\": [\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         mimeType:  \"${item.mime_type}\",\n",
    "                                                        \"gcsUri\": \"${item.gcs_uri}\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "                            result: llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "decc7737-767c-446c-8513-3fe5bf1fbb92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MM_MODEL= \"multimodalembedding@001\"\n",
    "MM_METHOD= \"predict\"\n",
    "PROJECT='nine-quality-test'\n",
    "LOCATION='us-central1'\n",
    "MM_LLM_ENDPOINT=\"https://\" + 'us-central1' + \"-aiplatform.googleapis.com\" + \"/v1/projects/\" + PROJECT + \"/locations/\" + 'us-central1' + \"/publishers/google/models/\" + MM_MODEL+\":\"+MM_METHOD\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65083e85-cf11-4364-9dab-e0056d461e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'code': 401, 'message': 'Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.', 'status': 'UNAUTHENTICATED', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'CREDENTIALS_MISSING', 'domain': 'googleapis.com', 'metadata': {'method': 'google.cloud.aiplatform.v1.PredictionService.Predict', 'service': 'aiplatform.googleapis.com'}}]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The API endpoint\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Data to be sent\n",
    "data = {\n",
    "     \"instances\": [\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         \"mimeType\":  \"image/png\",\n",
    "                                                        \"gcsUri\": \"gs://raw_nine_files/2023/1/a/816f62dz.png\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "}\n",
    "\n",
    "# A POST request to the API\n",
    "response = requests.post(MM_LLM_ENDPOINT, json=data)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08618712-9ba9-42c1-86e0-1dc6b9e3240c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "api_regional_endpoint= \"us-central1-aiplatform.googleapis.com\"\n",
    "client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "            client_options=client_options\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7f90ed3-d096-4851-b36e-0ca1197a46c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "instances=[\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         \"mimeType\":  \"image/png\",\n",
    "                                                        \"gcsUri\": \"gs://raw_nine_files/2023/1/a/816f62dz.png\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "\n",
    "endpoint = (\n",
    "           f\"projects/{PROJECT}/locations/{LOCATION}\"\n",
    "           \"/publishers/google/models/multimodalembedding@001\"\n",
    "        )\n",
    "response =client.predict(endpoint=endpoint, instances=instances)\n",
    "response.predictions[0].get(\"imageEmbedding\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07e10cda-9199-49c7-94a5-dd5a0a7f0815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_method= \"generateContent\"\n",
    "content_model= \"gemini-1.5-pro-002\"\n",
    "endpoint=(\"v1beta1/projects/\"  + PROJECT+\"/locations/\" + LOCATION + \"/publishers/google/models/\"+ content_model + \":\" + content_method)\n",
    " \n",
    "instances={\n",
    "                                       \n",
    "                                        \"contents\": [\n",
    "                                            {\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"parts\": [\n",
    "                                                {\n",
    "                                              \n",
    "                                                \n",
    "                                                \"fileData\": {\n",
    "                                                    \"mimeType\": \"video/mp4\",\n",
    "                                                    \"fileUri\": \"gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\"\n",
    "                                                },\n",
    "\n",
    "                                                \"videoMetadata\": {\n",
    "                                                    \"startOffset\": {\n",
    "                                                    \"seconds\": 300,\n",
    "                                                    \"nanos\": 0\n",
    "                                                    },\n",
    "                                                    \"endOffset\": {\n",
    "                                                    \"seconds\": 520,\n",
    "                                                    \"nanos\": 0\n",
    "                                                    }\n",
    "                                                }\n",
    "                                                },\n",
    "                                                 { \"text\": \"describe what is happening in this video in the given time frame from offset 300 second to offset 520 second in details and what people are talking about.\"}\n",
    "                                            ]\n",
    "                                            }\n",
    "                                        ] ,\n",
    "                                        \n",
    "                                        \"safetySettings\": [\n",
    "                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "                                        ],\n",
    "                                        \"generationConfig\": {\n",
    "                                            \"temperature\": 1,\n",
    "                                            \"topP\": 0.5,\n",
    "                                            \"topK\": 40,\n",
    "                                           # \"candidateCount\": integer,\n",
    "                                            \"maxOutputTokens\": 8192,\n",
    "                                            # \"presencePenalty\": float,\n",
    "                                            # \"frequencyPenalty\": float,\n",
    "                                            # \"stopSequences\": [\n",
    "                                            # string\n",
    "                                            # ],\n",
    "                                            # \"responseMimeType\": string,\n",
    "                                            # \"responseSchema\": schema,\n",
    "                                            # \"seed\": integer,\n",
    "                                            # \"responseLogprobs\": boolean,\n",
    "                                            # \"logprobs\": integer,\n",
    "                                            # \"audioTimestamp\": boolean\n",
    "                                        }#,\n",
    "\n",
    "                                       # ,\"cachedContent\": \"projects/494586852359/locations/us-central1/cachedContents/4906355134671355904\"  \n",
    "                                       \n",
    "} \n",
    "\n",
    "from google.cloud import aiplatform\n",
    "api_regional_endpoint= \"us-central1-aiplatform.googleapis.com\"\n",
    "client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "            client_options=client_options\n",
    "        )\n",
    "\n",
    "import google.cloud.aiplatform.v1beta1  as endpoint       \n",
    "#response =client.generate_content(instances)\n",
    "#response =client.predict(endpoint=endpoint, instances=[instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9890de0e-b31c-4020-90e9-ce20b6429589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClassificationPredictionResult',\n",
       " 'ImageObjectDetectionPredictionResult',\n",
       " 'ImageSegmentationPredictionResult',\n",
       " 'TabularClassificationPredictionResult',\n",
       " 'TabularRegressionPredictionResult',\n",
       " 'TextExtractionPredictionResult',\n",
       " 'TextSentimentPredictionResult',\n",
       " 'TimeSeriesForecastingPredictionResult',\n",
       " 'VideoActionRecognitionPredictionResult',\n",
       " 'VideoClassificationPredictionResult',\n",
       " 'VideoObjectTrackingPredictionResult',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'gapic_version',\n",
       " 'package_version',\n",
       " 'types']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dir(endpoint.schema.predict.prediction_v1beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62c24684-9d19-4409-904c-04afbd84b578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "creds, project = google.auth.default()\n",
    "\n",
    "# creds.valid is False, and creds.token is None\n",
    "# Need to refresh credentials to populate those\n",
    "\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b08ca94e-7e65-4a6c-a02e-a3c5f289137f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m video_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2885\u001b[39m\u001b[38;5;66;03m#math.ceil(get_video_duration(video_file))\u001b[39;00m\n\u001b[1;32m     54\u001b[0m video_chapters\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 55\u001b[0m prev\u001b[38;5;241m=\u001b[39m\u001b[43mvideo_start\u001b[49m\n\u001b[1;32m     56\u001b[0m log\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (segments_to_process,video_duration\u001b[38;5;241m+\u001b[39msegments_to_process,segments_to_process):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_start' is not defined"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import time\n",
    "import typing\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "#libraries to generate image summaries\n",
    "from vertexai.vision_models import Video\n",
    "from vertexai.vision_models import VideoSegmentConfig\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part as GenerativeModelPart,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from typing import Any, Dict, List, Literal, Optional, Union\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    " \n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import initializer as aiplatform_initializer\n",
    "import datetime\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "if 1==1:\n",
    "         generative_multimodal_model= GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "         #generation_config= GenerationConfig(temperature=1, top_k=40,top_p=0.95,max_output_tokens=8192) \n",
    "         generation_config=GenerationConfig(temperature=1, top_k=40,top_p=0.95,max_output_tokens=8192)#, response_mime_type='application/json',\n",
    "\t     # response_schema=json.loads(schema))  \n",
    "         #for video, BLOCK_NONE gives error. So, have to set it to BLOCK_ONLY_HIGH\n",
    "         safety_settings=  {\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    }\n",
    "         stream=False\n",
    "        \n",
    "         video_duration=2885#math.ceil(get_video_duration(video_file))\n",
    "         \n",
    "         video_chapters=[]\n",
    "         prev=0\n",
    "         log=[]\n",
    "        segments_to_process=120\n",
    "         \n",
    "         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                offset={'start':prev, 'end':val}\n",
    "                prev=val    \n",
    "                print(offset)\n",
    "                startOffset=offset['start']\n",
    "                endOffset=offset['end']\n",
    "                chapters=[]  \n",
    "                \n",
    "                if endOffset>=video_duration:\n",
    "                    endOffset=video_duration\n",
    "                    \n",
    "                video_description_prompt=f\"\"\"You are an assistant tasked with summarizing videos for retrieval.\\\n",
    "                     These summaries will be embedded and used to retrieve the raw video.\\\n",
    "                    Chapterize the video content by grouping the video content into chapters \\\n",
    "                    with intervals of {intervals} seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\\n",
    "                    If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\n",
    "                    Describe important scenes in the video concisely.\\\n",
    "                    If you are not sure about any info, please do not make it up. \\\n",
    "                    Only consider video from {startOffset} seconds to {endOffset} seconds. Ignore analyzing the rest of video.\\\n",
    "                    If it is the last chapter, set the endOffset to {endOffset} instead.\\ \n",
    "                    If a chapter includes prohibited content, set chapterSummary to \"\".\\\n",
    "                    For result, follow JSON schema.<JSONSchema>{json.dumps(schema)}</JSONSchema>\"\n",
    "                    \"\"\"   \n",
    " \n",
    "        \n",
    "                contents=[parts=[GenerativeModelPart.from_uri(video_file,mime_type=\"video/mp4\", ),\n",
    "                           video_description_prompt,]        \n",
    "\n",
    "                model_response = generative_multimodal_model.generate_content(\n",
    "                                    contents,\n",
    "                                    generation_config=generation_config,\n",
    "                                    stream=stream,\n",
    "                                    safety_settings=safety_settings, )        \n",
    "\n",
    "                response_list = []\n",
    "                    # print(model_response)\n",
    "\n",
    "                print('Called the API, processing the result now...')\n",
    "                prohibited_content=False\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1f66746-d055-4eb5-9d03-f446a35c41b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate_content in module vertexai.generative_models._generative_models:\n",
      "\n",
      "generate_content(contents: Union[List[ForwardRef('Content')], List[Dict[str, Any]], str, ForwardRef('Image'), ForwardRef('Part'), List[Union[str, ForwardRef('Image'), ForwardRef('Part')]]], *, generation_config: Union[ForwardRef('GenerationConfig'), Dict[str, Any], NoneType] = None, safety_settings: Union[List[ForwardRef('SafetySetting')], Dict[google.cloud.aiplatform_v1beta1.types.content.HarmCategory, google.cloud.aiplatform_v1beta1.types.content.SafetySetting.HarmBlockThreshold], NoneType] = None, tools: Optional[List[ForwardRef('Tool')]] = None, tool_config: Optional[ForwardRef('ToolConfig')] = None, stream: bool = False) -> Union[ForwardRef('GenerationResponse'), Iterable[ForwardRef('GenerationResponse')]] method of vertexai.preview.generative_models.GenerativeModel instance\n",
      "    Generates content.\n",
      "    \n",
      "    Args:\n",
      "        contents: Contents to send to the model.\n",
      "            Supports either a list of Content objects (passing a multi-turn conversation)\n",
      "            or a value that can be converted to a single Content object (passing a single message).\n",
      "            Supports\n",
      "            * str, Image, Part,\n",
      "            * List[Union[str, Image, Part]],\n",
      "            * List[Content]\n",
      "        generation_config: Parameters for the generation.\n",
      "        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n",
      "        tools: A list of tools (functions) that the model can try calling.\n",
      "        tool_config: Config shared for all tools provided in the request.\n",
      "        stream: Whether to stream the response.\n",
      "    \n",
      "    Returns:\n",
      "        A single GenerationResponse object if stream == False\n",
      "        A stream of GenerationResponse objects if stream == True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(generative_multimodal_model.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b5c78a9-56c4-4ba2-8f9f-654c35ba45bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: ya29.c.c0ASRK0GazjrHy8T5oxOjgzmUAvpcPg6TrD2uCutKDaCN3dLuPEdjFLtvwDqGKVErmiHO-4iC92m7j-813QysLmaxrDYuXiRI07YF4Fb4YEJx5d8ignk4uyiWgPd7WZnO9CqjQ8HxSB7ZEqZEFuz_WmtVXeSGLyccCDAmgokqzj8CbwV5o0q2Cv2A6R9SMfoLcuYUL7HzUK5QZZLIwb14-cpwKzJzXj-l8UyxGm2Qz2hlhczUKMl--WAIkOmGODPKDCOpdSajsuGRbrKpw13MKrDuZl2HITFRE0nfF1pwClYlqTgt2H_V8OtIN8YYsvNAl-Z3f0bimuUDiA095RSnPipdSqFCN3ECwJrHa-iOmXU9SZhcvQJF7Bg_rrR0ANFRmzKCjuIDly_mHBtgqjjRVH413AshIg25-IsyVXvcg3zxUBkFyJdWI8OwZOSlt8y3Rx95FZsJ8cS7SinQOo2z5wX8Xl832FYb7Jlf1jf-jOUhgraMIk6qget7VmyiJ1me6RV82FMt2mqgx5hrSzQ0o34_03zFvwe2ZpQv0r78JBjZpg9wflsJd6ZqkQqFui8207ozoqRzStc4YU9157lzs3pagOxhV3Ju1OMp32ZROwc97hBys2tSj4ZRiFecXqFYIejMYX399dlZ-qkOrdlS0yqyk3iX3jcIOUox5-rqrOXbB8qh6hrUc4p9uv_WgvMUpYUqii85-4VmXU_czOZ7RYfYoFR6Xm73ZQrMVac3BVkSv73O5vMOrboZFazhcuzJx8c8v458i8nb336m9fdp5BXma4RpiUXVrUQcg7fi0YYJgRv6wcdd78wVBrVbxtMbr0QgZ5ItIBqprY7yv8t6Uc4kcQcMdv7MykUrI_1ObV-5Z9_QgFnt7Z9ouSBnyjyOypetS1hgunf6rU9eQY6U57f16mI-wlhqvJke__mwgkxWoyF0pml9kfrYttXyJZprSb3_wmy1pn95BW-35cF4Rdj9b_M3kBQB9coso3wx_sXYdWtslM7bdZRg\n"
     ]
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "try:\n",
    "    creds = GoogleCredentials.get_application_default()\n",
    "except Exception as e:\n",
    "    quit(e)\n",
    "\n",
    "print(\"Access Token:\", creds.get_access_token().access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aaf3493-2c3c-47be-9166-c64580d2a54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "from google.cloud import storage\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"idx\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"mime_type\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"gcs_uri\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_name\", \"STRING\", mode=\"REQUIRED\")\n",
    "       \n",
    "    ]     \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "\n",
    "@functions_framework.http\n",
    "def get_gcs_info(request):\n",
    "    \"\"\"\n",
    "        loads gcs metadata info into big query table\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "    #request_args = request.args\n",
    " \n",
    "\n",
    "#     project_id=  request_args['project_id']\n",
    "#     dataset_id=  request_args['dataset']\n",
    "#     table= request_args['table']\n",
    "#     region= request_args['region']\n",
    "#     source_bucket= request_args['source_bucket']\n",
    "#     source_folder= request_args['source_folder']\n",
    "#     media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "            \n",
    "    project_id=  \"nine-quality-test\"\n",
    "    dataset_id= 'vlt_metadata'\n",
    "    table= \"vlt_image_metadata\"\n",
    "    region= \"us-central1\"\n",
    "    source_bucket= 'raw_nine_files'\n",
    "    source_folder= \"2024\"\n",
    "    media_types= ['image/png']\n",
    "    \n",
    "            \n",
    "\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(source_bucket)\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder)\n",
    "    \n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()  \n",
    "    rows_to_insert=[]        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "\n",
    "\n",
    "    job_list=[]\n",
    "    idx=0\n",
    "    for blob in blobs:\n",
    "        if blob.content_type in media_types:\n",
    "            rows_to_insert.append(\n",
    "                                {   \"idx\":  idx  , \n",
    "                                    \"name\": blob.name, \n",
    "                                    \"mime_type\":blob.content_type,\n",
    "                                    \"gcs_uri\":  \"gs://\"+source_bucket+\"/\"+blob.name,\n",
    "                                    \"media_name\":blob.name.split(\"/\")[-1].replace(\".\"+blob.content_type.split(\"/\")[-1],\"\")\n",
    "                                    }\n",
    "                                            )\n",
    "        \n",
    "            idx=idx+1\n",
    "            \n",
    "    print(rows_to_insert)\n",
    "    #create table new if does not exist\n",
    "    table=f\"{table}\" \n",
    "    table_schema=create_table(project_id,dataset_id,table)\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema = table_schema\n",
    "    job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)                \n",
    "    job_list.append(job.job_id)\n",
    "  \n",
    "    return {'status':'SUCCESS', 'record_count':idx,'count_of_tables':1,'table_name_prefix':table,'jobs':job_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f20504-2cf7-4f75-a87c-5762526f0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nine-quality-test\n",
      "Dataset 'vlt_metadata' already exists.\n",
      "[{'idx': 0, 'name': '2024/3/d/screenshot_directrycontent.png', 'mime_type': 'image/png', 'gcs_uri': 'gs://raw_nine_files/2024/3/d/screenshot_directrycontent.png', 'media_name': 'screenshot_directrycontent'}]\n",
      "Table 'nine-quality-test.vlt_metadata.vlt_image_metadata' created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS',\n",
       " 'record_count': 1,\n",
       " 'count_of_tables': 1,\n",
       " 'table_name_prefix': TableReference(DatasetReference('nine-quality-test', 'vlt_metadata'), 'vlt_image_metadata'),\n",
       " 'jobs': ['acdf507c-f282-4d3f-bce7-89aad5c12d4d']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gcs_info('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99233f49-0aa6-4804-90c6-926596200397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtable_schema\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table_schema' is not defined"
     ]
    }
   ],
   "source": [
    "table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff1c98b0-8cfd-46a2-be07-271b944c01a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE MODEL `vlt_media_multimodal_embeddings_prelanding.multimodal`\n",
    "REMOTE WITH CONNECTION `us-central1.vlt_multimodal_endpoint`\n",
    "OPTIONS(endpoint = 'multimodalembedding@001');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49499f00-1668-47e0-a53b-50eb7c98e5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1852933557.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    CREATE OR REPLACE TABLE `bqml_tutorial.met_image_embeddings`\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "855c1a0f-cf8f-4f2c-9224-6c4d21ef4f23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE EXTERNAL TABLE `vlt_metadata.met_images`\n",
    "WITH CONNECTION `us-central1.vlt_multimodal_endpoint`\n",
    "OPTIONS\n",
    "  ( object_metadata = 'SIMPLE',\n",
    "    uris = ['gs://raw_nine_files/vlt_video_extract/OTHERS/*']\n",
    "    )\n",
    "\"\"\"\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a236e34f-ddc6-482b-bd15-60a29072fd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings`\n",
    "AS\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.GENERATE_EMBEDDING(\n",
    "    MODEL `vlt_metadata.multimodal`,\n",
    "    (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT 1000));\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb5bb44c-6dda-445b-9c47-13aa382ceb32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings1`\n",
    "AS\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.GENERATE_EMBEDDING(\n",
    "    MODEL `vlt_metadata.multimodal`,\n",
    "    (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT 1000));\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.create_job( job_config={\n",
    "        \"query\": {\n",
    "            \"query\": create_model_sql,\n",
    "        },\n",
    "        \"labels\": {\"example-label\": \"example-value\"},\n",
    "        \"maximum_bytes_billed\": 10000000,\n",
    "    })\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed19a49-a4c3-42f5-9a68-7c506bde87d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bd806266-d3e0-4264-b7f1-737ece17ae8d'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_job.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7a2be05-5af6-4375-9795-3bd3e0ad5267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Initialize the loop variables\n",
    "offset = 0\n",
    "idx=0\n",
    "more_results = True\n",
    "job_list=[]\n",
    "row_count=1\n",
    "while more_results:\n",
    "    # Define your SQL CREATE MODEL statement\n",
    "    generate_mm_embedding_sql = f\"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings_{idx}`\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM\n",
    "      ML.GENERATE_EMBEDDING(\n",
    "        MODEL `vlt_metadata.multimodal`,\n",
    "        (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT {batch_size} OFFSET {offset}));\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the CREATE MODEL statement\n",
    "    job = client.create_job( job_config={\n",
    "            \"query\": {\n",
    "                \"query\": generate_mm_embedding_sql,\n",
    "            },\n",
    "            #\"labels\": {\"example-label\": \"example-value\"},\n",
    "            \"maximum_bytes_billed\": 10000000,\n",
    "        })\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job_list.append(job.job_id)\n",
    " \n",
    "    # Update offset for the next batch\n",
    "    offset += batch_size\n",
    "    idx =idx+1\n",
    "        \n",
    "    # Check if there are any results\n",
    "    if offset>row_count:\n",
    "        more_results = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bd66a-53a6-4487-b072-b76c7f52ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM ML.GENERATE_EMBEDDING(\n",
    "  MODEL `PROJECT_ID.DATASET_ID.MODEL_NAME`,\n",
    "  TABLE PROJECT_ID.DATASET_ID.TABLE_NAME,\n",
    "  STRUCT(TRUE AS flatten_json_output,\n",
    "    START_SECOND AS start_second,\n",
    "    END_SECOND AS end_second,\n",
    "    INTERVAL_SECONDS AS interval_seconds)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a1282-a56f-415e-af3a-99f2c4a981da",
   "metadata": {},
   "outputs": [],
   "source": [
    "${query+ \" SELECT *  FROM  ML.GENERATE_EMBEDDING ( MODEL\" +\"`\"+ METADATA_DATASET+\".\"+MODEL_NAME+\"`,\"+\n",
    "                                                    \" ( SELECT EVM.*  FROM `\"+METADATA_DATASET+\".\"+METADATA_TABLE+\"` EVM \"+\n",
    "                                                       \" INNER JOIN `\"+METADATA_DATASET+\".\"+VIDEO_METADATA_TABLE+\"` VM \"+\n",
    "                                                        \" ON EVM.uri=VM.gcs_uri \"+\n",
    "                                                        \" WHERE VW.duration>=\" + string(startSegment) +                                                    \n",
    "                                                        \" LIMIT \" +string(limit) +\" OFFSET \"+ string(offset) +\")),\"+\n",
    "                                                         \" STRUCT(TRUE AS flatten_json_output,\"+\n",
    "                                                            string(startSegment)+ \"  AS start_second,\"+\n",
    "                                                            string(endSegment)+ \" AS end_second,\"+\n",
    "                                                            string(INTERVALS) + \" AS interval_seconds)\"\n",
    "                                                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08ec5e-dfc0-49dc-b1aa-f4d1fc24ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    # Define your SQL CREATE MODEL statement\n",
    "    generate_mm_embedding_sql = f\"\"\"CREATE OR REPLACE TABLE `vlt_media_multimodal_embeddings_prelanding.vlt_video_multimodal_embeddings_test`\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM\n",
    "      ML.GENERATE_EMBEDDING(\n",
    "        MODEL `vlt_metadata.multimodal`,\n",
    "        (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images`\n",
    "         \n",
    "        \n",
    "        \n",
    "        LIMIT {batch_size} OFFSET {offset}));\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the CREATE MODEL statement\n",
    "    job = client.create_job( job_config={\n",
    "            \"query\": {\n",
    "                \"query\": generate_mm_embedding_sql,\n",
    "            },\n",
    "            #\"labels\": {\"example-label\": \"example-value\"},\n",
    "            \"maximum_bytes_billed\": 10000000,\n",
    "        })\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job_list.append(job.job_id)\n",
    " \n",
    "    # Update offset for the next batch\n",
    "    offset += batch_size\n",
    "    idx =idx+1\n",
    "        \n",
    "    # Check if there are any results\n",
    "    if offset>row_count:\n",
    "        more_results = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "742390ce-a138-4e55-9497-594eb38c85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document loaded.....\n",
      "document splitted.....\n",
      "nine-quality-test\n",
      "Dataset 'langchain_dataset' already exists.\n",
      "chunk done\n",
      "chucked_content_data_2024_10_24T014648231552Z_0\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0' created successfully.\n",
      "DONE\n",
      "Job 050f7e12-6091-4a84-a4d8-e069292d94ad completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0\n",
      "chucked_content_data_2024_10_24T014648231552Z_1\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_1' created successfully.\n",
      "DONE\n",
      "Job 90253b51-df5f-4d35-a93a-10f6d77c6925 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_1\n"
     ]
    }
   ],
   "source": [
    "import functions_framework\n",
    "\n",
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "\n",
    "def create_log_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a log table to keep the track of chunkings \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table. \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"asset_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema \n",
    "    \n",
    "    \n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"asset_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "  \n",
    "   \n",
    "  \n",
    "\n",
    "        \n",
    "    try:\n",
    "            project_id=  request_args['project_id']\n",
    "            dataset_id=  request_args['dataset']\n",
    "            table= request_args['table']\n",
    "            region= request_args['region']\n",
    "            metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "            page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "            source_query_str= request_args['source_query_str']\n",
    "            #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "            chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "            chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "            max_prompt_count_limit=30000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "\n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= \"chucked_content_data_2024_10_24T014648231552Z\"\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [\"asset_id\"]\n",
    "            page_content_columns= [\"HeadLine\",\"content\"]\n",
    "            source_query_str= \"\"\"\n",
    "           SELECT asset_id, headline as HeadLine, plain_text_column as Content  FROM `nine-quality-test.vlt_media_content_prelanding.vlt_article_content` ;\n",
    "            \"\"\"\n",
    "   \n",
    "            chunk_size= 1000\n",
    "            chunk_overlap= 100\n",
    "            max_prompt_count_limit=25000\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "             \n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print('document loaded.....')\n",
    "   # print(documents)\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "    \n",
    "    print('document splitted.....')\n",
    "\n",
    "    # # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    # now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"asset_id\"]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d')    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    print('chunk done')\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"asset_id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx  \n",
    "           \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"asset_id\"]\n",
    "                      \n",
    "             \n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = prefix+'_'+str(version)\n",
    "            #print(split.page_content)#.replace(page_content_columns[0]+\":\", \"\", 1).strip())\n",
    "            # print(chunk_idx)\n",
    "            # print()\n",
    "            if chunk_idx==1:                \n",
    "                 content=split.page_content  \n",
    "            else:                  \n",
    "                  content=page_content_columns[1]+\": \"+split.page_content \n",
    "                \n",
    "           # print(content)\n",
    "    \n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"asset_id\": split.metadata[\"asset_id\"], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\": content,#.replace(page_content_columns[0]+\":\", \"\", 1).strip(),\n",
    "                                   #\"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   #\"media_type\": split.metadata[\"media_type\"],\n",
    "                                   #\"path\": split.metadata[\"path\"],\n",
    "                                   #\"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "                \n",
    "                create_load_job()\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{prefix}_{version}\"\n",
    "                print(table)\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]              \n",
    "                #wait for the job to finish\n",
    "                job.result()\n",
    "                # Get job status \n",
    "                \n",
    "                print(job.state)\n",
    "                \n",
    "                if job.state == 'DONE':\n",
    "                    if job.error_result:\n",
    "                        print(f\"Job {job.job_id} for {table_id} failed with error: {job.error_result}\")\n",
    "                        job_execution_result['error_result']=job.error_result\n",
    "                        raise Exception(\"Sorry, no numbers below zero\")\n",
    "                    else:\n",
    "                        print(f\"Job {job.job_id} completed successfully. For \"+ table_id)\n",
    "                else:\n",
    "                    print(f\"Job {job.job_id} for {table_id} is still in progress.\")\n",
    "                \n",
    "                job_list.append(job_execution_result)\n",
    "                \n",
    "    if len(rows_to_insert)>0:\n",
    "        \n",
    "  \n",
    "    return   {'status':'SUCCESS', 'record_count':record_count,'count_of_tables':version+1 }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "\n",
    "        request_args={}\n",
    "    \n",
    "        if  'project_id' in os.environ:\n",
    "            request_args['project_id']= os.environ.get('project_id')\n",
    "            \n",
    "        if  'dataset' in os.environ:\n",
    "            request_args['dataset']= os.environ.get('dataset')\n",
    "            \n",
    "        if  'table' in os.environ:\n",
    "            request_args['table']= os.environ.get('table')\n",
    "            \n",
    "        if  'region' in os.environ:\n",
    "            request_args['region']= os.environ.get('region')\n",
    "            \n",
    "        if  'metadata_columns' in os.environ:\n",
    "            request_args['metadata_columns']= os.environ.get('metadata_columns')\n",
    "            \n",
    "        if  'page_content_columns' in os.environ:\n",
    "            request_args['page_content_columns']= os.environ.get('page_content_columns')\n",
    "            \n",
    "        if  'source_query_str' in os.environ:\n",
    "            request_args['source_query_str']= os.environ.get('source_query_str')\n",
    "            \n",
    "        if  'chunk_size' in os.environ:\n",
    "            request_args['chunk_size']= os.environ.get('chunk_size')\n",
    "            \n",
    "        if  'chunk_overlap' in os.environ:\n",
    "            request_args['chunk_overlap']= os.environ.get('chunk_overlap')\n",
    "            \n",
    "        if  'max_prompt_count_limit' in os.environ:\n",
    "            request_args['max_prompt_count_limit']= os.environ.get('max_prompt_count_limit')\n",
    "        \n",
    "        chunk_bq_content(request_args)\n",
    "            \n",
    "          \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c1e1b6b5-dc20-4dd1-9130-e629afefce24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document loaded.....\n",
      "document splitted.....\n",
      "nine-quality-test\n",
      "Dataset 'langchain_dataset' already exists.\n",
      "chunk done\n"
     ]
    }
   ],
   "source": [
    "if 1==1:\n",
    "    \n",
    "    status=''  \n",
    "  \n",
    "   \n",
    "  \n",
    "\n",
    "        \n",
    "    try:\n",
    "            project_id=  request_args['project_id']\n",
    "            dataset_id=  request_args['dataset']\n",
    "            table= request_args['table']\n",
    "            region= request_args['region']\n",
    "            metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "            page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "            source_query_str= request_args['source_query_str']\n",
    "            #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "            chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "            chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "            max_prompt_count_limit=30000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "\n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= \"chucked_content_data_2024_10_24T014648231552Z\"\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [\"asset_id\"]\n",
    "            page_content_columns= [\"HeadLine\",\"Content\"]\n",
    "            source_query_str= \"\"\"\n",
    "           SELECT asset_id, headline as HeadLine, plain_text_column as Content  FROM `nine-quality-test.vlt_media_content_prelanding.vlt_article_content` ;\n",
    "            \"\"\"\n",
    "   \n",
    "            chunk_size= 1000\n",
    "            chunk_overlap= 100\n",
    "            max_prompt_count_limit=25000\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "             \n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print('document loaded.....')\n",
    "   # print(documents)\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "    \n",
    "    print('document splitted.....')\n",
    "\n",
    "    # # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    # now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"asset_id\"]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d')    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    print('chunk done')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "f859bfd5-552f-446a-a8c4-df8596306fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62210"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0b7b7c-eb95-489e-915d-25d4e3c37408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded from source - \n",
      "           SELECT asset_id, headline as HeadLine, plain_text_column as Content  FROM `nine-quality-test.vlt_media_content_prelanding.vlt_article_content` ;\n",
      "            \n",
      "Documents splitted - chunk_size 1000, chunk_overlap 100\n",
      "nine-quality-test\n",
      "Dataset 'langchain_dataset' already exists.\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0' already exists.\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0' dropped successfully.\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0' created successfully.\n",
      "Job d39ce076-b79f-430b-81e8-d36752865c11 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_1' created successfully.\n",
      "Job e691f824-3f6e-4455-bae6-a64f93aece6d completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_1\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_2' created successfully.\n",
      "Job e51b4a98-ec09-4cd6-8cdd-60930a81b7c2 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_2\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_3' created successfully.\n",
      "Job 6c760e26-0c26-4a64-8eb3-eeb07e1174b9 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_3\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_4' created successfully.\n",
      "Job 89b94c2c-fae0-41fd-bc09-9816b6d92b2c completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_4\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_5' created successfully.\n",
      "Job 82e9266c-127c-4454-8bce-bccb27dc4814 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_5\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_6' created successfully.\n",
      "Job a8b6dc93-4211-4d3c-9c22-f3e10f421d3f completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_6\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_7' created successfully.\n",
      "Job f2fbc331-48b7-4847-a353-6a288f9b4c21 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_7\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_8' created successfully.\n",
      "Job f68a3394-7b92-4da0-83e5-458973d1c5e1 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_8\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_9' created successfully.\n",
      "Job ad51741f-8adc-4865-b720-4a0a725b6193 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_9\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_10' created successfully.\n",
      "Job c4cc1dff-9270-495a-934d-7b527ccdc1ba completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_10\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_11' created successfully.\n",
      "Job 2c83c2a0-38ca-478a-bd40-00b444f6b9f5 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_11\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_12' created successfully.\n",
      "Job 70d5551e-0e23-4427-a975-5e5d820e0aa4 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_12\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_13' created successfully.\n",
      "Job 51b9d66d-3cb9-4695-b15e-b02d200570cb completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_13\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_14' created successfully.\n",
      "Job 81cb6508-abe9-4ca4-8855-fda8c4706a55 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_14\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "   \n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"asset_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "def load_into_bq(table_prefix: str,project_id: str,dataset_id: str, region: str, data: list,  version: int):  \n",
    "    \"\"\"\n",
    "        Create a job to load data into big query table and wait for the job to finish \n",
    "        \n",
    "        Args:\n",
    "           str table_prefix: the prefix of biquery table name\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           list(json) data: list of json records that should be inserted into table.\n",
    "     \n",
    "             \n",
    "    \"\"\"    \n",
    "    client = bigquery.Client(project_id)    \n",
    "   \n",
    "    \n",
    "    #create table new if does not exist\n",
    "    table=f\"{table_prefix}_{version}\"\n",
    "    table_schema=create_table(project_id,dataset_id,table)\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema = table_schema\n",
    "    job = client.load_table_from_json(data, table, job_config = job_config)\n",
    "              \n",
    "    #wait for the job to finish\n",
    "    job.result()\n",
    "    # Get job status   \n",
    "                \n",
    "    if job.state == 'DONE':\n",
    "        if job.error_result:\n",
    "             print(f\"Job {job.job_id} for {table_id} failed with error: {job.error_result}\")\n",
    "             raise Exception(\"Sorry, no numbers below zero\")\n",
    "        else:\n",
    "             print(f\"Job {job.job_id} completed successfully. For \"+ table_id)\n",
    "    else:\n",
    "            print(f\"Job {job.job_id} for {table_id} is still in progress.\")\n",
    "                \n",
    "\n",
    "\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "  \n",
    "   \n",
    " \n",
    "\n",
    "    try:\n",
    "            project_id=  request_args['project_id']\n",
    "            dataset_id=  request_args['dataset']\n",
    "            table= request_args['table']\n",
    "            region= request_args['region']\n",
    "            metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "            page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "            source_query_str= request_args['source_query_str']\n",
    "            #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "            chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "            chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "            max_prompt_count_limit=30000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "\n",
    "\n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= \"chucked_content_data_2024_10_24T014648231552Z\"\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [\"asset_id\"]\n",
    "            page_content_columns= [\"HeadLine\",\"Content\"]\n",
    "            source_query_str= \"\"\"\n",
    "           SELECT asset_id, headline as HeadLine, plain_text_column as Content  FROM `nine-quality-test.vlt_media_content_prelanding.vlt_article_content` ;\n",
    "            \"\"\"\n",
    "   \n",
    "            chunk_size= 1000\n",
    "            chunk_overlap= 100\n",
    "            max_prompt_count_limit=25000\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "             \n",
    "                \n",
    "            \n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print(f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    print(f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    # now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[metadata_columns[0]]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d') \n",
    "    now = datetime.now()    \n",
    "    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "   \n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[metadata_columns[0]]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[metadata_columns[0]]\n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = prefix+'_'+str(version)\n",
    "            \n",
    "            if chunk_idx==1:                \n",
    "                 content=split.page_content  \n",
    "            else:                  \n",
    "                  content=page_content_columns[1]+\": \"+split.page_content \n",
    "                \n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"asset_id\": split.metadata[metadata_columns[0]], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\": content,#.replace(page_content_columns[0]+\":\", \"\", 1).strip(),\n",
    "                                   #\"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   #\"media_type\": split.metadata[\"media_type\"],\n",
    "                                   #\"path\": split.metadata[\"path\"],\n",
    "                                   #\"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:  \n",
    "                load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]   \n",
    "               \n",
    "                \n",
    "    if  len(rows_to_insert)>0:       \n",
    "        load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "        #moving to next batch\n",
    "        record_count=record_count+len(rows_to_insert)\n",
    "        rows_to_insert=[]   \n",
    "                \n",
    "    \n",
    "  \n",
    "    return   {'status':'SUCCESS', 'record_count':record_count,'count_of_tables':version+1 }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "\n",
    "        request_args={}\n",
    "        if  'project_id' in os.environ:\n",
    "            request_args['project_id']= os.environ.get('project_id')\n",
    "            \n",
    "        if  'dataset' in os.environ:\n",
    "            request_args['dataset']= os.environ.get('dataset')\n",
    "            \n",
    "        if  'table' in os.environ:\n",
    "            request_args['table']= os.environ.get('table')\n",
    "            \n",
    "        if  'region' in os.environ:\n",
    "            request_args['region']= os.environ.get('region')\n",
    "            \n",
    "        if  'metadata_columns' in os.environ:\n",
    "            request_args['metadata_columns']= os.environ.get('metadata_columns')\n",
    "            \n",
    "        if  'page_content_columns' in os.environ:\n",
    "            request_args['page_content_columns']= os.environ.get('page_content_columns')\n",
    "            \n",
    "        if  'source_query_str' in os.environ:\n",
    "            request_args['source_query_str']= os.environ.get('source_query_str')\n",
    "            \n",
    "        if  'chunk_size' in os.environ:\n",
    "            request_args['chunk_size']= os.environ.get('chunk_size')\n",
    "            \n",
    "        if  'chunk_overlap' in os.environ:\n",
    "            request_args['chunk_overlap']= os.environ.get('chunk_overlap')\n",
    "            \n",
    "        if  'max_prompt_count_limit' in os.environ:\n",
    "            request_args['max_prompt_count_limit']= os.environ.get('max_prompt_count_limit')\n",
    "        \n",
    "        chunk_bq_content(request_args)\n",
    "            \n",
    "          \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "080d22a5-2af1-49a2-9cac-fbd476a096c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.batch_prediction import BatchPredictionJob\n",
    "for job in BatchPredictionJob.list():\n",
    "    print(job.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b2439ab8-533e-4352-8030-08b6e9581de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 13) (2251090377.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[289], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Job ID: {job.job_id}, State: {job.state}, Created: {job.created},  Ended: {job.ended}   )\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 13)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "# List jobs for the project\n",
    "jobs = client.list_jobs()\n",
    "\n",
    "for job in jobs:\n",
    "    # try:\n",
    "    #     print(job.statement_type)\n",
    "    # except:\n",
    "    #     continue\n",
    "    if (job.job_type == 'query' and job.statement_type == 'ML.GENERATE_EMBEDDING') or (job.job_type == 'query' and 'ML.GENERATE_EMBEDDING' in job.query):\n",
    "        print(f\"Job ID: {job.job_id}, State: {job.state}, Created: {job.created},  Ended: {job.ended}   )\n",
    "              break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17da75bd-1d51-46d3-b85f-f8ac8984113f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'project_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 307\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_prompt_count_limit\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    305\u001b[0m     request_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_prompt_count_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_prompt_count_limit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mchunk_bq_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 154\u001b[0m, in \u001b[0;36mchunk_bq_content\u001b[0;34m(request_args)\u001b[0m\n\u001b[1;32m    148\u001b[0m status\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m project_id\u001b[38;5;241m=\u001b[39m  \u001b[43mrequest_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproject_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    155\u001b[0m dataset_id\u001b[38;5;241m=\u001b[39m  request_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    156\u001b[0m table\u001b[38;5;241m=\u001b[39m request_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'project_id'"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "   \n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"asset_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "def load_into_bq(table_prefix: str,project_id: str,dataset_id: str, region: str, data: list,  version: int):  \n",
    "    \"\"\"\n",
    "        Create a job to load data into big query table and wait for the job to finish \n",
    "        \n",
    "        Args:\n",
    "           str table_prefix: the prefix of biquery table name\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           list(json) data: list of json records that should be inserted into table.\n",
    "     \n",
    "             \n",
    "    \"\"\"    \n",
    "    client = bigquery.Client(project_id)    \n",
    "   \n",
    "    \n",
    "    #create table new if does not exist\n",
    "    table=f\"{table_prefix}_{version}\"\n",
    "    table_schema=create_table(project_id,dataset_id,table)\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema = table_schema\n",
    "    job = client.load_table_from_json(data, table, job_config = job_config)\n",
    "              \n",
    "    #wait for the job to finish\n",
    "    job.result()\n",
    "    # Get job status   \n",
    "                \n",
    "    if job.state == 'DONE':\n",
    "        if job.error_result:\n",
    "             print(f\"Job {job.job_id} for {table_id} failed with error: {job.error_result}\")\n",
    "             raise Exception(\"Sorry, no numbers below zero\")\n",
    "        else:\n",
    "             print(f\"Job {job.job_id} completed successfully. For \"+ table_id)\n",
    "    else:\n",
    "            print(f\"Job {job.job_id} for {table_id} is still in progress.\")\n",
    "                \n",
    "\n",
    "\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "  \n",
    "   \n",
    " \n",
    "\n",
    "    # try:\n",
    "    project_id=  request_args['project_id']\n",
    "    dataset_id=  request_args['dataset']\n",
    "    table= request_args['table']\n",
    "    region= request_args['region']\n",
    "    metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "    page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "    source_query_str= request_args['source_query_str']\n",
    "    #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "    chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "    chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "    max_prompt_count_limit=30000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "\n",
    "\n",
    "#     except:        \n",
    "#             project_id= 'nine-quality-test' \n",
    "#             dataset_id= 'langchain_dataset'\n",
    "#             table= \"chucked_content_data_2024_10_24T014648231552Z\"\n",
    "#             region= 'us-central1'\n",
    "#             metadata_columns= [\"asset_id\"]\n",
    "#             page_content_columns= [\"HeadLine\",\"Content\"]\n",
    "#             source_query_str= \"\"\"\n",
    "#            SELECT asset_id, headline as HeadLine, plain_text_column as Content  FROM `nine-quality-test.vlt_media_content_prelanding.vlt_article_content` ;\n",
    "#             \"\"\"\n",
    "   \n",
    "#             chunk_size= 1000\n",
    "#             chunk_overlap= 100\n",
    "#             max_prompt_count_limit=25000\n",
    "#             #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "             \n",
    "                \n",
    "            \n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print(f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    print(f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    # now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[metadata_columns[0]]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d') \n",
    "    now = datetime.now()    \n",
    "    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "   \n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[metadata_columns[0]]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[metadata_columns[0]]\n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = prefix+'_'+str(version)\n",
    "            \n",
    "            if chunk_idx==1:                \n",
    "                 content=split.page_content  \n",
    "            else:                  \n",
    "                  content=page_content_columns[1]+\": \"+split.page_content \n",
    "                \n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"asset_id\": split.metadata[metadata_columns[0]], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\": content,#.replace(page_content_columns[0]+\":\", \"\", 1).strip(),\n",
    "                                   #\"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   #\"media_type\": split.metadata[\"media_type\"],\n",
    "                                   #\"path\": split.metadata[\"path\"],\n",
    "                                   #\"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:  \n",
    "                load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]   \n",
    "               \n",
    "                \n",
    "    if  len(rows_to_insert)>0:       \n",
    "        load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "        #moving to next batch\n",
    "        record_count=record_count+len(rows_to_insert)\n",
    "        rows_to_insert=[]   \n",
    "                \n",
    "    \n",
    "  \n",
    "    return   {'status':'SUCCESS', 'record_count':record_count,'count_of_tables':version+1 }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "\n",
    "        request_args={}\n",
    "        if  'project_id' in os.environ:\n",
    "            request_args['project_id']= os.environ.get('project_id')\n",
    "            \n",
    "        if  'dataset' in os.environ:\n",
    "            request_args['dataset']= os.environ.get('dataset')\n",
    "            \n",
    "        if  'table' in os.environ:\n",
    "            request_args['table']= os.environ.get('table')\n",
    "            \n",
    "        if  'region' in os.environ:\n",
    "            request_args['region']= os.environ.get('region')\n",
    "            \n",
    "        if  'metadata_columns' in os.environ:\n",
    "            request_args['metadata_columns']= os.environ.get('metadata_columns')\n",
    "            \n",
    "        if  'page_content_columns' in os.environ:\n",
    "            request_args['page_content_columns']= os.environ.get('page_content_columns')\n",
    "            \n",
    "        if  'source_query_str' in os.environ:\n",
    "            request_args['source_query_str']= os.environ.get('source_query_str')\n",
    "            \n",
    "        if  'chunk_size' in os.environ:\n",
    "            request_args['chunk_size']= os.environ.get('chunk_size')\n",
    "            \n",
    "        if  'chunk_overlap' in os.environ:\n",
    "            request_args['chunk_overlap']= os.environ.get('chunk_overlap')\n",
    "            \n",
    "        if  'max_prompt_count_limit' in os.environ:\n",
    "            request_args['max_prompt_count_limit']= os.environ.get('max_prompt_count_limit')\n",
    "        \n",
    "        chunk_bq_content(request_args)\n",
    "            \n",
    "          \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e24c6f-d026-4648-9028-857c1626752f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
