{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835b64-eab5-4b12-8cb8-946b554ab1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8b4a0-9336-4ca8-8483-4ba688f4628c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db12b4e8-a141-4be8-897b-23d9cfc1d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.147.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-google-vertexai \"langchain-google-community[featurestore]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccbd0b78-00e8-47a4-aa70-0b9eba58f44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import google.cloud.bigquery as bq\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader,BigQueryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "from datetime import datetime\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "857e7fd4-aeae-445d-af46-0e1dfa8f6e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define our query\n",
    "query = \"\"\"\n",
    "SELECT id,media_type,content,test_metadata \n",
    "FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "loader = BigQueryLoader(\n",
    "    query, metadata_columns=[\"id\"], page_content_columns=[\"content\",\"media_type\",\"test_metadata\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da00fa47-16dc-46c4-a501-291698ff1508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "\n",
    "\n",
    "store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08949b5-f302-40d7-b2a9-3a73db018bd7",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/rag-with-bigquery-and-langchain-in-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ea0c737-71e6-4126-a6cb-7ab71393705f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "documents.extend(loader.load())\n",
    " \n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5#,\n",
    "   # separators=[\"\\n\\n\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# get current processing time to add it to metadata, datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Add chunk number to metadata\n",
    "chunk_idx=0\n",
    "prev=doc_splits[0].metadata[\"id\"]\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"process_time\"]=now\n",
    "    if prev==split.metadata[\"id\"]:\n",
    "       split.metadata[\"chunk\"] = chunk_idx      \n",
    "    else:\n",
    "        chunk_idx=0\n",
    "        split.metadata[\"chunk\"] = chunk_idx\n",
    "        prev=split.metadata[\"id\"]\n",
    "    chunk_idx +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f1cca1e-efc9-4f5d-ad4e-80d0e3b4d42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ") \n",
    "\n",
    "_=bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02ba5ed-5edd-4bcc-8b23-21272d2d4dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BASIC_FORMAT',\n",
       " 'BufferingFormatter',\n",
       " 'CRITICAL',\n",
       " 'DEBUG',\n",
       " 'ERROR',\n",
       " 'FATAL',\n",
       " 'FileHandler',\n",
       " 'Filter',\n",
       " 'Filterer',\n",
       " 'Formatter',\n",
       " 'Handler',\n",
       " 'INFO',\n",
       " 'LogRecord',\n",
       " 'Logger',\n",
       " 'LoggerAdapter',\n",
       " 'Manager',\n",
       " 'NOTSET',\n",
       " 'NullHandler',\n",
       " 'PercentStyle',\n",
       " 'PlaceHolder',\n",
       " 'RootLogger',\n",
       " 'StrFormatStyle',\n",
       " 'StreamHandler',\n",
       " 'StringTemplateStyle',\n",
       " 'Template',\n",
       " 'WARN',\n",
       " 'WARNING',\n",
       " '_STYLES',\n",
       " '_StderrHandler',\n",
       " '__all__',\n",
       " '__author__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__date__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__status__',\n",
       " '__version__',\n",
       " '_acquireLock',\n",
       " '_addHandlerRef',\n",
       " '_after_at_fork_child_reinit_locks',\n",
       " '_at_fork_reinit_lock_weakset',\n",
       " '_checkLevel',\n",
       " '_defaultFormatter',\n",
       " '_defaultLastResort',\n",
       " '_handlerList',\n",
       " '_handlers',\n",
       " '_levelToName',\n",
       " '_lock',\n",
       " '_logRecordFactory',\n",
       " '_loggerClass',\n",
       " '_nameToLevel',\n",
       " '_register_at_fork_reinit_lock',\n",
       " '_releaseLock',\n",
       " '_removeHandlerRef',\n",
       " '_showwarning',\n",
       " '_srcfile',\n",
       " '_startTime',\n",
       " '_str_formatter',\n",
       " '_warnings_showwarning',\n",
       " 'addLevelName',\n",
       " 'atexit',\n",
       " 'basicConfig',\n",
       " 'captureWarnings',\n",
       " 'collections',\n",
       " 'config',\n",
       " 'critical',\n",
       " 'currentframe',\n",
       " 'debug',\n",
       " 'disable',\n",
       " 'error',\n",
       " 'exception',\n",
       " 'fatal',\n",
       " 'getLevelName',\n",
       " 'getLogRecordFactory',\n",
       " 'getLogger',\n",
       " 'getLoggerClass',\n",
       " 'handlers',\n",
       " 'info',\n",
       " 'io',\n",
       " 'lastResort',\n",
       " 'log',\n",
       " 'logMultiprocessing',\n",
       " 'logProcesses',\n",
       " 'logThreads',\n",
       " 'makeLogRecord',\n",
       " 'os',\n",
       " 'raiseExceptions',\n",
       " 're',\n",
       " 'root',\n",
       " 'setLogRecordFactory',\n",
       " 'setLoggerClass',\n",
       " 'shutdown',\n",
       " 'sys',\n",
       " 'threading',\n",
       " 'time',\n",
       " 'traceback',\n",
       " 'warn',\n",
       " 'warning',\n",
       " 'warnings',\n",
       " 'weakref']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9714ce-acba-4d90-818f-c3d97fce3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "    \n",
    "def chunk_and_embedding(project_id: str= None, dataset: str= None, table: str= None, region: str =None,\\\n",
    "                        metadata_columns: list[str]=None, page_content_columns: list[str]= None, \\\n",
    "                        source_query_str: str= None, separators:  list[str]=None, chunk_size: int=None, \\\n",
    "                       chunk_overlap: int=0):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    embedding = VertexAIEmbeddings(\n",
    "        model_name=\"textembedding-gecko@latest\", project=project_id\n",
    "    )\n",
    "\n",
    "   \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "        \n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    \n",
    "    bq_store = BigQueryVectorStore(\n",
    "        project_id=project_id,\n",
    "        dataset_name=dataset,\n",
    "        table_name=table,\n",
    "        location=region,\n",
    "        embedding=embedding,\n",
    "    ) \n",
    "    logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "    _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "    logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "    chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\n\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "    message=chunk_and_embedding(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "                        metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "                        source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "                       chunk_overlap=chunk_overlap)\n",
    "    print(message)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0424463-a90c-42a9-bb30-99f3a8eded81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05dbe9-3eda-409d-9821-526491f7b66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
