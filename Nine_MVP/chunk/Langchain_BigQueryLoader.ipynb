{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835b64-eab5-4b12-8cb8-946b554ab1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8b4a0-9336-4ca8-8483-4ba688f4628c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db12b4e8-a141-4be8-897b-23d9cfc1d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.147.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-google-vertexai \"langchain-google-community[featurestore]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccbd0b78-00e8-47a4-aa70-0b9eba58f44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import google.cloud.bigquery as bq\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader,BigQueryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "from datetime import datetime\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "857e7fd4-aeae-445d-af46-0e1dfa8f6e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_710571/1683864506.py:8: LangChainDeprecationWarning: The class `BigQueryLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import BigQueryLoader``.\n",
      "  loader = BigQueryLoader(\n"
     ]
    }
   ],
   "source": [
    "# Define our query\n",
    "query = \"\"\"\n",
    "SELECT id,media_type,content,test_metadata \n",
    "FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "loader = BigQueryLoader(\n",
    "    query, metadata_columns=[\"id\"], page_content_columns=[\"content\",\"media_type\",\"test_metadata\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da00fa47-16dc-46c4-a501-291698ff1508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "\n",
    "\n",
    "store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08949b5-f302-40d7-b2a9-3a73db018bd7",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/rag-with-bigquery-and-langchain-in-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ea0c737-71e6-4126-a6cb-7ab71393705f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "documents.extend(loader.load())\n",
    " \n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5#,\n",
    "   # separators=[\"\\n\\n\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# get current processing time to add it to metadata, datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Add chunk number to metadata\n",
    "chunk_idx=0\n",
    "prev=doc_splits[0].metadata[\"id\"]\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"process_time\"]=now\n",
    "    if prev==split.metadata[\"id\"]:\n",
    "       split.metadata[\"chunk\"] = chunk_idx      \n",
    "    else:\n",
    "        chunk_idx=0\n",
    "        split.metadata[\"chunk\"] = chunk_idx\n",
    "        prev=split.metadata[\"id\"]\n",
    "    chunk_idx +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f1cca1e-efc9-4f5d-ad4e-80d0e3b4d42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ") \n",
    "\n",
    "_=bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e9714ce-acba-4d90-818f-c3d97fce3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load():\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "    \n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "    chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\n\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     project_id= os.environ.get(\"PROJECT_ID\") \n",
    "#     dataset= os.environ.get(\"DATASET\")  \n",
    "#     table= os.environ.get(\"TABLE\") \n",
    "#     region= os.environ.get(\"REGION\") \n",
    "#     metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "#     page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "#     source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "#     separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "#     chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "#     chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "#     project_id= 'nine-quality-test' \n",
    "#     dataset= 'my_langchain_dataset'\n",
    "#     table= 'doc_and_vectors'\n",
    "#     region= 'us-central1'\n",
    "#     metadata_columns= \"id\".split(\",\")\n",
    "#     page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "#     source_query_str= \"\"\"\n",
    "#     SELECT id,media_type,content,test_metadata \n",
    "#     FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "#     \"\"\"\n",
    "#     separators= \"\\n\\n\"\n",
    "#     chunk_size= 25\n",
    "#     chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "#     message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "#                         metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "#                         source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "#                        chunk_overlap=chunk_overlap)\n",
    "#     return(message)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0424463-a90c-42a9-bb30-99f3a8eded81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b05dbe9-3eda-409d-9821-526491f7b66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "\n",
    "def list_sub_directories(bucket_name, prefix):\n",
    "    \"\"\"Returns a list of sub-directories within the given bucket.\"\"\"\n",
    "    service = googleapiclient.discovery.build('storage', 'v1')\n",
    "\n",
    "    req = service.objects().list(bucket=bucket_name, prefix=prefix, delimiter='/')\n",
    "    \n",
    "    res = req.execute()\n",
    "    \n",
    "    return res['prefixes']\n",
    "\n",
    "# For the example (gs://abc/xyz), bucket_name is 'abc' and the prefix would be 'xyz/'\n",
    "#print(list_sub_directories(bucket_name='raw_nine_files/sub_dir', prefix=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46029a1-d72b-4ee2-a594-848a5bfa4c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023/', '2024/']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_sub_directories(bucket_name=bucket, prefix=prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee98b19e-ae8d-423a-8756-102822b0c081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/1/a/\n",
      "2023/1/b/\n",
      "2023/2/c/\n",
      "2024/3/d/\n"
     ]
    }
   ],
   "source": [
    "bucket='raw_nine_files'\n",
    "prefix=''\n",
    "l=[]\n",
    "for main in list(list_sub_directories(bucket_name=bucket, prefix=prefix)):\n",
    "    for dircty in list(list_sub_directories(bucket_name=bucket, prefix=main )):\n",
    "       for sub_dircty in list(list_sub_directories(bucket_name=bucket, prefix=dircty )):\n",
    "           print(sub_dircty)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "f6048d2c-dd54-46b8-b181-ade334045925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_gcs_objects(bucket_name):\n",
    "    \"\"\"List all objects in a Google Cloud Storage bucket and print their metadata.\"\"\"\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=\"vlt_video_extract\")\n",
    "\n",
    "    print(f\"Objects in bucket '{bucket_name}':\")\n",
    "    for blob in blobs:\n",
    "        print(f\"Name: {blob.name}\")\n",
    "        print(f\"Size: {blob.size} bytes\")\n",
    "        print(f\"Content Type: {blob.content_type}\")\n",
    "        print(f\"Media Link: {blob.public_url}\")  # Get public URL if the object is publicly accessible\n",
    "        print(f\"Created: {blob.time_created}\")\n",
    "        print(f\"Updated: {blob.updated}\")\n",
    "        print(f\"Updated: {blob.self_link}\")\n",
    "        print(f\"Updated: {blob.get}\")\n",
    "        #print(dir(blob))\n",
    "        \n",
    "        print(\"------\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f197a8-25b7-416a-a2a6-f04a6fd5da4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5634.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_duration(\"raw_nine_files\",\"vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced9a6e-a9a9-4f8e-9e39-5d410c5e7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ffcfb4b138e4ddd2456a59167c84bf4021642dff6ad6de28e57d52e-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/2024%2F3%2Fd%2Fscreenshot_directrycontent.png?jk=AdvOr8tEXuKMrGk7hapfYy4ZPHk1yEDCUJKOHuCrx660bHpMtnBM9ujMQYZYlGC3uD9x2FfhMgvBZNcuZcVX6z301E7Zm2RjL3pJkSIu8paY66HCL6H6SLrZ4R1eCjkaxareY7JnsKrMof1amj819hbOcCMgyGfuRm74dLtW501kSwMiUwyd_6yYs6EeoeG38UVmQBiioRHQgCSMdDBu3Ej3hAwXvcIPMfCIhqvjQD7uD1OFecUTJx40xk_o_Yiy-uMA5XNsHRBiPtZ3KnUphFw5BIxArUX3DDMPFpEySCHtPIb67btv3H-FXW8MvV_1HBkw3V-xlDt3lKggVN4WRq3IeS6cF0xFD25jpDSYA-jBKL6f_U0wprpd3918-ldVEE77DysLlD7EnhIn_KErSpWi9qYWnsvqftvC3R-4iJUkehhUjtUiefJrA0uEJsZoSEeevzM_0jwtMY2Rb4LCzroTIYLDwRc4n45ILr13Rev_ATTsSx6ccGzxhfRClVaHrBRY7udek7EXIPJAx8m8MkyQCfKScX4GoWQqmPhn3hNd0LuW44r5xpO-ljrOzsBndeswe0gtwM53ns9ObCMznjoJo3DeGKFxKxsIUxmoSRaLP4bpvz7YgZEIqvrS8Mt1ZxHEXG98_meWlYyPd3A2ir7G7ZLQcxiIBm7-IKms2rmapPc3u5bM69KhjQNR93HYRxCXGzQnIhR8HYXzj9P1_6INOw3UVkBdIYX-jEFsNhkzHHTEIqrmDW_76xQ_wSrr_dOh3Oi8Ri12DXaXksMFBvbJhUGmgETi0QKfqd8EI-ReT3BbZVChbAIJ1RNUe-wd7guT8mKdpxTicWK0KpMnGaU3DL8tlY9GZwpFyRDmJRHwBP7iqVjU4M4jF2KyHwdL589yyxlheVn-fmItOag0vPxkysf4pX0qL52WNP3ch15tN2D8LePFp4S__rQwXD-vdsciDDXBH3DOeLeKWR4-h6lVkYxi6XtHVbLgOE26lurqe1Q0XQQcJYA1usDNdJUNYwuW1qYGiB7VH6PDYSrSHuVj2YB3bmsGxpxOf2vxrhhJZzIhKU3Yxy-BX-tXp0AK8C4oowhu66feVTx6dIl_d3xQlOinsq5kEbsxvb9aSSKPkuNkfFLTbksJhW2-QWHnv4Bwtf7uC6zMLxVfocy-iCZAMTkGa9icvrHJeT3htPvpTQYbHq_kOeTVZUj3Hw-LRcaUQnxQfOTLVWmGXBAyWhWr28l5zblWxYxGMXMP&isca=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b19e03-d39e-40e9-be2e-4aa2ec427834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Duration: 5634.0\n"
     ]
    }
   ],
   "source": [
    "#video_url = \"https://storage.cloud.google.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\"\n",
    "video_url=\"https://ffba87f3998dc58bc084d165a1de6442fbc82dae22183352ca8b1bc-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4?jk=AdvOr8stvY3hRXD9psznNOnR68vfY46qQuSRwB7uO7udZdeU2LGGz-_54PmfT-yobQGxdyq1OJd1dnjB-KHuYc64Ky3hZ45dZPi3LocJ9kOpF2E7EAfzzbh1tnu855pcL63SAw-jIOe4AKQdfRQcXxO-KhuhEkUxHY9jthAzHPVv7wYH5zPsrKLuKIZzlfqJ-kh2KYLi-HlC9SDa9FtEPZbHXSuJZvlUXZK9exW_8Qnxlo5EtNLP386be6siB0rndlMgdKeHPTcO3r7RpXPlfeFm6hQuWDpe-vAW6wRz-BJzlnXOc1-cdXQ7_2Hz85JrdQ6SO-7Sf9fG7Nhk-wSLWGf_9hzaD4AaaVqhCJqmojJGQ1NID5iLry10QGX6r2Ce23Rp0P0dicWb0ePc6BygOVOPM2NZZWyHQO0OJB63aaXsz5MAFTA7dU0oQsz36-d4NzwcEiIfb4Sv86r5uowgeS_pBBOJrv0Y3NlmF1BJ_Eq5bN-q10S0wJ040NMQNs-fWfDKSSCBd-gWovKWk16r4DLSOvt10-EVfNWs8qHEnI9VJB_3E03XxhrONQxX2VqQoCnPMYcAob1lbfezAefR9dpy6xoXvcoB_GajqVoBBLlF6lk8cX69YxsL_gdSzNVJIbRMwVYfdipLKhXxiipUJM3GuB-OlD9OwKjpbYK2hGCxN0GR_pY5mrRREka8FAsxMpZvmEXEgqwYKwMPRCHmtfRj4uAyubVUPP-WCBEQQ_0r8VG1IBFqhLLrM4rqrWwxM5Vejjo6NPS6QP2Va5sOfptdcyLqTXpaNBesHkrO_hyC4ClOtwkhx1-x8rTPHat8VkUHUcPGHug2aVHp8l-zzTs-puLcF4N24hfkD06MVqbUWslkKHXTUiEbRiIOgg854ObQ976NsZI8GPeDjIIbCYzCZyDGUR9lYcJ98oQlopCEtPZsKrwUE-nV40WvQ2ABXGrW3szWrOlaOPQDwmwCTTSgWvlyxWlqI8txdIV3B1pccav3bkOiqlz7e2BcBc6rxaxu_P4Xy9_ZBGggkFO7dIiBizs0O6kkWoFib5985-bXbEvXNdeClyhuTSG9JML5-EiQqMvfm0_uW69Y9LtAUw9QwlMg6AP9nZytgw1aPBTDJlU6nZHM4ARrS7-Tv9QV3eUIGANS466_mAPP-M5wR3sWIy8fq7LhUCWmVxOqCt-iMgiqP6ZgyyRd6AgY1wAk-lshrySf1uDZV5xgeQrp8WhfNS19HNMexHSqreqPYIs&isca=1\"\n",
    "try:\n",
    "    clip = VideoFileClip(video_url)\n",
    "    print(\"Video Duration:\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba45b01-3702-4224-a881-cedccbd6daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from moviepy.editor import VideoFileClip\n",
    "import io\n",
    "\n",
    " \n",
    "# Initialize the storage client\n",
    "client = storage.Client( )\n",
    "\n",
    "# Set your bucket and file names\n",
    "bucket_name = 'raw_nine_files'\n",
    "object_name = 'vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4'\n",
    "\n",
    "# Create a file-like object from GCS\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(object_name)\n",
    "\n",
    "# Download the blob as a byte stream\n",
    "video_stream = io.BytesIO()\n",
    "blob.download_to_file(video_stream)\n",
    "video_stream.seek(0)  # Reset stream position to the beginning\n",
    "\n",
    "# Use moviepy to get the video length from the stream\n",
    "try:\n",
    "    clip = VideoFileClip(video_stream)\n",
    "    print(\"Video Duration (seconds):\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "2c4fe292-1592-4b04-b153-140927adb4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in bucket 'raw_nine_files':\n",
      "Name: vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Size: 3613937486 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:14.024000+00:00\n",
      "Updated: 2024-10-07 01:41:14.024000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Size: 2748590138 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:14.209000+00:00\n",
      "Updated: 2024-10-07 01:41:14.209000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_21_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_21_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Size: 2508037349 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:14.432000+00:00\n",
      "Updated: 2024-10-07 01:41:14.432000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_36_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_36_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Size: 1094890707 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Created: 2024-10-07 01:41:14.648000+00:00\n",
      "Updated: 2024-10-07 01:41:14.648000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW24_21_A.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW24_21_A.mp4\n",
      "------\n",
      "Name: vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Size: 1109220796 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Created: 2024-10-07 01:41:14.853000+00:00\n",
      "Updated: 2024-10-07 01:41:14.853000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW724_21_A.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW724_21_A.mp4\n",
      "------\n",
      "Name: vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Size: 7183155286 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Created: 2024-10-07 01:41:15.056000+00:00\n",
      "Updated: 2024-10-07 01:41:15.056000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FOTHERS%2Fepisode-18-brs-full-story.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FOTHERS%2Fepisode-18-brs-full-story.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Size: 1744302481 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:15.254000+00:00\n",
      "Updated: 2024-10-07 01:41:15.254000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Size: 1743380331 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Created: 2024-10-07 01:41:15.461000+00:00\n",
      "Updated: 2024-10-07 01:41:15.461000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB_fixed.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB_fixed.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Size: 1850846344 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:15.655000+00:00\n",
      "Updated: 2024-10-07 01:41:15.655000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_1_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_1_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Size: 1694178315 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:15.847000+00:00\n",
      "Updated: 2024-10-07 01:41:15.847000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_6_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_6_A_HBB.mp4\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Call the function with your bucket name\n",
    "bucket_name = 'raw_nine_files'\n",
    "list_gcs_objects(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bfb29f21-1f9f-441e-8361-206b471d0af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in bucket raw_nine_files:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Blob' object has no attribute 'mediaLink'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function with your bucket name\u001b[39;00m\n\u001b[1;32m     21\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_nine_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mlist_gcs_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 18\u001b[0m, in \u001b[0;36mlist_gcs_objects\u001b[0;34m(bucket_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blob \u001b[38;5;129;01min\u001b[39;00m blobs:\n\u001b[1;32m     16\u001b[0m     blob \u001b[38;5;241m=\u001b[39m bucket\u001b[38;5;241m.\u001b[39mget_blob(blob\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmediaLink\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Blob' object has no attribute 'mediaLink'"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def list_gcs_objects(bucket_name):\n",
    "    \"\"\"List all objects in a Google Cloud Storage bucket.\"\"\"\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    print(f\"Objects in bucket {bucket_name}:\")\n",
    "    for blob in blobs:\n",
    "        blob = bucket.get_blob(blob.name)\n",
    "\n",
    "        print(blob.mediaLink)\n",
    "\n",
    "# Call the function with your bucket name\n",
    "bucket_name = 'raw_nine_files'\n",
    "list_gcs_objects(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2da52ee7-4a76-4d33-81bf-81e624583f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rows have been added.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load(project_id: str= None, dataset: str= None, table: str= None, region: str =None,\\\n",
    "                        metadata_columns: list[str]=None, page_content_columns: list[str]= None, \\\n",
    "                        source_query_str: str= None, separators:  list[str]=None, chunk_size: int=None, \\\n",
    "                       chunk_overlap: int=0):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "   \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= 1000 if str(os.environ.get(\"CHUNK_SIZE\"))==\"None\" else int(str(os.environ.get(\"CHUNK_SIZE\")))  \n",
    "    chunk_overlap= 0 if str(os.environ.get(\"CHUNK_OVERLAP\"))==\"None\" else int(str(os.environ.get(\"CHUNK_OVERLAP\")))\n",
    " \n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\\\n\\\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "    message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "                        metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "                        source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "                       chunk_overlap=chunk_overlap)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bfba5a73-0e9e-490d-a3f8-04c72069e550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS'}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0):\n",
    "     temp=request_file\n",
    "     client = storage.Client()\n",
    "     # Extract name to the temp file\n",
    "     temp_file = \"\".join([str(temp.name)])\n",
    "     # Uploading the temp image file to the bucket\n",
    "     dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+str(version)+\".json\" \n",
    "     dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "     dest_blob = dest_bucket.blob(dest_filename)\n",
    "     dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                                \n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None):\n",
    "    \"\"\"create batch request file(s) of up to 30000 and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            str path:  path to the gcs request file object\n",
    "          \n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix='2023')  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=30000\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version)\n",
    "                                rf.close()                                \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version)\n",
    " \n",
    "    return 1\n",
    " \n",
    "def create_batch_request_file( ):\n",
    " \n",
    "  \n",
    "  \n",
    "    dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "    source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "    request_file_prefix = \"image_request\"#request_args['request_file_prefix']\n",
    "    request_file_folder =  \"image_batch_request_fldr\"#request_args['request_file_folder']\n",
    "    prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "    media_types= ['image/jpeg','image/png']#list(request_args['media_type'])\n",
    "    request_content= \"image\"#request_args['request_content']\n",
    " \n",
    "    temperature=0.5\n",
    "    max_output_tokens=2048\n",
    "    top_p=50\n",
    "    top_k=0.5\n",
    "\n",
    "    if  request_content=='image':\n",
    "      _=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k\n",
    "                                      )  \n",
    "\n",
    "    return {\"status\":'SUCCESS'}\n",
    "\n",
    "create_batch_request_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c557d79-e327-410b-ad2d-843624a63db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                      \n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None, video_metadata_file: str=\"\"):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for video and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "    \n",
    "\n",
    "    # Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    blob = bucket.blob(video_metadata_file)\n",
    "\n",
    "    # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "    data = json.loads(blob.download_as_string(client=None))\n",
    "    video_metadata=data['items']\n",
    "    \n",
    "\n",
    "    segments_to_process=120 #segments duration\n",
    "    intervals=120#intervals\n",
    "    video_start =0 #where from video to start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for blob in blobs:                         \n",
    "        if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         metadata=list(filter(lambda element: element['gcs_uri'] ==gcsuri, video_metadata))[0]\n",
    "                         video_duration=metadata[\"videoOriginalDurationSecond\"]\n",
    "                         prev=video_start\n",
    "                         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                                offset={'start':prev, 'end':val}\n",
    "                                \n",
    "                                startOffset=offset['start']\n",
    "                                endOffset=offset['end']\n",
    "                                if endOffset>=video_duration:\n",
    "                                     endOffset=video_duration\n",
    "                                print(offset)\n",
    "                                prev=val  \n",
    "                                if index==0:\n",
    "                                    request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                                    rf= open(request_file.name, \"a\") \n",
    "                                \n",
    "                                 \n",
    "                                request_list=[\n",
    "                                       json.dumps(\n",
    "                                              {\n",
    "                                                \"request\": \n",
    "                                                     {\n",
    "                                                      \"contents\":  {\n",
    "                                                                        \"parts\": [\n",
    "                                                                            {\n",
    "                                                                            \"fileData\":  {\"fileUri\": gcsuri, \"mimeType\": mimeType},\n",
    "                                                                            \"videoMetadata\": {\n",
    "                                                                                \"endOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": endOffset\n",
    "                                                                                },\n",
    "                                                                                \"startOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": startOffset\n",
    "                                                                                }\n",
    "                                                                            }\n",
    "\n",
    "                                                                            },\n",
    "                                                                            {\n",
    "                                                                            \"text\": prompt_text +\"\\n\"+ f\n",
    "                                                                            } \n",
    "\n",
    "                                                                        ],\n",
    "                                                                        \"role\": \"user\"\n",
    "                                                                        }\n",
    "                                                          , \n",
    "                                                          \"generation_config\": \n",
    "                                                               {\"max_output_tokens\": max_output_tokens, \n",
    "                                                                \"temperature\":temperature, \n",
    "                                                                 \"top_k\": top_k, \n",
    "                                                                 \"top_p\": top_p\n",
    "                                                                }\n",
    "                                                          , \n",
    "                                                          \"safety_settings\": \n",
    "                                                           [\n",
    "                                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                             \"threshold\": \"BLOCK_NONE\"\n",
    "                                                             }\n",
    "                                                           , \n",
    "                                                           {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           },\n",
    "                                                           {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           }, \n",
    "                                                           {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                            \"threshold\": \"BLOCK_NONE\"\n",
    "                                                            }\n",
    "\n",
    "\n",
    "                                                           ]\n",
    "                                                     }\n",
    "                                              }\n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "                                rf.writelines(request_list)\n",
    "                                rf.flush()\n",
    "\n",
    "                                if index==(max_index-1):\n",
    "                                        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                        rf.close() \n",
    "                                        versions.append(version)                               \n",
    "                                        index=0\n",
    "                                        version +=1\n",
    "                                        request_list=[]\n",
    "                                        rf=None\n",
    "\n",
    "                                else:                               \n",
    "                                        index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "@functions_framework.http\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "#     request_json = request.get_json(silent=True)\n",
    "#     request_args = request.args\n",
    "\n",
    "#     dest_bucket_name =request_args['destination_bucket']\n",
    "#     source_bucket_name =request_args['source_bucket']\n",
    "#     source_folder_name=request_args['source_folder']\n",
    "#     request_file_prefix =request_args['request_file_prefix']\n",
    "#     request_file_folder =request_args['request_file_folder']\n",
    "#     prompt_text= request_args['prompt_text']\n",
    "#     media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    " \n",
    "#     request_content= request_args['request_content']\n",
    "\n",
    "#     if request_args and 'temperature' in request_args:\n",
    "#         temperature= request_args['temperature']\n",
    "#     else:\n",
    "#       temperature=0.5\n",
    "\n",
    "#     if request_args and 'max_output_tokens' in request_args:\n",
    "#        max_output_tokens= request_args['max_output_tokens'] \n",
    "#     else:\n",
    "#          max_output_tokens=2048\n",
    "\n",
    "#     if request_args and 'top_p' in request_args:\n",
    "#         top_p= request_args['top_p']\n",
    "#     else:\n",
    "#          top_p=0.5\n",
    "\n",
    "#     if request_args and 'top_k' in request_args:\n",
    "#         top_k= request_args['top_k']\n",
    "#     else:\n",
    "#          top_k=50\n",
    "\n",
    "#     if request_args and 'max_request_per_file' in request_args:\n",
    "#         max_request_per_file= request_args['max_request_per_file']\n",
    "#     else:\n",
    "#       max_request_per_file=30000\n",
    "    \n",
    "    dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "    source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "    request_file_prefix = \"video_request\"#request_args['request_file_prefix']\n",
    "    request_file_folder =  \"video_batch_request_fldr\"#request_args['request_file_folder']\n",
    "    prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "    media_types= ['video/mp4']#list(request_args['media_type'])\n",
    "    request_content= \"video\"#request_args['request_content']\n",
    "    source_folder_name=\"vlt_video_extract/OTHERS\"\n",
    "    temperature=0.5\n",
    "    max_output_tokens=2048\n",
    "    top_p=50\n",
    "    top_k=0.5\n",
    "    max_request_per_file=30000\n",
    "    video_metadata_file=\"vlt_video_metadata_fldr/vlt_video_metadata.json\"\n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file\n",
    "                                      )  \n",
    "    if  request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file,\n",
    "                                     video_metadata_file=video_metadata_file\n",
    "                                      ) \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "892bd4e3-490c-43d0-b7a1-38fecfea7735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 120}\n",
      "{'start': 120, 'end': 240}\n",
      "{'start': 240, 'end': 360}\n",
      "{'start': 360, 'end': 480}\n",
      "{'start': 480, 'end': 600}\n",
      "{'start': 600, 'end': 720}\n",
      "{'start': 720, 'end': 840}\n",
      "{'start': 840, 'end': 960}\n",
      "{'start': 960, 'end': 1080}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 1}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9514a955-db7d-41a4-b790-6b30585d23b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Copied 60MI23_33_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Copied 60MI23_33_A_HBB_fixed.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Copied 60MI24_1_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Copied 60MI24_6_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Copied MAAT2024_1_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Copied MAAT2024_21_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Copied MAAT2024_36_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Copied SYD-NINE_NNNTW24_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Copied SYD-NINE_NNNTW724_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Copied episode-18-brs-full-story.mp4 to vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Copied vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Copied vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Copied vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Copied vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Copied vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Copied vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4 to vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def move_files( dest_bucket_name: str= None, source_bucket_name: str= None,\n",
    "                          source_folder: str= None, destination_folder: str= None, mime_types: list[str]= None):\n",
    "    \n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs()  \n",
    " \n",
    "\n",
    "    for blob in blobs:\n",
    "        # Check if the file is an .mp4 file\n",
    "        if blob.content_type in [\"video/mp4\"]:\n",
    "        \n",
    "            # Create the new blob name for the destination\n",
    "            temp=blob.name.split(\"/\")\n",
    "            name=blob.name.split('/')[len(temp)-1]\n",
    "            if name.startswith(\"SYD-NINE\"):\n",
    "                dest_folder= f\"{destination_folder}/NINE_NIEWS\"\n",
    "            elif name.startswith(\"MAAT\"):\n",
    "                dest_folder= f\"{destination_folder}/MAAT\"\n",
    "            elif name.startswith(\"60MI\"):\n",
    "                dest_folder= f\"{destination_folder}/SIXTY_MINUTES\"\n",
    "            else:\n",
    "                 dest_folder= f\"{destination_folder}/OTHERS\"\n",
    "                     \n",
    "            new_blob_name = f\"{dest_folder}/{name}\"\n",
    "            print(new_blob_name)\n",
    "\n",
    "            # Copy the blob to the new location\n",
    "            bucket.copy_blob(blob, bucket, new_blob_name)\n",
    "            print(f\"Copied {blob.name} to {new_blob_name}\")\n",
    "\n",
    "# Example usage\n",
    "bucket_name = 'raw_nine_files'\n",
    "source_folder = ''  # Make sure to include the trailing slash\n",
    "destination_folder = 'vlt_video_extract'\n",
    "\n",
    "move_files(dest_bucket_name=bucket_name,source_bucket_name=bucket_name, source_folder=source_folder, destination_folder=destination_folder,mime_types=[])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "ff0ec9a2-e279-4f9e-bd31-64294f9d6a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "       \n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Truncate the table if exist\n",
    "        query = f\"TRUNCATE TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' truncated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error truncating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "        #add partitioning\n",
    "        # table.time_partitioning = bigquery.TimePartitioning(\n",
    "        #     type_=bigquery.TimePartitioningType.DAY,\n",
    "        #     field=\"request_id\",  # This field will be used for partitioning\n",
    "        # )\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5bc05941-78d7-45ae-bc59-77841f2b803f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''\n",
    " \n",
    "    try:\n",
    "        project_id=  request_args['project_id']\n",
    "        dataset_id=  request_args['dataset']\n",
    "        table= request_args['table']\n",
    "        region= request_args['region']\n",
    "        metadata_columns= str(request_args['metadata_columns']).split(',') \n",
    "        page_content_columns= str(request_args['page_content_columns']).split(',') \n",
    "        source_query_str= request_args['source_query_str']\n",
    "        separators= None if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "        chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "        chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap']))  \n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= 'chunked_data'\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [ col.strip() for col in  \"id,content,path,media_type,test_metadata\".split(\",\")]\n",
    "            page_content_columns= [col.strip() for col in \"content,test_metadata\".split(',') ]\n",
    "            source_query_str= \"\"\"\n",
    "            SELECT id,media_type,content,test_metadata, path\n",
    "            FROM `nine-quality-test.Nine_Quality_Test.test_long_artcles` ;\n",
    "            \"\"\"\n",
    "            separators= \"\\n\"\n",
    "            chunk_size= 500\n",
    "            chunk_overlap= 10\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    request_date=datetime.today().strftime('%Y_%m_%d')\n",
    "    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=30000 #maximum number of requests in a batch\n",
    " \n",
    "    max_index=2\n",
    "    record_count=0\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"id\"]\n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = request_date+'_'+str(version)\n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"id\": split.metadata[\"id\"], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\":  split.page_content,\n",
    "                                   \"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   \"media_type\": split.metadata[\"media_type\"],\n",
    "                                   \"path\": split.metadata[\"path\"],\n",
    "                                   \"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{dataset_id}_{request_id}\"\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]\n",
    "    \n",
    "    return {'status':'SUCCESS', 'record_count':record_count, 'count_of_tables':version+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "6bc54925-7ff7-47a9-863f-8bc6f142cd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'langchain_dataset' already exists.\n",
      "Table 'nine-quality-test.langchain_dataset.langchain_dataset_2024_10_08_0' created successfully.\n",
      "LoadJob<project=nine-quality-test, location=us-central1, id=7a692df9-5ea4-44dc-b0b0-07fdf4e90f5b>\n",
      "Table 'nine-quality-test.langchain_dataset.langchain_dataset_2024_10_08_1' created successfully.\n",
      "LoadJob<project=nine-quality-test, location=us-central1, id=62281b95-20fd-480c-946d-79189b1475c5>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'record_count': 4, 'count_of_tables': 2}"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_bq_content('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c60f3-8855-4a89-a0a3-ade9fb6ae849",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Truncate the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "\n",
    "if 1==1:\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "\n",
    " \n",
    "\n",
    "    # project_id=  request_args['project_id']\n",
    "    # dataset_id=  request_args['dataset']\n",
    "    # table= request_args['table']\n",
    "    # region= request_args['region']\n",
    "    # metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "    # page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "    # source_query_str= request_args['source_query_str']\n",
    "    # #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "    # chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "    # chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "    # max_prompt_count_limit=3000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "    # # except Exception as e: \n",
    "    if 1==1:\n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= 'chunked_data'\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [ col.strip() for col in  \"id, content, path, media_type, test_metadata\".split(\",\")]\n",
    "            page_content_columns= [col.strip() for col in \"content, test_metadata\".split(',') ]\n",
    "            source_query_str= \"\"\"\n",
    "            SELECT id,media_type,content,test_metadata, path\n",
    "            FROM `nine-quality-test.Nine_Quality_Test.test_long_artcles` ;\n",
    "            \"\"\"\n",
    "            #separators= \"\\n\"\n",
    "            chunk_size= 500\n",
    "            chunk_overlap= 10\n",
    "            max_prompt_count_limit=2\n",
    "     \n",
    "\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters '+e}\n",
    "\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    request_date=datetime.today().strftime('%Y_%m_%d')    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}_{request_date}\" \n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"id\"]\n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = request_date+'_'+str(version)\n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"id\": split.metadata[\"id\"], \n",
    "                                  ## \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\":  split.page_content,\n",
    "                                   \"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   \"media_type\": split.metadata[\"media_type\"],\n",
    "                                   \"path\": split.metadata[\"path\"],\n",
    "                                   \"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{prefix}_{version}\"\n",
    "               \n",
    "                print(table)\n",
    "                print('************')\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]\n",
    "                print(job.job_id)\n",
    "    \n",
    "    #return {'status':'SUCCESS', 'record_count':record_count, 'count_of_tables':version+1, 'table_name_prefix':prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55942516-176e-4dc7-8226-c95e6cf08201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_bq_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d64fcb8-d6b3-4c81-95b3-b6a8c156a343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5634\n",
      "4285\n",
      "3910\n",
      "2701\n",
      "2690\n",
      "2980\n",
      "2719\n",
      "2720\n",
      "2885\n",
      "2641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from moviepy.editor import VideoFileClip\n",
    "import datetime as date_time\n",
    "\n",
    "import math\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                                \n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", max_prompt_count_limit: int=30000, temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None):\n",
    "\n",
    "    \"\"\"create batch request file(s) of up to 30000 and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int : as number of generated request files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_prompt_count_limit\n",
    "\n",
    "    now=dt.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=dt.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "\n",
    "\n",
    "def generate_signed_url_v4(blob):\n",
    "    \"\"\"Generates a v4 signed URL for downloading a blob.\n",
    "\n",
    "    Note that this method requires a service account key file. You can not use\n",
    "    this if you are using Application Default Credentials from Google Compute\n",
    "    Engine or from the Google Cloud SDK.\n",
    "    \"\"\"\n",
    "\n",
    "    from google.auth.transport import requests\n",
    "    from google.auth import default, compute_engine\n",
    "    \n",
    "    credentials, _ = default()\n",
    "    \n",
    "    # then within your abstraction\n",
    "    auth_request = requests.Request()\n",
    "    credentials.refresh(auth_request)\n",
    "    \n",
    "    signing_credentials = compute_engine.IDTokenCredentials(\n",
    "        auth_request,\n",
    "        \"\",\n",
    "        service_account_email=credentials.service_account_email\n",
    "    )\n",
    "    url = blob.generate_signed_url(\n",
    "        version=\"v4\",\n",
    "        # This URL is valid for 15 minutes\n",
    "        expiration=date_time.timedelta(minutes=15),\n",
    "        # Allow GET requests using this URL.\n",
    "        method=\"GET\",\n",
    "         credentials=signing_credentials,\n",
    "    )\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_video_duration(signed_url):\n",
    "   \n",
    "  try:\n",
    "    video_uri=signed_url\n",
    "    \n",
    "    clip = VideoFileClip(video_uri)\n",
    "    duration = clip.duration\n",
    "    clip.close()  # Release resources\n",
    "    return duration\n",
    "  except Exception as e:\n",
    "    if \"moov atom not found\" in str(e):\n",
    "      print(\"Error: The video file seems to be corrupted or incomplete.\")\n",
    "     \n",
    "    else:\n",
    "      print(f\"Error reading video file: {e}\")\n",
    "    \n",
    "\n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", max_prompt_count_limit: int=30000, temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None):\n",
    "\n",
    "    \"\"\"create batch request file(s) of up to 30000 and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_prompt_count_limit\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         signed_url= generate_signed_url_v4(blob)\n",
    "                         video_duration = 2641# math.ceil(get_video_duration(signed_url))\n",
    "                         print(video_duration)\n",
    "                         for \n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": {\n",
    "                                            \"contents\": {\n",
    "                                                \"parts\": [\n",
    "                                                    {\n",
    "                                                        \"fileData\": {\n",
    "                                                            \"fileUri\": \"gs://raw_nine_files/60MI23_33_A_HBB.mp4\",\n",
    "                                                            \"mimeType\": \"video/mp4\"\n",
    "                                                        },\n",
    "                                                        \"videoMetadata\": {\n",
    "                                                            \"endOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 240\n",
    "                                                            },\n",
    "                                                            \"startOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 120\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                        \"text\": \"You are an assistant tasked with describing videos for retrieval.These descriptions will be embedded and used to retrieve th\n",
    "                                                        e raw video.\\n Chapterize the video content by grouping the video content into chapters with intervals of 120 seconds \n",
    "                                                        and providing a concise detail for each chapter that is well optimized for retrieval.\\nIf there is a \n",
    "                                                        famous person like politician, celebrity or athlete, indicate their name and describe what they \n",
    "                                                        are famous for.\\n Describe important scenes in the video concisely. If you are not sure about any info, \n",
    "                                                        please do not make it up. \\n Only consider video from 120 seconds to 240 seconds. \\nIf it is the last chapter, \n",
    "                                                        set the endOffset to 240 instead.\\n For result, follow JSON schema.<JSONSchema>{\\\"description\\\":\\\"A list of chapters\\\",\\\"items\\\":{\\\"properties\\\":{\\\"Content\\\":{\\\"type\\\":\\\"string\\\"},\\\"endOffset\\\":{\\\"type\\\":\\\"integer\\\"},\\\"startOffset\\\":{\\\"type\\\":\\\"integer\\\"}},\\\"required\\\":[\\\"startOffset\\\",\\\"endOffset\\\",\\\"Content\\\"],\\\"type\\\":\\\"object\\\"},\\\"type\\\":\\\"array\\\"}</JSONSchema>\"\n",
    "                                                    }\n",
    "                                                ],\n",
    "                                                \"role\": \"user\"\n",
    "                                            },\n",
    "                                            \"generation_config\": {\n",
    "                                                \"max_output_tokens\": 2048,\n",
    "                                                \"temperature\": 0.5,\n",
    "                                                \"top_k\": 40,\n",
    "                                                \"top_p\": 0.8\n",
    "                                            },\n",
    "                                            \"safety_settings\": [\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        }\n",
    "                                    }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request_json = request.get_json(silent=True)\n",
    "        request_args = request.args\n",
    "\n",
    "        dest_bucket_name =request_args['destination_bucket']\n",
    "        source_bucket_name =request_args['source_bucket']\n",
    "        source_folder_name=request_args['source_folder']\n",
    "        request_file_prefix =request_args['request_file_prefix']\n",
    "        request_file_folder =request_args['request_file_folder']\n",
    "        prompt_text= request_args['prompt_text']\n",
    "        max_prompt_count_limit=int(str(request_args['max_prompt_count_limit']))  \n",
    "        media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "\n",
    "        request_content= request_args['request_content']\n",
    "\n",
    "        if request_args and 'temperature' in request_args:\n",
    "            temperature= request_args['temperature']\n",
    "        else:\n",
    "          temperature=0.5\n",
    "\n",
    "        if request_args and 'max_output_tokens' in request_args:\n",
    "           max_output_tokens= request_args['max_output_tokens'] \n",
    "        else:\n",
    "             max_output_tokens=2048\n",
    "\n",
    "        if request_args and 'top_p' in request_args:\n",
    "            top_p= request_args['top_p']\n",
    "        else:\n",
    "             top_p=0.5\n",
    "\n",
    "        if request_args and 'top_k' in request_args:\n",
    "            top_k= request_args['top_k']\n",
    "        else:\n",
    "             top_k=50\n",
    "    except:\n",
    "            dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "            source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "            request_file_prefix = \"image_request\"#request_args['request_file_prefix']\n",
    "            request_file_folder =  \"image_batch_request_fldr\"#request_args['request_file_folder']\n",
    "            prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "            media_types= ['image/jpeg','image/png']#list(request_args['media_type'])\n",
    "            request_content= \"image\"#request_args['request_content']\n",
    "\n",
    "            temperature=0.5\n",
    "            max_output_tokens=2048\n",
    "            top_p=50\n",
    "            top_k=0.5\n",
    "            request_content=\"video\"\n",
    "            media_types= ['video/mp4']\n",
    "            max_prompt_count_limit=2\n",
    "            source_folder_name='vlt_video_extract'\n",
    "            prompt_text=\"You are an assistant tasked with describing videos for retrieval. \\\n",
    "                                Describe this video well optimized for retrieval. Mention anything important the people talk about.\\\n",
    "                                If there is a text or logo mention that in details. If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\n",
    "                                If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"\n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,max_prompt_count_limit=max_prompt_count_limit,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k\n",
    "                                      )  \n",
    "    elif request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,max_prompt_count_limit=max_prompt_count_limit,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k\n",
    "                                      )  \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n",
    "\n",
    "\n",
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba5aeac-eb33-42c7-9d3d-04476356f193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import gcsfs\n",
    "from pymediainfo import MediaInfo\n",
    "\n",
    "def get_video_metadata_from_stream(gcsuri):\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "        # Use pymediainfo to extract metadata directly from the stream\n",
    "        media_info = MediaInfo.parse(video_file)\n",
    "        for track in media_info.tracks:\n",
    "            if track.track_type == 'Video':\n",
    "                return track.duration / 1000  # Convert ms to seconds\n",
    "\n",
    "# bucket_name = 'raw_nine_files'\n",
    "# source_blob_name = 'vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4'\n",
    "\n",
    "# # Get video metadata (duration) from stream\n",
    "# duration = get_video_metadata_from_stream(bucket_name, source_blob_name)\n",
    "# print(f\"Video duration: {duration} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d80235-e734-407e-84e9-3694dcf705a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_video_metadata( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          metadata_file_prefix: str= None, metadata_file_folder: str= None, mime_types: list[str]= None \n",
    "                           ):\n",
    "\n",
    "    \"\"\" generates video length\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    \n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         video_duration=get_video_metadata_from_stream(gcsuri)\n",
    "                         print(video_duration)\n",
    " \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf53e216-3eff-4dd5-87b1-2a7d6e5706cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5634.0\n"
     ]
    }
   ],
   "source": [
    "video_duration=get_video_metadata_from_stream(\"gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\")\n",
    "print(video_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62c53298-0bfb-47ab-800b-f0d7cf69b7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005.0\n"
     ]
    }
   ],
   "source": [
    "video_duration=get_video_metadata_from_stream(\"gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4\")\n",
    "print(video_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e00b3d2-521f-495e-a1d3-e6bc5bf40625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5634.0\n",
      "4285.0\n",
      "3910.0\n",
      "2700.04\n",
      "2690.0\n",
      "2979.01\n",
      "1005.0\n",
      "2719.0\n",
      "2719.04\n",
      "2885.0\n",
      "2641.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_video_metadata(   source_bucket_name=\"raw_nine_files\", source_folder_name=\"vlt_video_extract\",\n",
    "                           mime_types=['video/mp4']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90c12451-fb58-4edd-9d53-8dde1eaa087c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320504668865953792\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "import datetime\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "\n",
    " \n",
    "system_instruction = \"\"\"\n",
    "You are an expert researcher. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at these research papers, and answer the following questions.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(days=2),\n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)\n",
    "# Example response:\n",
    "# 1234567890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df716674-c8cd-4880-91f2-df9ff80f4963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cf3d063-c54f-42ed-ace5-ada7a184ec39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/494586852359/locations/us-central1/cachedContents/1320504668865953792',\n",
       " 'model': 'projects/nine-quality-test/locations/us-central1/publishers/google/models/gemini-1.5-pro-002',\n",
       " 'createTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'updateTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'expireTime': '2024-10-12T00:05:18.129147Z',\n",
       " 'displayName': 'example-cache'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_content.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40be2737-2aaa-4a80-9b25-cd49ee49a3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'createTime': 'llm_response.body.createTime',\n",
       "  'expireTime': 'llm_response.body.expireTime',\n",
       "  'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4',\n",
       "  'model': 'llm_response.body.model',\n",
       "  'name': 'llm_response.body.name',\n",
       "  'updateTime': 'llm_response.body.updateTime',\n",
       "  'usageMetadata': 'llm_response.body.usageMetadata'},\n",
       " {'createTime': 'llm_response.body.createTime',\n",
       "  'expireTime': 'llm_response.body.expireTime',\n",
       "  'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4',\n",
       "  'model': 'llm_response.body.model',\n",
       "  'name': 'llm_response.body.name',\n",
       "  'updateTime': 'llm_response.body.updateTime',\n",
       "  'usageMetadata': 'llm_response.body.usageMetadata'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= data['items']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1db7b32-ad04-468a-809b-96daefab45b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l=list(filter(lambda element: element['gcs_uri'] == 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b0cc048-04d5-499e-b4e8-966969bc39b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][\"gcs_uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0db5932-f361-467b-b6b2-9ef846dad71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n",
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-10-12T23:45:42.158262Z'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import the Google Cloud client library and JSON library\n",
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('raw_nine_files')\n",
    "blob = bucket.blob('test111.json')\n",
    "\n",
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for x in data['items']:\n",
    "    print(x)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "data[\"cache_expiry_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76829048-51c8-4427-8ca1-475e0553df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for item in enumerate( data.items):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae3296-e97c-46ae-8ad2-2647a3c92c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": {\n",
    "                                            \"contents\": {\n",
    "                                                \"parts\": [\n",
    "                                                    {\n",
    "                                                        \"fileData\": {\n",
    "                                                            \"fileUri\": \"gs://raw_nine_files/60MI23_33_A_HBB.mp4\",\n",
    "                                                            \"mimeType\": \"video/mp4\"\n",
    "                                                        },\n",
    "                                                        \"videoMetadata\": {\n",
    "                                                            \"endOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 240\n",
    "                                                            },\n",
    "                                                            \"startOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 120\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                        \"text\": \"You are an assistant tasked with describing videos for retrieval.These descriptions will be embedded and used to retrieve the raw video.\\n Chapterize the video content by grouping the video content into chapters with intervals of 120 seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\nIf there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\n Describe important scenes in the video concisely. If you are not sure about any info, please do not make it up. \\n Only consider video from 120 seconds to 240 seconds. \\nIf it is the last chapter, set the endOffset to 240 instead.\\n For result, follow JSON schema.<JSONSchema>{\\\"description\\\":\\\"A list of chapters\\\",\\\"items\\\":{\\\"properties\\\":{\\\"Content\\\":{\\\"type\\\":\\\"string\\\"},\\\"endOffset\\\":{\\\"type\\\":\\\"integer\\\"},\\\"startOffset\\\":{\\\"type\\\":\\\"integer\\\"}},\\\"required\\\":[\\\"startOffset\\\",\\\"endOffset\\\",\\\"Content\\\"],\\\"type\\\":\\\"object\\\"},\\\"type\\\":\\\"array\\\"}</JSONSchema>\"\n",
    "                                                    }\n",
    "                                                ],\n",
    "                                                \"role\": \"user\"\n",
    "                                            },\n",
    "                                            \"generation_config\": {\n",
    "                                                \"max_output_tokens\": 2048,\n",
    "                                                \"temperature\": 0.5,\n",
    "                                                \"top_k\": 40,\n",
    "                                                \"top_p\": 0.8\n",
    "                                            },\n",
    "                                            \"safety_settings\": [\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        }\n",
    "                                    }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcba9a1d-5832-4e71-83bc-7b13d46c491a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "# cache_id = \"your-cache-id\"\n",
    " \n",
    "# cached_content = caching.CachedContent(cached_content_name=cache_id)\n",
    "# cached_content.delete()\n",
    "\n",
    "for x in caching.CachedContent.list():\n",
    "     x=x.name.split(\"/\")[-1]\n",
    "     print(x)\n",
    "     cached_content = caching.CachedContent(cached_content_name=x)\n",
    "     #cached_content.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46dedaab-b53c-4f9b-9e17-8037bf254d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VideoRequest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_video_duration\u001b[39m(request: \u001b[43mVideoRequest\u001b[49m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" generates video length\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m   \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m          \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     fs \u001b[38;5;241m=\u001b[39m gcsfs\u001b[38;5;241m.\u001b[39mGCSFileSystem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VideoRequest' is not defined"
     ]
    }
   ],
   "source": [
    "def get_video_duration(request: VideoRequest):\n",
    "    \"\"\" generates video length\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7b74fb8-6815-4022-bdfc-bd13da79aaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import gcsfs\n",
    "from pymediainfo import MediaInfo\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the input data model\n",
    "class VideoRequest(BaseModel):\n",
    "    url: str  # Input: video URL\n",
    "\n",
    "# Define the output data model\n",
    "class VideoResponse(BaseModel):\n",
    "    duration: float  # Output: Duration in seconds\n",
    "\n",
    "@app.post(\"/get-video-duration\", response_model=VideoResponse)\n",
    "async def get_video_duration(request: VideoRequest):\n",
    "    \"\"\"\n",
    "    get video duration of a given gcs url\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break\n",
    " \n",
    "\n",
    "    return VideoResponse(duration=duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c1998d85-7777-4975-8454-6ebe00317d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                      \n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None, video_metadata_file: str=\"\"):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for video and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "    \n",
    "\n",
    "    # Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    blob = bucket.blob(video_metadata_file)\n",
    "\n",
    "    # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "    data = json.loads(blob.download_as_string(client=None))\n",
    "    video_metadata=data['items']\n",
    "    \n",
    "\n",
    "    segments_to_process=120 #segments duration\n",
    "    intervals=120#intervals\n",
    "    video_start =0 #where from video to start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for blob in blobs:                         \n",
    "        if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         metadata=list(filter(lambda element: element['gcs_uri'] ==gcsuri, video_metadata))[0]\n",
    "                         video_duration=metadata[\"videoOriginalDurationSecond\"]\n",
    "                         cache_id=metadata[\"name\"]\n",
    "                         prev=video_start\n",
    "                         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                                offset={'start':prev, 'end':val}\n",
    "                                \n",
    "                                startOffset=offset['start']\n",
    "                                endOffset=offset['end']\n",
    "                                if endOffset>=video_duration:\n",
    "                                     endOffset=video_duration\n",
    "                                print(offset)\n",
    "                                prev=val \n",
    "                                segment_prompt= f\"Only consider video from {startOffset} seconds to {endOffset} seconds. Ignore analyzing the rest of video.\\n\" \n",
    "                                if index==0:\n",
    "                                    request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                                    rf= open(request_file.name, \"a\") \n",
    "                                \n",
    "                                 \n",
    "                                request_list=[\n",
    "                                       json.dumps(\n",
    "                                              {\n",
    "                                                \"request\": \n",
    "                                                     {\n",
    "                                                     \"cached_content\": cache_id,\n",
    "                                                      \"contents\":  {\n",
    "                                                                        \"parts\": [\n",
    "                                                                            {\n",
    "                                                                            \"fileData\":  {\"fileUri\": gcsuri, \"mimeType\": mimeType},\n",
    "                                                                            \"videoMetadata\": {\n",
    "                                                                                \"endOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": endOffset\n",
    "                                                                                },\n",
    "                                                                                \"startOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": startOffset\n",
    "                                                                                }\n",
    "                                                                            }\n",
    "\n",
    "                                                                            },\n",
    "                                                                            {\n",
    "                                                                            \"text\": prompt_text +\"\\n\"+ segment_prompt\n",
    "                                                                            } \n",
    "\n",
    "                                                                        ],\n",
    "                                                                        \"role\": \"user\"\n",
    "                                                                        }\n",
    "                                                          , \n",
    "                                                          \"generation_config\": \n",
    "                                                               {\"max_output_tokens\": max_output_tokens, \n",
    "                                                                \"temperature\":temperature, \n",
    "                                                                 \"top_k\": top_k, \n",
    "                                                                 \"top_p\": top_p\n",
    "                                                                }\n",
    "                                                          , \n",
    "                                                          \"safety_settings\": \n",
    "                                                           [\n",
    "                                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                             \"threshold\": \"BLOCK_NONE\"\n",
    "                                                             }\n",
    "                                                           , \n",
    "                                                           {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           },\n",
    "                                                           {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           }, \n",
    "                                                           {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                            \"threshold\": \"BLOCK_NONE\"\n",
    "                                                            }\n",
    "\n",
    "\n",
    "                                                           ]\n",
    "                                                     }\n",
    "                                              }\n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "                                rf.writelines(request_list)\n",
    "                                rf.flush()\n",
    "\n",
    "                                if index==(max_index-1):\n",
    "                                        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                        rf.close() \n",
    "                                        versions.append(version)                               \n",
    "                                        index=0\n",
    "                                        version +=1\n",
    "                                        request_list=[]\n",
    "                                        rf=None\n",
    "\n",
    "                                else:                               \n",
    "                                        index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "@functions_framework.http\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "#     request_json = request.get_json(silent=True)\n",
    "#     request_args = request.args\n",
    "\n",
    "#     dest_bucket_name =request_args['destination_bucket']\n",
    "#     source_bucket_name =request_args['source_bucket']\n",
    "#     source_folder_name=request_args['source_folder']\n",
    "#     request_file_prefix =request_args['request_file_prefix']\n",
    "#     request_file_folder =request_args['request_file_folder']\n",
    "#     prompt_text= request_args['prompt_text']\n",
    "#     media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    " \n",
    "#     request_content= request_args['request_content']\n",
    "\n",
    "#     if request_args and 'temperature' in request_args:\n",
    "#         temperature= request_args['temperature']\n",
    "#     else:\n",
    "#       temperature=0.5\n",
    "\n",
    "#     if request_args and 'max_output_tokens' in request_args:\n",
    "#        max_output_tokens= request_args['max_output_tokens'] \n",
    "#     else:\n",
    "#          max_output_tokens=2048\n",
    "\n",
    "#     if request_args and 'top_p' in request_args:\n",
    "#         top_p= request_args['top_p']\n",
    "#     else:\n",
    "#          top_p=0.5\n",
    "\n",
    "#     if request_args and 'top_k' in request_args:\n",
    "#         top_k= request_args['top_k']\n",
    "#     else:\n",
    "#          top_k=50\n",
    "\n",
    "#     if request_args and 'max_request_per_file' in request_args:\n",
    "#         max_request_per_file= request_args['max_request_per_file']\n",
    "#     else:\n",
    "#       max_request_per_file=30000\n",
    "    \n",
    "    dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "    source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "    request_file_prefix = \"video_request\"#request_args['request_file_prefix']\n",
    "    request_file_folder =  \"video_batch_request_fldr\"#request_args['request_file_folder']\n",
    "    prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "    media_types= ['video/mp4']#list(request_args['media_type'])\n",
    "    request_content= \"video\"#request_args['request_content']\n",
    "    source_folder_name=\"vlt_video_extract/OTHERS\"\n",
    "    temperature=0.5\n",
    "    max_output_tokens=2048\n",
    "    top_p=50\n",
    "    top_k=0.5\n",
    "    max_request_per_file=30000\n",
    "    video_metadata_file=\"vlt_video_metadata_fldr/vlt_video_metadata.json\"\n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file\n",
    "                                      )  \n",
    "    if  request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file,\n",
    "                                     video_metadata_file=video_metadata_file\n",
    "                                      ) \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "28e800a6-f02c-49ef-a0fd-9c91330edbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 120}\n",
      "{'start': 120, 'end': 240}\n",
      "{'start': 240, 'end': 360}\n",
      "{'start': 360, 'end': 480}\n",
      "{'start': 480, 'end': 600}\n",
      "{'start': 600, 'end': 720}\n",
      "{'start': 720, 'end': 840}\n",
      "{'start': 840, 'end': 960}\n",
      "{'start': 960, 'end': 1080}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 1}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db570928-ad71-4027-8cc3-13db077eef24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None, video_metadata_file: str=\"\"):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for video and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str source_bucket_name: name of the source  gcs bucket to read files from\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str source_folder_name: name of the source folder name to read files from\n",
    "            str request_file_prefix: prefix of the request file name\n",
    "            list mime_types: list of accepted mime_types\n",
    "            str prompt_text: prompt for Gimini\n",
    "            float temperature: Gimini temprature\n",
    "            float top_p: Gimini top_p\n",
    "            float top_k: Gimini top_k\n",
    "            int max_request_per_file: max number of requests per batch\n",
    "            int max_output_tokens: Gimini max_output_tokens          \n",
    "            str video_metadata_file: name of video metadata\n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]    \n",
    "\n",
    "    # Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    blob = bucket.blob(video_metadata_file)\n",
    "\n",
    "    # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "    data = json.loads(blob.download_as_string(client=None))\n",
    "    video_metadata=data['items']\n",
    "    \n",
    "\n",
    "    segments_to_process=120 #segments duration\n",
    "    intervals=120#intervals\n",
    "    video_start =0 #where from video to start\n",
    "\n",
    "    for blob in blobs:                         \n",
    "        if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         metadata=list(filter(lambda element: element['gcs_uri'] ==gcsuri, video_metadata))[0]\n",
    "                         video_duration=metadata[\"videoOriginalDurationSecond\"]\n",
    "                         cache_id=metadata[\"name\"]\n",
    "\n",
    "                         prev=video_start\n",
    "                         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                                offset={'start':prev, 'end':val}\n",
    "                                \n",
    "                                startOffset=offset['start']\n",
    "                                endOffset=offset['end']\n",
    "                                if endOffset>=video_duration:\n",
    "                                     endOffset=video_duration\n",
    "                                print(offset)\n",
    "                                prev=val \n",
    "                                segment_prompt= \"Only consider video from\" + str(startOffset)+\" seconds to \"+ str(endOffset)+\" seconds. Ignore analyzing the rest of video.\\n\" \n",
    "                                if index==0:\n",
    "                                    request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                                    rf= open(request_file.name, \"a\") \n",
    "                                \n",
    "                                 \n",
    "                                request_list=[\n",
    "                                       json.dumps(\n",
    "                                              {\n",
    "                                                \"request\": \n",
    "                                                     {\n",
    "                                                      \"cached_content\": cache_id,\n",
    "                                                      \"contents\":  {\n",
    "                                                                        \"parts\": [\n",
    "                                                                            {\n",
    "                                                                            \"fileData\":  {\"fileUri\": gcsuri, \"mimeType\": mimeType},\n",
    "                                                                            \"videoMetadata\": {\n",
    "                                                                                \"endOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": endOffset\n",
    "                                                                                },\n",
    "                                                                                \"startOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": startOffset\n",
    "                                                                                }\n",
    "                                                                            }\n",
    "\n",
    "                                                                            },\n",
    "                                                                            {\n",
    "                                                                            \"text\": prompt_text +\"\\n\"+ segment_prompt\n",
    "                                                                            } \n",
    "\n",
    "                                                                        ],\n",
    "                                                                        \"role\": \"user\"\n",
    "                                                                        }\n",
    "                                                          , \n",
    "                                                          \"generation_config\": \n",
    "                                                               {\"max_output_tokens\": max_output_tokens, \n",
    "                                                                \"temperature\":temperature, \n",
    "                                                                 \"top_k\": top_k, \n",
    "                                                                 \"top_p\": top_p\n",
    "                                                                }\n",
    "                                                          , \n",
    "                                                          \"safety_settings\": \n",
    "                                                           [\n",
    "                                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                             \"threshold\": \"BLOCK_NONE\"\n",
    "                                                             }\n",
    "                                                           , \n",
    "                                                           {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           },\n",
    "                                                           {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           }, \n",
    "                                                           {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                            \"threshold\": \"BLOCK_NONE\"\n",
    "                                                            }\n",
    "\n",
    "\n",
    "                                                           ]\n",
    "                                                     }\n",
    "                                              }\n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "                                rf.writelines(request_list)\n",
    "                                rf.flush()\n",
    "\n",
    "                                if index==(max_index-1):\n",
    "                                        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                        rf.close() \n",
    "                                        versions.append(version)                               \n",
    "                                        index=0\n",
    "                                        version +=1\n",
    "                                        request_list=[]\n",
    "                                        rf=None\n",
    "\n",
    "                                else:                               \n",
    "                                        index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for videos and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str source_bucket_name: name of the source  gcs bucket to read files from\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str source_folder_name: name of the source folder name to read files from\n",
    "            str request_file_prefix: prefix of the request file name\n",
    "            list mime_types: list of accepted mime_types\n",
    "            str prompt_text: prompt for Gimini\n",
    "            float temperature: Gimini temprature\n",
    "            float top_p: Gimini top_p\n",
    "            float top_k: Gimini top_k\n",
    "            int max_request_per_file: max number of requests per batch\n",
    "            int max_output_tokens: Gimini max_output_tokens          \n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"           \n",
    "         \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "@functions_framework.http\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request_json = request.get_json(silent=True)\n",
    "        request_args = request.args\n",
    "\n",
    "        dest_bucket_name =request_args['destination_bucket']\n",
    "        source_bucket_name =request_args['source_bucket']\n",
    "        source_folder_name=request_args['source_folder']\n",
    "        request_file_prefix =request_args['request_file_prefix']\n",
    "        request_file_folder =request_args['request_file_folder']\n",
    "        prompt_text= request_args['prompt_text']\n",
    "        media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "\n",
    "        request_content= request_args['request_content']\n",
    "\n",
    "        if request_args and 'video_metadata_file' in request_args:\n",
    "            video_metadata_file= request_args['video_metadata_file']\n",
    "        else:\n",
    "          video_metadata_file=\"\"\n",
    "\n",
    "        if request_args and 'temperature' in request_args:\n",
    "            temperature= request_args['temperature']\n",
    "        else:\n",
    "          temperature=1\n",
    "\n",
    "        if request_args and 'max_output_tokens' in request_args:\n",
    "           max_output_tokens= request_args['max_output_tokens'] \n",
    "        else:\n",
    "             max_output_tokens=8192\n",
    "\n",
    "        if request_args and 'top_p' in request_args:\n",
    "            top_p= request_args['top_p']\n",
    "        else:\n",
    "             top_p=0.95\n",
    "\n",
    "        if request_args and 'top_k' in request_args:\n",
    "            top_k= request_args['top_k']\n",
    "        else:\n",
    "             top_k=40\n",
    "\n",
    "        if request_args and 'max_request_per_file' in request_args:\n",
    "            max_request_per_file= request_args['max_request_per_file']\n",
    "        else:\n",
    "          max_request_per_file=30000\n",
    "    except:\n",
    "            dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "            source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "            request_file_prefix = \"video_request\"#request_args['request_file_prefix']\n",
    "            request_file_folder =  \"video_batch_request_fldr\"#request_args['request_file_folder']\n",
    "            prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "            media_types= ['video/mp4']#list(request_args['media_type'])\n",
    "            request_content= \"video\"#request_args['request_content']\n",
    "            source_folder_name=\"vlt_video_extract/OTHERS\"\n",
    "            temperature=0.5\n",
    "            max_output_tokens=2048\n",
    "            top_p=50\n",
    "            top_k=0.5\n",
    "            max_request_per_file=30000\n",
    "            video_metadata_file=\"vlt_video_metadata_fldr/vlt_video_metadata.json\"\n",
    "\n",
    " \n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file\n",
    "                                      )  \n",
    "    if  request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file,\n",
    "                                     video_metadata_file=video_metadata_file\n",
    "                                      ) \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b17d6b7-db52-48ef-b9b3-504fdeb11bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 120}\n",
      "{'start': 120, 'end': 240}\n",
      "{'start': 240, 'end': 360}\n",
      "{'start': 360, 'end': 480}\n",
      "{'start': 480, 'end': 600}\n",
      "{'start': 600, 'end': 720}\n",
      "{'start': 720, 'end': 840}\n",
      "{'start': 840, 'end': 960}\n",
      "{'start': 960, 'end': 1080}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 1}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6f51d-1fc9-4b4a-ba40-1f6b196b8927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7323099284709048320\n"
     ]
    }
   ],
   "source": [
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    \n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ae371a-c0db-4f4c-86f6-87c86ff0eb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first paper, \"Gemini: A Family of Highly Capable Multimodal Models\", introduces Gemini 1.0, Google's family of multimodal models. It details their architecture, training, capabilities, and responsible deployment.  Gemini 1.0 models come in different sizes (Ultra, Pro, Nano) for various applications.  The paper highlights Gemini Ultra's state-of-the-art performance on many benchmarks, exceeding human expert performance on MMLU and achieving strong results in multimodal reasoning (like MMMU) and coding tasks.  The paper emphasizes responsible AI development, outlining impact assessments, safety mitigations, and evaluations to address potential harms.\n",
      "\n",
      "The second paper, \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\", introduces Gemini 1.5 Pro, an evolution focusing on significantly expanded context length and efficiency.  It details the model's architecture as a mixture-of-experts model and the infrastructure advancements enabling its capabilities.  The paper emphasizes Gemini 1.5 Pro's near-perfect recall on long-context retrieval across modalities (text, video, audio) up to millions of tokens. It highlights improvements over Gemini 1.0 on core benchmarks while requiring less compute, and showcases surprising new capabilities like in-context language learning for low-resource languages with limited data.  Responsible development remains a focus, outlining updated impact assessment, model mitigation, and safety evaluations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "# cache_id = \"your-cache-id\"\n",
    "\n",
    " \n",
    "cached_content = caching.CachedContent(cached_content_name=\"7323099284709048320\")\n",
    "\n",
    "model = GenerativeModel.from_cached_content(cached_content=cached_content)\n",
    "\n",
    "response = model.generate_content(\"What are the papers about?\")\n",
    "\n",
    "print(response.text)\n",
    "# Example response:\n",
    "# The provided text is about a new family of multimodal models called Gemini, developed by Google.\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc92a70a-f78b-40e1-84d0-82165a547135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GenerativeModel' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mdir\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GenerativeModel' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802f1da-660c-4c80-84b8-8e6eec3961fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " url: ${MM_LLM_ENDPOINT}\n",
    "                                auth:\n",
    "                                    type: OAuth2\n",
    "                                body:\n",
    "                                    \"instances\": [\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         mimeType:  \"${item.mime_type}\",\n",
    "                                                        \"gcsUri\": \"${item.gcs_uri}\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "                            result: llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "decc7737-767c-446c-8513-3fe5bf1fbb92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MM_MODEL= \"multimodalembedding@001\"\n",
    "MM_METHOD= \"predict\"\n",
    "PROJECT='nine-quality-test'\n",
    "LOCATION='us-central1'\n",
    "MM_LLM_ENDPOINT=\"https://\" + 'us-central1' + \"-aiplatform.googleapis.com\" + \"/v1/projects/\" + PROJECT + \"/locations/\" + 'us-central1' + \"/publishers/google/models/\" + MM_MODEL+\":\"+MM_METHOD\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65083e85-cf11-4364-9dab-e0056d461e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'code': 401, 'message': 'Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.', 'status': 'UNAUTHENTICATED', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'CREDENTIALS_MISSING', 'domain': 'googleapis.com', 'metadata': {'method': 'google.cloud.aiplatform.v1.PredictionService.Predict', 'service': 'aiplatform.googleapis.com'}}]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The API endpoint\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Data to be sent\n",
    "data = {\n",
    "     \"instances\": [\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         \"mimeType\":  \"image/png\",\n",
    "                                                        \"gcsUri\": \"gs://raw_nine_files/2023/1/a/816f62dz.png\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "}\n",
    "\n",
    "# A POST request to the API\n",
    "response = requests.post(MM_LLM_ENDPOINT, json=data)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08618712-9ba9-42c1-86e0-1dc6b9e3240c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "api_regional_endpoint= \"us-central1-aiplatform.googleapis.com\"\n",
    "client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "            client_options=client_options\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7f90ed3-d096-4851-b36e-0ca1197a46c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "instances=[\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         \"mimeType\":  \"image/png\",\n",
    "                                                        \"gcsUri\": \"gs://raw_nine_files/2023/1/a/816f62dz.png\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "\n",
    "endpoint = (\n",
    "           f\"projects/{PROJECT}/locations/{LOCATION}\"\n",
    "           \"/publishers/google/models/multimodalembedding@001\"\n",
    "        )\n",
    "response =client.predict(endpoint=endpoint, instances=instances)\n",
    "response.predictions[0].get(\"imageEmbedding\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07e10cda-9199-49c7-94a5-dd5a0a7f0815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_method= \"generateContent\"\n",
    "content_model= \"gemini-1.5-pro-002\"\n",
    "endpoint=(\"v1beta1/projects/\"  + PROJECT+\"/locations/\" + LOCATION + \"/publishers/google/models/\"+ content_model + \":\" + content_method)\n",
    " \n",
    "instances={\n",
    "                                       \n",
    "                                        \"contents\": [\n",
    "                                            {\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"parts\": [\n",
    "                                                {\n",
    "                                              \n",
    "                                                \n",
    "                                                \"fileData\": {\n",
    "                                                    \"mimeType\": \"video/mp4\",\n",
    "                                                    \"fileUri\": \"gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\"\n",
    "                                                },\n",
    "\n",
    "                                                \"videoMetadata\": {\n",
    "                                                    \"startOffset\": {\n",
    "                                                    \"seconds\": 300,\n",
    "                                                    \"nanos\": 0\n",
    "                                                    },\n",
    "                                                    \"endOffset\": {\n",
    "                                                    \"seconds\": 520,\n",
    "                                                    \"nanos\": 0\n",
    "                                                    }\n",
    "                                                }\n",
    "                                                },\n",
    "                                                 { \"text\": \"describe what is happening in this video in the given time frame from offset 300 second to offset 520 second in details and what people are talking about.\"}\n",
    "                                            ]\n",
    "                                            }\n",
    "                                        ] ,\n",
    "                                        \n",
    "                                        \"safetySettings\": [\n",
    "                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "                                        ],\n",
    "                                        \"generationConfig\": {\n",
    "                                            \"temperature\": 1,\n",
    "                                            \"topP\": 0.5,\n",
    "                                            \"topK\": 40,\n",
    "                                           # \"candidateCount\": integer,\n",
    "                                            \"maxOutputTokens\": 8192,\n",
    "                                            # \"presencePenalty\": float,\n",
    "                                            # \"frequencyPenalty\": float,\n",
    "                                            # \"stopSequences\": [\n",
    "                                            # string\n",
    "                                            # ],\n",
    "                                            # \"responseMimeType\": string,\n",
    "                                            # \"responseSchema\": schema,\n",
    "                                            # \"seed\": integer,\n",
    "                                            # \"responseLogprobs\": boolean,\n",
    "                                            # \"logprobs\": integer,\n",
    "                                            # \"audioTimestamp\": boolean\n",
    "                                        }#,\n",
    "\n",
    "                                       # ,\"cachedContent\": \"projects/494586852359/locations/us-central1/cachedContents/4906355134671355904\"  \n",
    "                                       \n",
    "} \n",
    "\n",
    "from google.cloud import aiplatform\n",
    "api_regional_endpoint= \"us-central1-aiplatform.googleapis.com\"\n",
    "client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "            client_options=client_options\n",
    "        )\n",
    "\n",
    "import google.cloud.aiplatform.v1beta1  as endpoint       \n",
    "#response =client.generate_content(instances)\n",
    "#response =client.predict(endpoint=endpoint, instances=[instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9890de0e-b31c-4020-90e9-ce20b6429589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClassificationPredictionResult',\n",
       " 'ImageObjectDetectionPredictionResult',\n",
       " 'ImageSegmentationPredictionResult',\n",
       " 'TabularClassificationPredictionResult',\n",
       " 'TabularRegressionPredictionResult',\n",
       " 'TextExtractionPredictionResult',\n",
       " 'TextSentimentPredictionResult',\n",
       " 'TimeSeriesForecastingPredictionResult',\n",
       " 'VideoActionRecognitionPredictionResult',\n",
       " 'VideoClassificationPredictionResult',\n",
       " 'VideoObjectTrackingPredictionResult',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'gapic_version',\n",
       " 'package_version',\n",
       " 'types']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dir(endpoint.schema.predict.prediction_v1beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62c24684-9d19-4409-904c-04afbd84b578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "creds, project = google.auth.default()\n",
    "\n",
    "# creds.valid is False, and creds.token is None\n",
    "# Need to refresh credentials to populate those\n",
    "\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b08ca94e-7e65-4a6c-a02e-a3c5f289137f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m video_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2885\u001b[39m\u001b[38;5;66;03m#math.ceil(get_video_duration(video_file))\u001b[39;00m\n\u001b[1;32m     54\u001b[0m video_chapters\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 55\u001b[0m prev\u001b[38;5;241m=\u001b[39m\u001b[43mvideo_start\u001b[49m\n\u001b[1;32m     56\u001b[0m log\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (segments_to_process,video_duration\u001b[38;5;241m+\u001b[39msegments_to_process,segments_to_process):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_start' is not defined"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import time\n",
    "import typing\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "#libraries to generate image summaries\n",
    "from vertexai.vision_models import Video\n",
    "from vertexai.vision_models import VideoSegmentConfig\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part as GenerativeModelPart,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from typing import Any, Dict, List, Literal, Optional, Union\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    " \n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import initializer as aiplatform_initializer\n",
    "import datetime\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "if 1==1:\n",
    "         generative_multimodal_model= GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "         #generation_config= GenerationConfig(temperature=1, top_k=40,top_p=0.95,max_output_tokens=8192) \n",
    "         generation_config=GenerationConfig(temperature=1, top_k=40,top_p=0.95,max_output_tokens=8192)#, response_mime_type='application/json',\n",
    "\t     # response_schema=json.loads(schema))  \n",
    "         #for video, BLOCK_NONE gives error. So, have to set it to BLOCK_ONLY_HIGH\n",
    "         safety_settings=  {\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    }\n",
    "         stream=False\n",
    "        \n",
    "         video_duration=2885#math.ceil(get_video_duration(video_file))\n",
    "         \n",
    "         video_chapters=[]\n",
    "         prev=0\n",
    "         log=[]\n",
    "        segments_to_process=120\n",
    "         \n",
    "         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                offset={'start':prev, 'end':val}\n",
    "                prev=val    \n",
    "                print(offset)\n",
    "                startOffset=offset['start']\n",
    "                endOffset=offset['end']\n",
    "                chapters=[]  \n",
    "                \n",
    "                if endOffset>=video_duration:\n",
    "                    endOffset=video_duration\n",
    "                    \n",
    "                video_description_prompt=f\"\"\"You are an assistant tasked with summarizing videos for retrieval.\\\n",
    "                     These summaries will be embedded and used to retrieve the raw video.\\\n",
    "                    Chapterize the video content by grouping the video content into chapters \\\n",
    "                    with intervals of {intervals} seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\\n",
    "                    If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\n",
    "                    Describe important scenes in the video concisely.\\\n",
    "                    If you are not sure about any info, please do not make it up. \\\n",
    "                    Only consider video from {startOffset} seconds to {endOffset} seconds. Ignore analyzing the rest of video.\\\n",
    "                    If it is the last chapter, set the endOffset to {endOffset} instead.\\ \n",
    "                    If a chapter includes prohibited content, set chapterSummary to \"\".\\\n",
    "                    For result, follow JSON schema.<JSONSchema>{json.dumps(schema)}</JSONSchema>\"\n",
    "                    \"\"\"   \n",
    " \n",
    "        \n",
    "                contents=[parts=[GenerativeModelPart.from_uri(video_file,mime_type=\"video/mp4\", ),\n",
    "                           video_description_prompt,]        \n",
    "\n",
    "                model_response = generative_multimodal_model.generate_content(\n",
    "                                    contents,\n",
    "                                    generation_config=generation_config,\n",
    "                                    stream=stream,\n",
    "                                    safety_settings=safety_settings, )        \n",
    "\n",
    "                response_list = []\n",
    "                    # print(model_response)\n",
    "\n",
    "                print('Called the API, processing the result now...')\n",
    "                prohibited_content=False\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1f66746-d055-4eb5-9d03-f446a35c41b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate_content in module vertexai.generative_models._generative_models:\n",
      "\n",
      "generate_content(contents: Union[List[ForwardRef('Content')], List[Dict[str, Any]], str, ForwardRef('Image'), ForwardRef('Part'), List[Union[str, ForwardRef('Image'), ForwardRef('Part')]]], *, generation_config: Union[ForwardRef('GenerationConfig'), Dict[str, Any], NoneType] = None, safety_settings: Union[List[ForwardRef('SafetySetting')], Dict[google.cloud.aiplatform_v1beta1.types.content.HarmCategory, google.cloud.aiplatform_v1beta1.types.content.SafetySetting.HarmBlockThreshold], NoneType] = None, tools: Optional[List[ForwardRef('Tool')]] = None, tool_config: Optional[ForwardRef('ToolConfig')] = None, stream: bool = False) -> Union[ForwardRef('GenerationResponse'), Iterable[ForwardRef('GenerationResponse')]] method of vertexai.preview.generative_models.GenerativeModel instance\n",
      "    Generates content.\n",
      "    \n",
      "    Args:\n",
      "        contents: Contents to send to the model.\n",
      "            Supports either a list of Content objects (passing a multi-turn conversation)\n",
      "            or a value that can be converted to a single Content object (passing a single message).\n",
      "            Supports\n",
      "            * str, Image, Part,\n",
      "            * List[Union[str, Image, Part]],\n",
      "            * List[Content]\n",
      "        generation_config: Parameters for the generation.\n",
      "        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n",
      "        tools: A list of tools (functions) that the model can try calling.\n",
      "        tool_config: Config shared for all tools provided in the request.\n",
      "        stream: Whether to stream the response.\n",
      "    \n",
      "    Returns:\n",
      "        A single GenerationResponse object if stream == False\n",
      "        A stream of GenerationResponse objects if stream == True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(generative_multimodal_model.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b5c78a9-56c4-4ba2-8f9f-654c35ba45bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: ya29.c.c0ASRK0GazjrHy8T5oxOjgzmUAvpcPg6TrD2uCutKDaCN3dLuPEdjFLtvwDqGKVErmiHO-4iC92m7j-813QysLmaxrDYuXiRI07YF4Fb4YEJx5d8ignk4uyiWgPd7WZnO9CqjQ8HxSB7ZEqZEFuz_WmtVXeSGLyccCDAmgokqzj8CbwV5o0q2Cv2A6R9SMfoLcuYUL7HzUK5QZZLIwb14-cpwKzJzXj-l8UyxGm2Qz2hlhczUKMl--WAIkOmGODPKDCOpdSajsuGRbrKpw13MKrDuZl2HITFRE0nfF1pwClYlqTgt2H_V8OtIN8YYsvNAl-Z3f0bimuUDiA095RSnPipdSqFCN3ECwJrHa-iOmXU9SZhcvQJF7Bg_rrR0ANFRmzKCjuIDly_mHBtgqjjRVH413AshIg25-IsyVXvcg3zxUBkFyJdWI8OwZOSlt8y3Rx95FZsJ8cS7SinQOo2z5wX8Xl832FYb7Jlf1jf-jOUhgraMIk6qget7VmyiJ1me6RV82FMt2mqgx5hrSzQ0o34_03zFvwe2ZpQv0r78JBjZpg9wflsJd6ZqkQqFui8207ozoqRzStc4YU9157lzs3pagOxhV3Ju1OMp32ZROwc97hBys2tSj4ZRiFecXqFYIejMYX399dlZ-qkOrdlS0yqyk3iX3jcIOUox5-rqrOXbB8qh6hrUc4p9uv_WgvMUpYUqii85-4VmXU_czOZ7RYfYoFR6Xm73ZQrMVac3BVkSv73O5vMOrboZFazhcuzJx8c8v458i8nb336m9fdp5BXma4RpiUXVrUQcg7fi0YYJgRv6wcdd78wVBrVbxtMbr0QgZ5ItIBqprY7yv8t6Uc4kcQcMdv7MykUrI_1ObV-5Z9_QgFnt7Z9ouSBnyjyOypetS1hgunf6rU9eQY6U57f16mI-wlhqvJke__mwgkxWoyF0pml9kfrYttXyJZprSb3_wmy1pn95BW-35cF4Rdj9b_M3kBQB9coso3wx_sXYdWtslM7bdZRg\n"
     ]
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "try:\n",
    "    creds = GoogleCredentials.get_application_default()\n",
    "except Exception as e:\n",
    "    quit(e)\n",
    "\n",
    "print(\"Access Token:\", creds.get_access_token().access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aaf3493-2c3c-47be-9166-c64580d2a54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "from google.cloud import storage\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"idx\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"mime_type\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"gcs_uri\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_name\", \"STRING\", mode=\"REQUIRED\")\n",
    "       \n",
    "    ]     \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "\n",
    "@functions_framework.http\n",
    "def get_gcs_info(request):\n",
    "    \"\"\"\n",
    "        loads gcs metadata info into big query table\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "    #request_args = request.args\n",
    " \n",
    "\n",
    "#     project_id=  request_args['project_id']\n",
    "#     dataset_id=  request_args['dataset']\n",
    "#     table= request_args['table']\n",
    "#     region= request_args['region']\n",
    "#     source_bucket= request_args['source_bucket']\n",
    "#     source_folder= request_args['source_folder']\n",
    "#     media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "            \n",
    "    project_id=  \"nine-quality-test\"\n",
    "    dataset_id= 'vlt_metadata'\n",
    "    table= \"vlt_image_metadata\"\n",
    "    region= \"us-central1\"\n",
    "    source_bucket= 'raw_nine_files'\n",
    "    source_folder= \"2024\"\n",
    "    media_types= ['image/png']\n",
    "    \n",
    "            \n",
    "\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(source_bucket)\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder)\n",
    "    \n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()  \n",
    "    rows_to_insert=[]        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "\n",
    "\n",
    "    job_list=[]\n",
    "    idx=0\n",
    "    for blob in blobs:\n",
    "        if blob.content_type in media_types:\n",
    "            rows_to_insert.append(\n",
    "                                {   \"idx\":  idx  , \n",
    "                                    \"name\": blob.name, \n",
    "                                    \"mime_type\":blob.content_type,\n",
    "                                    \"gcs_uri\":  \"gs://\"+source_bucket+\"/\"+blob.name,\n",
    "                                    \"media_name\":blob.name.split(\"/\")[-1].replace(\".\"+blob.content_type.split(\"/\")[-1],\"\")\n",
    "                                    }\n",
    "                                            )\n",
    "        \n",
    "            idx=idx+1\n",
    "            \n",
    "    print(rows_to_insert)\n",
    "    #create table new if does not exist\n",
    "    table=f\"{table}\" \n",
    "    table_schema=create_table(project_id,dataset_id,table)\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema = table_schema\n",
    "    job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)                \n",
    "    job_list.append(job.job_id)\n",
    "  \n",
    "    return {'status':'SUCCESS', 'record_count':idx,'count_of_tables':1,'table_name_prefix':table,'jobs':job_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f20504-2cf7-4f75-a87c-5762526f0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nine-quality-test\n",
      "Dataset 'vlt_metadata' already exists.\n",
      "[{'idx': 0, 'name': '2024/3/d/screenshot_directrycontent.png', 'mime_type': 'image/png', 'gcs_uri': 'gs://raw_nine_files/2024/3/d/screenshot_directrycontent.png', 'media_name': 'screenshot_directrycontent'}]\n",
      "Table 'nine-quality-test.vlt_metadata.vlt_image_metadata' created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS',\n",
       " 'record_count': 1,\n",
       " 'count_of_tables': 1,\n",
       " 'table_name_prefix': TableReference(DatasetReference('nine-quality-test', 'vlt_metadata'), 'vlt_image_metadata'),\n",
       " 'jobs': ['acdf507c-f282-4d3f-bce7-89aad5c12d4d']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gcs_info('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99233f49-0aa6-4804-90c6-926596200397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtable_schema\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table_schema' is not defined"
     ]
    }
   ],
   "source": [
    "table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff1c98b0-8cfd-46a2-be07-271b944c01a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE MODEL `vlt_metadata.multimodal`\n",
    "REMOTE WITH CONNECTION `us-central1.vlt_multimodal_endpoint`\n",
    "OPTIONS(endpoint = 'multimodalembedding@001');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49499f00-1668-47e0-a53b-50eb7c98e5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1852933557.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    CREATE OR REPLACE TABLE `bqml_tutorial.met_image_embeddings`\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "855c1a0f-cf8f-4f2c-9224-6c4d21ef4f23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE EXTERNAL TABLE `vlt_metadata.met_images`\n",
    "WITH CONNECTION `us-central1.vlt_multimodal_endpoint`\n",
    "OPTIONS\n",
    "  ( object_metadata = 'SIMPLE',\n",
    "    uris = ['gs://raw_nine_files/2023/*']\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a236e34f-ddc6-482b-bd15-60a29072fd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid job configuration received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m create_model_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings`\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mAS\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mSELECT *\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT 1000));\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Execute the CREATE MODEL statement\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m query_job \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_model_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Wait for the job to complete\u001b[39;00m\n\u001b[1;32m     20\u001b[0m query_job\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/client.py:2143\u001b[0m, in \u001b[0;36mClient.create_job\u001b[0;34m(self, job_config, retry, timeout)\u001b[0m\n\u001b[1;32m   2136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m   2137\u001b[0m         query,\n\u001b[1;32m   2138\u001b[0m         job_config\u001b[38;5;241m=\u001b[39mtyping\u001b[38;5;241m.\u001b[39mcast(QueryJobConfig, query_job_config),\n\u001b[1;32m   2139\u001b[0m         retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[1;32m   2140\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   2141\u001b[0m     )\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid job configuration received.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid job configuration received."
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings`\n",
    "AS\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.GENERATE_EMBEDDING(\n",
    "    MODEL `vlt_metadata.multimodal`,\n",
    "    (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT 1000));\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb5bb44c-6dda-445b-9c47-13aa382ceb32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings1`\n",
    "AS\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.GENERATE_EMBEDDING(\n",
    "    MODEL `vlt_metadata.multimodal`,\n",
    "    (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT 1000));\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.create_job( job_config={\n",
    "        \"query\": {\n",
    "            \"query\": create_model_sql,\n",
    "        },\n",
    "        \"labels\": {\"example-label\": \"example-value\"},\n",
    "        \"maximum_bytes_billed\": 10000000,\n",
    "    })\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ed19a49-a4c3-42f5-9a68-7c506bde87d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bd806266-d3e0-4264-b7f1-737ece17ae8d'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_job.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2be05-5af6-4375-9795-3bd3e0ad5267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
