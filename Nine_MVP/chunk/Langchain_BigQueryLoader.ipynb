{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835b64-eab5-4b12-8cb8-946b554ab1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8b4a0-9336-4ca8-8483-4ba688f4628c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db12b4e8-a141-4be8-897b-23d9cfc1d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.147.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-google-vertexai \"langchain-google-community[featurestore]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccbd0b78-00e8-47a4-aa70-0b9eba58f44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import google.cloud.bigquery as bq\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader,BigQueryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "from datetime import datetime\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "857e7fd4-aeae-445d-af46-0e1dfa8f6e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_710571/1683864506.py:8: LangChainDeprecationWarning: The class `BigQueryLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import BigQueryLoader``.\n",
      "  loader = BigQueryLoader(\n"
     ]
    }
   ],
   "source": [
    "# Define our query\n",
    "query = \"\"\"\n",
    "SELECT id,media_type,content,test_metadata \n",
    "FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "loader = BigQueryLoader(\n",
    "    query, metadata_columns=[\"id\"], page_content_columns=[\"content\",\"media_type\",\"test_metadata\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da00fa47-16dc-46c4-a501-291698ff1508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "\n",
    "\n",
    "store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08949b5-f302-40d7-b2a9-3a73db018bd7",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/rag-with-bigquery-and-langchain-in-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ea0c737-71e6-4126-a6cb-7ab71393705f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "documents.extend(loader.load())\n",
    " \n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5#,\n",
    "   # separators=[\"\\n\\n\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# get current processing time to add it to metadata, datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Add chunk number to metadata\n",
    "chunk_idx=0\n",
    "prev=doc_splits[0].metadata[\"id\"]\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"process_time\"]=now\n",
    "    if prev==split.metadata[\"id\"]:\n",
    "       split.metadata[\"chunk\"] = chunk_idx      \n",
    "    else:\n",
    "        chunk_idx=0\n",
    "        split.metadata[\"chunk\"] = chunk_idx\n",
    "        prev=split.metadata[\"id\"]\n",
    "    chunk_idx +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f1cca1e-efc9-4f5d-ad4e-80d0e3b4d42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ") \n",
    "\n",
    "_=bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e9714ce-acba-4d90-818f-c3d97fce3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load():\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "    \n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "    chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\n\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     project_id= os.environ.get(\"PROJECT_ID\") \n",
    "#     dataset= os.environ.get(\"DATASET\")  \n",
    "#     table= os.environ.get(\"TABLE\") \n",
    "#     region= os.environ.get(\"REGION\") \n",
    "#     metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "#     page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "#     source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "#     separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "#     chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "#     chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "#     project_id= 'nine-quality-test' \n",
    "#     dataset= 'my_langchain_dataset'\n",
    "#     table= 'doc_and_vectors'\n",
    "#     region= 'us-central1'\n",
    "#     metadata_columns= \"id\".split(\",\")\n",
    "#     page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "#     source_query_str= \"\"\"\n",
    "#     SELECT id,media_type,content,test_metadata \n",
    "#     FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "#     \"\"\"\n",
    "#     separators= \"\\n\\n\"\n",
    "#     chunk_size= 25\n",
    "#     chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "#     message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "#                         metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "#                         source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "#                        chunk_overlap=chunk_overlap)\n",
    "#     return(message)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0424463-a90c-42a9-bb30-99f3a8eded81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b05dbe9-3eda-409d-9821-526491f7b66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "\n",
    "def list_sub_directories(bucket_name, prefix):\n",
    "    \"\"\"Returns a list of sub-directories within the given bucket.\"\"\"\n",
    "    service = googleapiclient.discovery.build('storage', 'v1')\n",
    "\n",
    "    req = service.objects().list(bucket=bucket_name, prefix=prefix, delimiter='/')\n",
    "    \n",
    "    res = req.execute()\n",
    "    \n",
    "    return res['prefixes']\n",
    "\n",
    "# For the example (gs://abc/xyz), bucket_name is 'abc' and the prefix would be 'xyz/'\n",
    "#print(list_sub_directories(bucket_name='raw_nine_files/sub_dir', prefix=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46029a1-d72b-4ee2-a594-848a5bfa4c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023/', '2024/']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_sub_directories(bucket_name=bucket, prefix=prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee98b19e-ae8d-423a-8756-102822b0c081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/1/a/\n",
      "2023/1/b/\n",
      "2023/2/c/\n",
      "2024/3/d/\n"
     ]
    }
   ],
   "source": [
    "bucket='raw_nine_files'\n",
    "prefix=''\n",
    "l=[]\n",
    "for main in list(list_sub_directories(bucket_name=bucket, prefix=prefix)):\n",
    "    for dircty in list(list_sub_directories(bucket_name=bucket, prefix=main )):\n",
    "       for sub_dircty in list(list_sub_directories(bucket_name=bucket, prefix=dircty )):\n",
    "           print(sub_dircty)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "f6048d2c-dd54-46b8-b181-ade334045925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_gcs_objects(bucket_name):\n",
    "    \"\"\"List all objects in a Google Cloud Storage bucket and print their metadata.\"\"\"\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=\"vlt_video_extract\")\n",
    "\n",
    "    print(f\"Objects in bucket '{bucket_name}':\")\n",
    "    for blob in blobs:\n",
    "        print(f\"Name: {blob.name}\")\n",
    "        print(f\"Size: {blob.size} bytes\")\n",
    "        print(f\"Content Type: {blob.content_type}\")\n",
    "        print(f\"Media Link: {blob.public_url}\")  # Get public URL if the object is publicly accessible\n",
    "        print(f\"Created: {blob.time_created}\")\n",
    "        print(f\"Updated: {blob.updated}\")\n",
    "        print(f\"Updated: {blob.self_link}\")\n",
    "        print(f\"Updated: {blob.get}\")\n",
    "        #print(dir(blob))\n",
    "        \n",
    "        print(\"------\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f197a8-25b7-416a-a2a6-f04a6fd5da4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5634.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_duration(\"raw_nine_files\",\"vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced9a6e-a9a9-4f8e-9e39-5d410c5e7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ffcfb4b138e4ddd2456a59167c84bf4021642dff6ad6de28e57d52e-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/2024%2F3%2Fd%2Fscreenshot_directrycontent.png?jk=AdvOr8tEXuKMrGk7hapfYy4ZPHk1yEDCUJKOHuCrx660bHpMtnBM9ujMQYZYlGC3uD9x2FfhMgvBZNcuZcVX6z301E7Zm2RjL3pJkSIu8paY66HCL6H6SLrZ4R1eCjkaxareY7JnsKrMof1amj819hbOcCMgyGfuRm74dLtW501kSwMiUwyd_6yYs6EeoeG38UVmQBiioRHQgCSMdDBu3Ej3hAwXvcIPMfCIhqvjQD7uD1OFecUTJx40xk_o_Yiy-uMA5XNsHRBiPtZ3KnUphFw5BIxArUX3DDMPFpEySCHtPIb67btv3H-FXW8MvV_1HBkw3V-xlDt3lKggVN4WRq3IeS6cF0xFD25jpDSYA-jBKL6f_U0wprpd3918-ldVEE77DysLlD7EnhIn_KErSpWi9qYWnsvqftvC3R-4iJUkehhUjtUiefJrA0uEJsZoSEeevzM_0jwtMY2Rb4LCzroTIYLDwRc4n45ILr13Rev_ATTsSx6ccGzxhfRClVaHrBRY7udek7EXIPJAx8m8MkyQCfKScX4GoWQqmPhn3hNd0LuW44r5xpO-ljrOzsBndeswe0gtwM53ns9ObCMznjoJo3DeGKFxKxsIUxmoSRaLP4bpvz7YgZEIqvrS8Mt1ZxHEXG98_meWlYyPd3A2ir7G7ZLQcxiIBm7-IKms2rmapPc3u5bM69KhjQNR93HYRxCXGzQnIhR8HYXzj9P1_6INOw3UVkBdIYX-jEFsNhkzHHTEIqrmDW_76xQ_wSrr_dOh3Oi8Ri12DXaXksMFBvbJhUGmgETi0QKfqd8EI-ReT3BbZVChbAIJ1RNUe-wd7guT8mKdpxTicWK0KpMnGaU3DL8tlY9GZwpFyRDmJRHwBP7iqVjU4M4jF2KyHwdL589yyxlheVn-fmItOag0vPxkysf4pX0qL52WNP3ch15tN2D8LePFp4S__rQwXD-vdsciDDXBH3DOeLeKWR4-h6lVkYxi6XtHVbLgOE26lurqe1Q0XQQcJYA1usDNdJUNYwuW1qYGiB7VH6PDYSrSHuVj2YB3bmsGxpxOf2vxrhhJZzIhKU3Yxy-BX-tXp0AK8C4oowhu66feVTx6dIl_d3xQlOinsq5kEbsxvb9aSSKPkuNkfFLTbksJhW2-QWHnv4Bwtf7uC6zMLxVfocy-iCZAMTkGa9icvrHJeT3htPvpTQYbHq_kOeTVZUj3Hw-LRcaUQnxQfOTLVWmGXBAyWhWr28l5zblWxYxGMXMP&isca=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b19e03-d39e-40e9-be2e-4aa2ec427834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Duration: 5634.0\n"
     ]
    }
   ],
   "source": [
    "#video_url = \"https://storage.cloud.google.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\"\n",
    "video_url=\"https://ffba87f3998dc58bc084d165a1de6442fbc82dae22183352ca8b1bc-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4?jk=AdvOr8stvY3hRXD9psznNOnR68vfY46qQuSRwB7uO7udZdeU2LGGz-_54PmfT-yobQGxdyq1OJd1dnjB-KHuYc64Ky3hZ45dZPi3LocJ9kOpF2E7EAfzzbh1tnu855pcL63SAw-jIOe4AKQdfRQcXxO-KhuhEkUxHY9jthAzHPVv7wYH5zPsrKLuKIZzlfqJ-kh2KYLi-HlC9SDa9FtEPZbHXSuJZvlUXZK9exW_8Qnxlo5EtNLP386be6siB0rndlMgdKeHPTcO3r7RpXPlfeFm6hQuWDpe-vAW6wRz-BJzlnXOc1-cdXQ7_2Hz85JrdQ6SO-7Sf9fG7Nhk-wSLWGf_9hzaD4AaaVqhCJqmojJGQ1NID5iLry10QGX6r2Ce23Rp0P0dicWb0ePc6BygOVOPM2NZZWyHQO0OJB63aaXsz5MAFTA7dU0oQsz36-d4NzwcEiIfb4Sv86r5uowgeS_pBBOJrv0Y3NlmF1BJ_Eq5bN-q10S0wJ040NMQNs-fWfDKSSCBd-gWovKWk16r4DLSOvt10-EVfNWs8qHEnI9VJB_3E03XxhrONQxX2VqQoCnPMYcAob1lbfezAefR9dpy6xoXvcoB_GajqVoBBLlF6lk8cX69YxsL_gdSzNVJIbRMwVYfdipLKhXxiipUJM3GuB-OlD9OwKjpbYK2hGCxN0GR_pY5mrRREka8FAsxMpZvmEXEgqwYKwMPRCHmtfRj4uAyubVUPP-WCBEQQ_0r8VG1IBFqhLLrM4rqrWwxM5Vejjo6NPS6QP2Va5sOfptdcyLqTXpaNBesHkrO_hyC4ClOtwkhx1-x8rTPHat8VkUHUcPGHug2aVHp8l-zzTs-puLcF4N24hfkD06MVqbUWslkKHXTUiEbRiIOgg854ObQ976NsZI8GPeDjIIbCYzCZyDGUR9lYcJ98oQlopCEtPZsKrwUE-nV40WvQ2ABXGrW3szWrOlaOPQDwmwCTTSgWvlyxWlqI8txdIV3B1pccav3bkOiqlz7e2BcBc6rxaxu_P4Xy9_ZBGggkFO7dIiBizs0O6kkWoFib5985-bXbEvXNdeClyhuTSG9JML5-EiQqMvfm0_uW69Y9LtAUw9QwlMg6AP9nZytgw1aPBTDJlU6nZHM4ARrS7-Tv9QV3eUIGANS466_mAPP-M5wR3sWIy8fq7LhUCWmVxOqCt-iMgiqP6ZgyyRd6AgY1wAk-lshrySf1uDZV5xgeQrp8WhfNS19HNMexHSqreqPYIs&isca=1\"\n",
    "try:\n",
    "    clip = VideoFileClip(video_url)\n",
    "    print(\"Video Duration:\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba45b01-3702-4224-a881-cedccbd6daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from moviepy.editor import VideoFileClip\n",
    "import io\n",
    "\n",
    " \n",
    "# Initialize the storage client\n",
    "client = storage.Client( )\n",
    "\n",
    "# Set your bucket and file names\n",
    "bucket_name = 'raw_nine_files'\n",
    "object_name = 'vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4'\n",
    "\n",
    "# Create a file-like object from GCS\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(object_name)\n",
    "\n",
    "# Download the blob as a byte stream\n",
    "video_stream = io.BytesIO()\n",
    "blob.download_to_file(video_stream)\n",
    "video_stream.seek(0)  # Reset stream position to the beginning\n",
    "\n",
    "# Use moviepy to get the video length from the stream\n",
    "try:\n",
    "    clip = VideoFileClip(video_stream)\n",
    "    print(\"Video Duration (seconds):\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "2c4fe292-1592-4b04-b153-140927adb4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in bucket 'raw_nine_files':\n",
      "Name: vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Size: 3613937486 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:14.024000+00:00\n",
      "Updated: 2024-10-07 01:41:14.024000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Size: 2748590138 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:14.209000+00:00\n",
      "Updated: 2024-10-07 01:41:14.209000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_21_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_21_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Size: 2508037349 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:14.432000+00:00\n",
      "Updated: 2024-10-07 01:41:14.432000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_36_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_36_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Size: 1094890707 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Created: 2024-10-07 01:41:14.648000+00:00\n",
      "Updated: 2024-10-07 01:41:14.648000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW24_21_A.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW24_21_A.mp4\n",
      "------\n",
      "Name: vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Size: 1109220796 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Created: 2024-10-07 01:41:14.853000+00:00\n",
      "Updated: 2024-10-07 01:41:14.853000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW724_21_A.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FNINE_NIEWS%2FSYD-NINE_NNNTW724_21_A.mp4\n",
      "------\n",
      "Name: vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Size: 7183155286 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Created: 2024-10-07 01:41:15.056000+00:00\n",
      "Updated: 2024-10-07 01:41:15.056000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FOTHERS%2Fepisode-18-brs-full-story.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FOTHERS%2Fepisode-18-brs-full-story.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Size: 1744302481 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:15.254000+00:00\n",
      "Updated: 2024-10-07 01:41:15.254000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Size: 1743380331 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Created: 2024-10-07 01:41:15.461000+00:00\n",
      "Updated: 2024-10-07 01:41:15.461000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB_fixed.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI23_33_A_HBB_fixed.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Size: 1850846344 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:15.655000+00:00\n",
      "Updated: 2024-10-07 01:41:15.655000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_1_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_1_A_HBB.mp4\n",
      "------\n",
      "Name: vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Size: 1694178315 bytes\n",
      "Content Type: video/mp4\n",
      "Media Link: https://storage.googleapis.com/raw_nine_files/vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Created: 2024-10-07 01:41:15.847000+00:00\n",
      "Updated: 2024-10-07 01:41:15.847000+00:00\n",
      "Updated: https://www.googleapis.com/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_6_A_HBB.mp4\n",
      "Updated: /b/raw_nine_files/o/vlt_video_extract%2FSIXTY_MINUTES%2F60MI24_6_A_HBB.mp4\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Call the function with your bucket name\n",
    "bucket_name = 'raw_nine_files'\n",
    "list_gcs_objects(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bfb29f21-1f9f-441e-8361-206b471d0af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in bucket raw_nine_files:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Blob' object has no attribute 'mediaLink'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function with your bucket name\u001b[39;00m\n\u001b[1;32m     21\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_nine_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mlist_gcs_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 18\u001b[0m, in \u001b[0;36mlist_gcs_objects\u001b[0;34m(bucket_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blob \u001b[38;5;129;01min\u001b[39;00m blobs:\n\u001b[1;32m     16\u001b[0m     blob \u001b[38;5;241m=\u001b[39m bucket\u001b[38;5;241m.\u001b[39mget_blob(blob\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmediaLink\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Blob' object has no attribute 'mediaLink'"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def list_gcs_objects(bucket_name):\n",
    "    \"\"\"List all objects in a Google Cloud Storage bucket.\"\"\"\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    print(f\"Objects in bucket {bucket_name}:\")\n",
    "    for blob in blobs:\n",
    "        blob = bucket.get_blob(blob.name)\n",
    "\n",
    "        print(blob.mediaLink)\n",
    "\n",
    "# Call the function with your bucket name\n",
    "bucket_name = 'raw_nine_files'\n",
    "list_gcs_objects(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2da52ee7-4a76-4d33-81bf-81e624583f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rows have been added.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load(project_id: str= None, dataset: str= None, table: str= None, region: str =None,\\\n",
    "                        metadata_columns: list[str]=None, page_content_columns: list[str]= None, \\\n",
    "                        source_query_str: str= None, separators:  list[str]=None, chunk_size: int=None, \\\n",
    "                       chunk_overlap: int=0):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "   \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= 1000 if str(os.environ.get(\"CHUNK_SIZE\"))==\"None\" else int(str(os.environ.get(\"CHUNK_SIZE\")))  \n",
    "    chunk_overlap= 0 if str(os.environ.get(\"CHUNK_OVERLAP\"))==\"None\" else int(str(os.environ.get(\"CHUNK_OVERLAP\")))\n",
    " \n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\\\n\\\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "    message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "                        metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "                        source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "                       chunk_overlap=chunk_overlap)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bfba5a73-0e9e-490d-a3f8-04c72069e550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS'}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0):\n",
    "     temp=request_file\n",
    "     client = storage.Client()\n",
    "     # Extract name to the temp file\n",
    "     temp_file = \"\".join([str(temp.name)])\n",
    "     # Uploading the temp image file to the bucket\n",
    "     dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+str(version)+\".json\" \n",
    "     dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "     dest_blob = dest_bucket.blob(dest_filename)\n",
    "     dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                                \n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None):\n",
    "    \"\"\"create batch request file(s) of up to 30000 and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            str path:  path to the gcs request file object\n",
    "          \n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix='2023')  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=30000\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version)\n",
    "                                rf.close()                                \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version)\n",
    " \n",
    "    return 1\n",
    " \n",
    "def create_batch_request_file( ):\n",
    " \n",
    "  \n",
    "  \n",
    "    dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "    source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "    request_file_prefix = \"image_request\"#request_args['request_file_prefix']\n",
    "    request_file_folder =  \"image_batch_request_fldr\"#request_args['request_file_folder']\n",
    "    prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "    media_types= ['image/jpeg','image/png']#list(request_args['media_type'])\n",
    "    request_content= \"image\"#request_args['request_content']\n",
    " \n",
    "    temperature=0.5\n",
    "    max_output_tokens=2048\n",
    "    top_p=50\n",
    "    top_k=0.5\n",
    "\n",
    "    if  request_content=='image':\n",
    "      _=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k\n",
    "                                      )  \n",
    "\n",
    "    return {\"status\":'SUCCESS'}\n",
    "\n",
    "create_batch_request_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8c557d79-e327-410b-ad2d-843624a63db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                                \n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None):\n",
    "\n",
    "    \"\"\"create batch request file(s) of up to 30000 and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=30000\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "def create_batch_request_file():\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    " \n",
    "    source_folder_name='2023'\n",
    "    dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "    source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "    request_file_prefix = \"image_request\"#request_args['request_file_prefix']\n",
    "    request_file_folder =  \"image_batch_request_fldr\"#request_args['request_file_folder']\n",
    "    prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "    media_types= ['image/jpeg','image/png']#list(request_args['media_type'])\n",
    "    request_content= \"image\"#request_args['request_content']\n",
    " \n",
    "    temperature=0.5\n",
    "    max_output_tokens=2048\n",
    "    top_p=50\n",
    "    top_k=0.5\n",
    "\n",
    "\n",
    "    versions=[]\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k\n",
    "                                      )  \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":len(versions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "892bd4e3-490c-43d0-b7a1-38fecfea7735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[285], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_batch_request_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[284], line 174\u001b[0m, in \u001b[0;36mcreate_batch_request_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  request_content\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m   versions\u001b[38;5;241m=\u001b[39mcreate_image_request_file(dest_bucket_name\u001b[38;5;241m=\u001b[39mdest_bucket_name,source_bucket_name\u001b[38;5;241m=\u001b[39msource_bucket_name,source_folder_name\u001b[38;5;241m=\u001b[39msource_folder_name,\n\u001b[1;32m    169\u001b[0m                                   request_file_prefix\u001b[38;5;241m=\u001b[39mrequest_file_prefix,request_file_folder\u001b[38;5;241m=\u001b[39mrequest_file_folder,\n\u001b[1;32m    170\u001b[0m                                   mime_types\u001b[38;5;241m=\u001b[39mmedia_types, prompt_text\u001b[38;5;241m=\u001b[39mprompt_text,temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    171\u001b[0m                                  max_output_tokens\u001b[38;5;241m=\u001b[39mmax_output_tokens,top_p\u001b[38;5;241m=\u001b[39mtop_p,top_k\u001b[38;5;241m=\u001b[39mtop_k\n\u001b[1;32m    172\u001b[0m                                   )  \n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUCCESS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_count\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mversions\u001b[49m\u001b[43m)\u001b[49m}\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "create_batch_request_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9514a955-db7d-41a4-b790-6b30585d23b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Copied 60MI23_33_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Copied 60MI23_33_A_HBB_fixed.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Copied 60MI24_1_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Copied 60MI24_6_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Copied MAAT2024_1_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Copied MAAT2024_21_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Copied MAAT2024_36_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Copied SYD-NINE_NNNTW24_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Copied SYD-NINE_NNNTW724_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Copied episode-18-brs-full-story.mp4 to vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "Copied vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "Copied vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_21_A_HBB.mp4\n",
      "vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "Copied vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4 to vlt_video_extract/MAAT/MAAT2024_36_A_HBB.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "Copied vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW24_21_A.mp4\n",
      "vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "Copied vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4 to vlt_video_extract/NINE_NIEWS/SYD-NINE_NNNTW724_21_A.mp4\n",
      "vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "Copied vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4 to vlt_video_extract/OTHERS/episode-18-brs-full-story.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB_fixed.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_1_A_HBB.mp4\n",
      "vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n",
      "Copied vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4 to vlt_video_extract/SIXTY_MINUTES/60MI24_6_A_HBB.mp4\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def move_files( dest_bucket_name: str= None, source_bucket_name: str= None,\n",
    "                          source_folder: str= None, destination_folder: str= None, mime_types: list[str]= None):\n",
    "    \n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs()  \n",
    " \n",
    "\n",
    "    for blob in blobs:\n",
    "        # Check if the file is an .mp4 file\n",
    "        if blob.content_type in [\"video/mp4\"]:\n",
    "        \n",
    "            # Create the new blob name for the destination\n",
    "            temp=blob.name.split(\"/\")\n",
    "            name=blob.name.split('/')[len(temp)-1]\n",
    "            if name.startswith(\"SYD-NINE\"):\n",
    "                dest_folder= f\"{destination_folder}/NINE_NIEWS\"\n",
    "            elif name.startswith(\"MAAT\"):\n",
    "                dest_folder= f\"{destination_folder}/MAAT\"\n",
    "            elif name.startswith(\"60MI\"):\n",
    "                dest_folder= f\"{destination_folder}/SIXTY_MINUTES\"\n",
    "            else:\n",
    "                 dest_folder= f\"{destination_folder}/OTHERS\"\n",
    "                     \n",
    "            new_blob_name = f\"{dest_folder}/{name}\"\n",
    "            print(new_blob_name)\n",
    "\n",
    "            # Copy the blob to the new location\n",
    "            bucket.copy_blob(blob, bucket, new_blob_name)\n",
    "            print(f\"Copied {blob.name} to {new_blob_name}\")\n",
    "\n",
    "# Example usage\n",
    "bucket_name = 'raw_nine_files'\n",
    "source_folder = ''  # Make sure to include the trailing slash\n",
    "destination_folder = 'vlt_video_extract'\n",
    "\n",
    "move_files(dest_bucket_name=bucket_name,source_bucket_name=bucket_name, source_folder=source_folder, destination_folder=destination_folder,mime_types=[])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "ff0ec9a2-e279-4f9e-bd31-64294f9d6a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "       \n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Truncate the table if exist\n",
    "        query = f\"TRUNCATE TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' truncated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error truncating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "        #add partitioning\n",
    "        # table.time_partitioning = bigquery.TimePartitioning(\n",
    "        #     type_=bigquery.TimePartitioningType.DAY,\n",
    "        #     field=\"request_id\",  # This field will be used for partitioning\n",
    "        # )\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5bc05941-78d7-45ae-bc59-77841f2b803f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''\n",
    " \n",
    "    try:\n",
    "        project_id=  request_args['project_id']\n",
    "        dataset_id=  request_args['dataset']\n",
    "        table= request_args['table']\n",
    "        region= request_args['region']\n",
    "        metadata_columns= str(request_args['metadata_columns']).split(',') \n",
    "        page_content_columns= str(request_args['page_content_columns']).split(',') \n",
    "        source_query_str= request_args['source_query_str']\n",
    "        separators= None if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "        chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "        chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap']))  \n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= 'chunked_data'\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [ col.strip() for col in  \"id,content,path,media_type,test_metadata\".split(\",\")]\n",
    "            page_content_columns= [col.strip() for col in \"content,test_metadata\".split(',') ]\n",
    "            source_query_str= \"\"\"\n",
    "            SELECT id,media_type,content,test_metadata, path\n",
    "            FROM `nine-quality-test.Nine_Quality_Test.test_long_artcles` ;\n",
    "            \"\"\"\n",
    "            separators= \"\\n\"\n",
    "            chunk_size= 500\n",
    "            chunk_overlap= 10\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    request_date=datetime.today().strftime('%Y_%m_%d')\n",
    "    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=30000 #maximum number of requests in a batch\n",
    " \n",
    "    max_index=2\n",
    "    record_count=0\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"id\"]\n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = request_date+'_'+str(version)\n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"id\": split.metadata[\"id\"], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\":  split.page_content,\n",
    "                                   \"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   \"media_type\": split.metadata[\"media_type\"],\n",
    "                                   \"path\": split.metadata[\"path\"],\n",
    "                                   \"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{dataset_id}_{request_id}\"\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]\n",
    "    \n",
    "    return {'status':'SUCCESS', 'record_count':record_count, 'count_of_tables':version+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "6bc54925-7ff7-47a9-863f-8bc6f142cd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'langchain_dataset' already exists.\n",
      "Table 'nine-quality-test.langchain_dataset.langchain_dataset_2024_10_08_0' created successfully.\n",
      "LoadJob<project=nine-quality-test, location=us-central1, id=7a692df9-5ea4-44dc-b0b0-07fdf4e90f5b>\n",
      "Table 'nine-quality-test.langchain_dataset.langchain_dataset_2024_10_08_1' created successfully.\n",
      "LoadJob<project=nine-quality-test, location=us-central1, id=62281b95-20fd-480c-946d-79189b1475c5>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'record_count': 4, 'count_of_tables': 2}"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_bq_content('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c60f3-8855-4a89-a0a3-ade9fb6ae849",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Truncate the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "\n",
    "if 1==1:\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "\n",
    " \n",
    "\n",
    "    # project_id=  request_args['project_id']\n",
    "    # dataset_id=  request_args['dataset']\n",
    "    # table= request_args['table']\n",
    "    # region= request_args['region']\n",
    "    # metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "    # page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "    # source_query_str= request_args['source_query_str']\n",
    "    # #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "    # chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "    # chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "    # max_prompt_count_limit=3000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "    # # except Exception as e: \n",
    "    if 1==1:\n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= 'chunked_data'\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [ col.strip() for col in  \"id, content, path, media_type, test_metadata\".split(\",\")]\n",
    "            page_content_columns= [col.strip() for col in \"content, test_metadata\".split(',') ]\n",
    "            source_query_str= \"\"\"\n",
    "            SELECT id,media_type,content,test_metadata, path\n",
    "            FROM `nine-quality-test.Nine_Quality_Test.test_long_artcles` ;\n",
    "            \"\"\"\n",
    "            #separators= \"\\n\"\n",
    "            chunk_size= 500\n",
    "            chunk_overlap= 10\n",
    "            max_prompt_count_limit=2\n",
    "     \n",
    "\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters '+e}\n",
    "\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    request_date=datetime.today().strftime('%Y_%m_%d')    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}_{request_date}\" \n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"id\"]\n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = request_date+'_'+str(version)\n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"id\": split.metadata[\"id\"], \n",
    "                                  ## \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\":  split.page_content,\n",
    "                                   \"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   \"media_type\": split.metadata[\"media_type\"],\n",
    "                                   \"path\": split.metadata[\"path\"],\n",
    "                                   \"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{prefix}_{version}\"\n",
    "               \n",
    "                print(table)\n",
    "                print('************')\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]\n",
    "                print(job.job_id)\n",
    "    \n",
    "    #return {'status':'SUCCESS', 'record_count':record_count, 'count_of_tables':version+1, 'table_name_prefix':prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55942516-176e-4dc7-8226-c95e6cf08201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_bq_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d64fcb8-d6b3-4c81-95b3-b6a8c156a343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5634\n",
      "4285\n",
      "3910\n",
      "2701\n",
      "2690\n",
      "2980\n",
      "2719\n",
      "2720\n",
      "2885\n",
      "2641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from moviepy.editor import VideoFileClip\n",
    "import datetime as date_time\n",
    "\n",
    "import math\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                                \n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", max_prompt_count_limit: int=30000, temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None):\n",
    "\n",
    "    \"\"\"create batch request file(s) of up to 30000 and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int : as number of generated request files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_prompt_count_limit\n",
    "\n",
    "    now=dt.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=dt.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "\n",
    "\n",
    "def generate_signed_url_v4(blob):\n",
    "    \"\"\"Generates a v4 signed URL for downloading a blob.\n",
    "\n",
    "    Note that this method requires a service account key file. You can not use\n",
    "    this if you are using Application Default Credentials from Google Compute\n",
    "    Engine or from the Google Cloud SDK.\n",
    "    \"\"\"\n",
    "\n",
    "    from google.auth.transport import requests\n",
    "    from google.auth import default, compute_engine\n",
    "    \n",
    "    credentials, _ = default()\n",
    "    \n",
    "    # then within your abstraction\n",
    "    auth_request = requests.Request()\n",
    "    credentials.refresh(auth_request)\n",
    "    \n",
    "    signing_credentials = compute_engine.IDTokenCredentials(\n",
    "        auth_request,\n",
    "        \"\",\n",
    "        service_account_email=credentials.service_account_email\n",
    "    )\n",
    "    url = blob.generate_signed_url(\n",
    "        version=\"v4\",\n",
    "        # This URL is valid for 15 minutes\n",
    "        expiration=date_time.timedelta(minutes=15),\n",
    "        # Allow GET requests using this URL.\n",
    "        method=\"GET\",\n",
    "         credentials=signing_credentials,\n",
    "    )\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_video_duration(signed_url):\n",
    "   \n",
    "  try:\n",
    "    video_uri=signed_url\n",
    "    \n",
    "    clip = VideoFileClip(video_uri)\n",
    "    duration = clip.duration\n",
    "    clip.close()  # Release resources\n",
    "    return duration\n",
    "  except Exception as e:\n",
    "    if \"moov atom not found\" in str(e):\n",
    "      print(\"Error: The video file seems to be corrupted or incomplete.\")\n",
    "     \n",
    "    else:\n",
    "      print(f\"Error reading video file: {e}\")\n",
    "    \n",
    "\n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", max_prompt_count_limit: int=30000, temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None):\n",
    "\n",
    "    \"\"\"create batch request file(s) of up to 30000 and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_prompt_count_limit\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         signed_url= generate_signed_url_v4(blob)\n",
    "                         video_duration = 2641# math.ceil(get_video_duration(signed_url))\n",
    "                         print(video_duration)\n",
    "                         for \n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": {\n",
    "                                            \"contents\": {\n",
    "                                                \"parts\": [\n",
    "                                                    {\n",
    "                                                        \"fileData\": {\n",
    "                                                            \"fileUri\": \"gs://raw_nine_files/60MI23_33_A_HBB.mp4\",\n",
    "                                                            \"mimeType\": \"video/mp4\"\n",
    "                                                        },\n",
    "                                                        \"videoMetadata\": {\n",
    "                                                            \"endOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 240\n",
    "                                                            },\n",
    "                                                            \"startOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 120\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                        \"text\": \"You are an assistant tasked with describing videos for retrieval.These descriptions will be embedded and used to retrieve the raw video.\\n Chapterize the video content by grouping the video content into chapters with intervals of 120 seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\nIf there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\n Describe important scenes in the video concisely. If you are not sure about any info, please do not make it up. \\n Only consider video from 120 seconds to 240 seconds. \\nIf it is the last chapter, set the endOffset to 240 instead.\\n For result, follow JSON schema.<JSONSchema>{\\\"description\\\":\\\"A list of chapters\\\",\\\"items\\\":{\\\"properties\\\":{\\\"Content\\\":{\\\"type\\\":\\\"string\\\"},\\\"endOffset\\\":{\\\"type\\\":\\\"integer\\\"},\\\"startOffset\\\":{\\\"type\\\":\\\"integer\\\"}},\\\"required\\\":[\\\"startOffset\\\",\\\"endOffset\\\",\\\"Content\\\"],\\\"type\\\":\\\"object\\\"},\\\"type\\\":\\\"array\\\"}</JSONSchema>\"\n",
    "                                                    }\n",
    "                                                ],\n",
    "                                                \"role\": \"user\"\n",
    "                                            },\n",
    "                                            \"generation_config\": {\n",
    "                                                \"max_output_tokens\": 2048,\n",
    "                                                \"temperature\": 0.5,\n",
    "                                                \"top_k\": 40,\n",
    "                                                \"top_p\": 0.8\n",
    "                                            },\n",
    "                                            \"safety_settings\": [\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        }\n",
    "                                    }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request_json = request.get_json(silent=True)\n",
    "        request_args = request.args\n",
    "\n",
    "        dest_bucket_name =request_args['destination_bucket']\n",
    "        source_bucket_name =request_args['source_bucket']\n",
    "        source_folder_name=request_args['source_folder']\n",
    "        request_file_prefix =request_args['request_file_prefix']\n",
    "        request_file_folder =request_args['request_file_folder']\n",
    "        prompt_text= request_args['prompt_text']\n",
    "        max_prompt_count_limit=int(str(request_args['max_prompt_count_limit']))  \n",
    "        media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "\n",
    "        request_content= request_args['request_content']\n",
    "\n",
    "        if request_args and 'temperature' in request_args:\n",
    "            temperature= request_args['temperature']\n",
    "        else:\n",
    "          temperature=0.5\n",
    "\n",
    "        if request_args and 'max_output_tokens' in request_args:\n",
    "           max_output_tokens= request_args['max_output_tokens'] \n",
    "        else:\n",
    "             max_output_tokens=2048\n",
    "\n",
    "        if request_args and 'top_p' in request_args:\n",
    "            top_p= request_args['top_p']\n",
    "        else:\n",
    "             top_p=0.5\n",
    "\n",
    "        if request_args and 'top_k' in request_args:\n",
    "            top_k= request_args['top_k']\n",
    "        else:\n",
    "             top_k=50\n",
    "    except:\n",
    "            dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "            source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "            request_file_prefix = \"image_request\"#request_args['request_file_prefix']\n",
    "            request_file_folder =  \"image_batch_request_fldr\"#request_args['request_file_folder']\n",
    "            prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "            media_types= ['image/jpeg','image/png']#list(request_args['media_type'])\n",
    "            request_content= \"image\"#request_args['request_content']\n",
    "\n",
    "            temperature=0.5\n",
    "            max_output_tokens=2048\n",
    "            top_p=50\n",
    "            top_k=0.5\n",
    "            request_content=\"video\"\n",
    "            media_types= ['video/mp4']\n",
    "            max_prompt_count_limit=2\n",
    "            source_folder_name='vlt_video_extract'\n",
    "            prompt_text=\"You are an assistant tasked with describing videos for retrieval. \\\n",
    "                                Describe this video well optimized for retrieval. Mention anything important the people talk about.\\\n",
    "                                If there is a text or logo mention that in details. If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\n",
    "                                If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"\n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,max_prompt_count_limit=max_prompt_count_limit,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k\n",
    "                                      )  \n",
    "    elif request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,max_prompt_count_limit=max_prompt_count_limit,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k\n",
    "                                      )  \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n",
    "\n",
    "\n",
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba5aeac-eb33-42c7-9d3d-04476356f193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import gcsfs\n",
    "from pymediainfo import MediaInfo\n",
    "\n",
    "def get_video_metadata_from_stream(gcsuri):\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "        # Use pymediainfo to extract metadata directly from the stream\n",
    "        media_info = MediaInfo.parse(video_file)\n",
    "        for track in media_info.tracks:\n",
    "            if track.track_type == 'Video':\n",
    "                return track.duration / 1000  # Convert ms to seconds\n",
    "\n",
    "# bucket_name = 'raw_nine_files'\n",
    "# source_blob_name = 'vlt_video_extract/SIXTY_MINUTES/60MI23_33_A_HBB.mp4'\n",
    "\n",
    "# # Get video metadata (duration) from stream\n",
    "# duration = get_video_metadata_from_stream(bucket_name, source_blob_name)\n",
    "# print(f\"Video duration: {duration} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d80235-e734-407e-84e9-3694dcf705a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_video_metadata( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          metadata_file_prefix: str= None, metadata_file_folder: str= None, mime_types: list[str]= None \n",
    "                           ):\n",
    "\n",
    "    \"\"\" generates video length\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    \n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         video_duration=get_video_metadata_from_stream(gcsuri)\n",
    "                         print(video_duration)\n",
    " \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf53e216-3eff-4dd5-87b1-2a7d6e5706cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5634.0\n"
     ]
    }
   ],
   "source": [
    "video_duration=get_video_metadata_from_stream(\"gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\")\n",
    "print(video_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62c53298-0bfb-47ab-800b-f0d7cf69b7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005.0\n"
     ]
    }
   ],
   "source": [
    "video_duration=get_video_metadata_from_stream(\"gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4\")\n",
    "print(video_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e00b3d2-521f-495e-a1d3-e6bc5bf40625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5634.0\n",
      "4285.0\n",
      "3910.0\n",
      "2700.04\n",
      "2690.0\n",
      "2979.01\n",
      "1005.0\n",
      "2719.0\n",
      "2719.04\n",
      "2885.0\n",
      "2641.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_video_metadata(   source_bucket_name=\"raw_nine_files\", source_folder_name=\"vlt_video_extract\",\n",
    "                           mime_types=['video/mp4']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90c12451-fb58-4edd-9d53-8dde1eaa087c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320504668865953792\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "import datetime\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "\n",
    " \n",
    "system_instruction = \"\"\"\n",
    "You are an expert researcher. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at these research papers, and answer the following questions.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(days=2),\n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)\n",
    "# Example response:\n",
    "# 1234567890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df716674-c8cd-4880-91f2-df9ff80f4963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cf3d063-c54f-42ed-ace5-ada7a184ec39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/494586852359/locations/us-central1/cachedContents/1320504668865953792',\n",
       " 'model': 'projects/nine-quality-test/locations/us-central1/publishers/google/models/gemini-1.5-pro-002',\n",
       " 'createTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'updateTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'expireTime': '2024-10-12T00:05:18.129147Z',\n",
       " 'displayName': 'example-cache'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_content.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0db5932-f361-467b-b6b2-9ef846dad71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n",
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-10-12T23:45:42.158262Z'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import the Google Cloud client library and JSON library\n",
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('raw_nine_files')\n",
    "blob = bucket.blob('test111.json')\n",
    "\n",
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for x in data['items']:\n",
    "    print(x)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "data[\"cache_expiry_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76829048-51c8-4427-8ca1-475e0553df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for item in enumerate( data.items):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebe51b55-bf56-44a1-ba11-a8dfe1cfd946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 99, 'name': 'Guillaume'}\n",
      "{'age': 9988, 'name': 'Guillaume1'}\n",
      "{'age': 5, 'name': 'Guillaume2'}\n"
     ]
    }
   ],
   "source": [
    "for x in data['items']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae3296-e97c-46ae-8ad2-2647a3c92c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": {\n",
    "                                            \"contents\": {\n",
    "                                                \"parts\": [\n",
    "                                                    {\n",
    "                                                        \"fileData\": {\n",
    "                                                            \"fileUri\": \"gs://raw_nine_files/60MI23_33_A_HBB.mp4\",\n",
    "                                                            \"mimeType\": \"video/mp4\"\n",
    "                                                        },\n",
    "                                                        \"videoMetadata\": {\n",
    "                                                            \"endOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 240\n",
    "                                                            },\n",
    "                                                            \"startOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 120\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                        \"text\": \"You are an assistant tasked with describing videos for retrieval.These descriptions will be embedded and used to retrieve the raw video.\\n Chapterize the video content by grouping the video content into chapters with intervals of 120 seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\nIf there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\n Describe important scenes in the video concisely. If you are not sure about any info, please do not make it up. \\n Only consider video from 120 seconds to 240 seconds. \\nIf it is the last chapter, set the endOffset to 240 instead.\\n For result, follow JSON schema.<JSONSchema>{\\\"description\\\":\\\"A list of chapters\\\",\\\"items\\\":{\\\"properties\\\":{\\\"Content\\\":{\\\"type\\\":\\\"string\\\"},\\\"endOffset\\\":{\\\"type\\\":\\\"integer\\\"},\\\"startOffset\\\":{\\\"type\\\":\\\"integer\\\"}},\\\"required\\\":[\\\"startOffset\\\",\\\"endOffset\\\",\\\"Content\\\"],\\\"type\\\":\\\"object\\\"},\\\"type\\\":\\\"array\\\"}</JSONSchema>\"\n",
    "                                                    }\n",
    "                                                ],\n",
    "                                                \"role\": \"user\"\n",
    "                                            },\n",
    "                                            \"generation_config\": {\n",
    "                                                \"max_output_tokens\": 2048,\n",
    "                                                \"temperature\": 0.5,\n",
    "                                                \"top_k\": 40,\n",
    "                                                \"top_p\": 0.8\n",
    "                                            },\n",
    "                                            \"safety_settings\": [\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        }\n",
    "                                    }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcba9a1d-5832-4e71-83bc-7b13d46c491a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6129645383455866880\n",
      "Deleting CachedContent : projects/nine-quality-test/locations/us-central1/cachedContents/6129645383455866880\n",
      "5842118694745931776\n",
      "Deleting CachedContent : projects/nine-quality-test/locations/us-central1/cachedContents/5842118694745931776\n",
      "81311083897290752\n",
      "Deleting CachedContent : projects/nine-quality-test/locations/us-central1/cachedContents/81311083897290752\n",
      "4461061721515098112\n",
      "Deleting CachedContent : projects/nine-quality-test/locations/us-central1/cachedContents/4461061721515098112\n",
      "8633646776273862656\n",
      "Deleting CachedContent : projects/nine-quality-test/locations/us-central1/cachedContents/8633646776273862656\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "# cache_id = \"your-cache-id\"\n",
    " \n",
    "# cached_content = caching.CachedContent(cached_content_name=cache_id)\n",
    "# cached_content.delete()\n",
    "\n",
    "for x in caching.CachedContent.list():\n",
    "     x=x.name.split(\"/\")[-1]\n",
    "     print(x)\n",
    "     cached_content = caching.CachedContent(cached_content_name=x)\n",
    "     cached_content.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46dedaab-b53c-4f9b-9e17-8037bf254d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VideoRequest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_video_duration\u001b[39m(request: \u001b[43mVideoRequest\u001b[49m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" generates video length\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m   \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m          \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     fs \u001b[38;5;241m=\u001b[39m gcsfs\u001b[38;5;241m.\u001b[39mGCSFileSystem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VideoRequest' is not defined"
     ]
    }
   ],
   "source": [
    "def get_video_duration(request: VideoRequest):\n",
    "    \"\"\" generates video length\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7b74fb8-6815-4022-bdfc-bd13da79aaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import gcsfs\n",
    "from pymediainfo import MediaInfo\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the input data model\n",
    "class VideoRequest(BaseModel):\n",
    "    url: str  # Input: video URL\n",
    "\n",
    "# Define the output data model\n",
    "class VideoResponse(BaseModel):\n",
    "    duration: float  # Output: Duration in seconds\n",
    "\n",
    "@app.post(\"/get-video-duration\", response_model=VideoResponse)\n",
    "async def get_video_duration(request: VideoRequest):\n",
    "    \"\"\"\n",
    "    get video duration of a given gcs url\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break\n",
    " \n",
    "\n",
    "    return VideoResponse(duration=duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1998d85-7777-4975-8454-6ebe00317d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
