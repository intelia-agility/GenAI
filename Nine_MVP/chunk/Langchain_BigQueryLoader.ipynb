{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835b64-eab5-4b12-8cb8-946b554ab1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8b4a0-9336-4ca8-8483-4ba688f4628c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db12b4e8-a141-4be8-897b-23d9cfc1d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.147.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-google-vertexai \"langchain-google-community[featurestore]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccbd0b78-00e8-47a4-aa70-0b9eba58f44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import google.cloud.bigquery as bq\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader,BigQueryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "from datetime import datetime\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "857e7fd4-aeae-445d-af46-0e1dfa8f6e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_710571/1683864506.py:8: LangChainDeprecationWarning: The class `BigQueryLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import BigQueryLoader``.\n",
      "  loader = BigQueryLoader(\n"
     ]
    }
   ],
   "source": [
    "# Define our query\n",
    "query = \"\"\"\n",
    "SELECT id,media_type,content,test_metadata \n",
    "FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "loader = BigQueryLoader(\n",
    "    query, metadata_columns=[\"id\"], page_content_columns=[\"content\",\"media_type\",\"test_metadata\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da00fa47-16dc-46c4-a501-291698ff1508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "\n",
    "\n",
    "store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08949b5-f302-40d7-b2a9-3a73db018bd7",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/rag-with-bigquery-and-langchain-in-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ea0c737-71e6-4126-a6cb-7ab71393705f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "documents.extend(loader.load())\n",
    " \n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5#,\n",
    "   # separators=[\"\\n\\n\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# get current processing time to add it to metadata, datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Add chunk number to metadata\n",
    "chunk_idx=0\n",
    "prev=doc_splits[0].metadata[\"id\"]\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"process_time\"]=now\n",
    "    if prev==split.metadata[\"id\"]:\n",
    "       split.metadata[\"chunk\"] = chunk_idx      \n",
    "    else:\n",
    "        chunk_idx=0\n",
    "        split.metadata[\"chunk\"] = chunk_idx\n",
    "        prev=split.metadata[\"id\"]\n",
    "    chunk_idx +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f1cca1e-efc9-4f5d-ad4e-80d0e3b4d42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ") \n",
    "\n",
    "_=bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e9714ce-acba-4d90-818f-c3d97fce3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rows have been added.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_embedding(project_id: str= None, dataset: str= None, table: str= None, region: str =None,\\\n",
    "                        metadata_columns: list[str]=None, page_content_columns: list[str]= None, \\\n",
    "                        source_query_str: str= None, separators:  list[str]=None, chunk_size: int=None, \\\n",
    "                       chunk_overlap: int=0):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "    chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\n\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "    message=chunk_and_embedding(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "                        metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "                        source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "                       chunk_overlap=chunk_overlap)\n",
    "    print(message)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0424463-a90c-42a9-bb30-99f3a8eded81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b05dbe9-3eda-409d-9821-526491f7b66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "\n",
    "def list_sub_directories(bucket_name, prefix):\n",
    "    \"\"\"Returns a list of sub-directories within the given bucket.\"\"\"\n",
    "    service = googleapiclient.discovery.build('storage', 'v1')\n",
    "\n",
    "    req = service.objects().list(bucket=bucket_name, prefix=prefix, delimiter='/')\n",
    "    \n",
    "    res = req.execute()\n",
    "    \n",
    "    return res['prefixes']\n",
    "\n",
    "# For the example (gs://abc/xyz), bucket_name is 'abc' and the prefix would be 'xyz/'\n",
    "#print(list_sub_directories(bucket_name='raw_nine_files/sub_dir', prefix=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46029a1-d72b-4ee2-a594-848a5bfa4c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023/', '2024/']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_sub_directories(bucket_name=bucket, prefix=prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee98b19e-ae8d-423a-8756-102822b0c081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/1/a/\n",
      "2023/1/b/\n",
      "2023/2/c/\n",
      "2024/3/d/\n"
     ]
    }
   ],
   "source": [
    "bucket='raw_nine_files'\n",
    "prefix=''\n",
    "l=[]\n",
    "for main in list(list_sub_directories(bucket_name=bucket, prefix=prefix)):\n",
    "    for dircty in list(list_sub_directories(bucket_name=bucket, prefix=main )):\n",
    "       for sub_dircty in list(list_sub_directories(bucket_name=bucket, prefix=dircty )):\n",
    "           print(sub_dircty)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb29f21-1f9f-441e-8361-206b471d0af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
