{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835b64-eab5-4b12-8cb8-946b554ab1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8b4a0-9336-4ca8-8483-4ba688f4628c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db12b4e8-a141-4be8-897b-23d9cfc1d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.147.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-google-vertexai \"langchain-google-community[featurestore]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccbd0b78-00e8-47a4-aa70-0b9eba58f44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import google.cloud.bigquery as bq\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader,BigQueryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "from datetime import datetime\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "857e7fd4-aeae-445d-af46-0e1dfa8f6e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_710571/1683864506.py:8: LangChainDeprecationWarning: The class `BigQueryLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import BigQueryLoader``.\n",
      "  loader = BigQueryLoader(\n"
     ]
    }
   ],
   "source": [
    "# Define our query\n",
    "query = \"\"\"\n",
    "SELECT id,media_type,content,test_metadata \n",
    "FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "loader = BigQueryLoader(\n",
    "    query, metadata_columns=[\"id\"], page_content_columns=[\"content\",\"media_type\",\"test_metadata\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da00fa47-16dc-46c4-a501-291698ff1508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "\n",
    "\n",
    "store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08949b5-f302-40d7-b2a9-3a73db018bd7",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/rag-with-bigquery-and-langchain-in-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ea0c737-71e6-4126-a6cb-7ab71393705f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "documents.extend(loader.load())\n",
    " \n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5#,\n",
    "   # separators=[\"\\n\\n\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# get current processing time to add it to metadata, datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Add chunk number to metadata\n",
    "chunk_idx=0\n",
    "prev=doc_splits[0].metadata[\"id\"]\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"process_time\"]=now\n",
    "    if prev==split.metadata[\"id\"]:\n",
    "       split.metadata[\"chunk\"] = chunk_idx      \n",
    "    else:\n",
    "        chunk_idx=0\n",
    "        split.metadata[\"chunk\"] = chunk_idx\n",
    "        prev=split.metadata[\"id\"]\n",
    "    chunk_idx +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f1cca1e-efc9-4f5d-ad4e-80d0e3b4d42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table nine-quality-test.my_langchain_dataset.doc_and_vectors initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=nine-quality-test&ws=!1m5!1m4!4m3!1snine-quality-test!2smy_langchain_dataset!3sdoc_and_vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET = \"my_langchain_dataset\"  # @param {type: \"string\"}\n",
    "TABLE = \"doc_and_vectors\"  # @param {type: \"string\"}\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")\n",
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ") \n",
    "\n",
    "_=bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e9714ce-acba-4d90-818f-c3d97fce3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load():\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "    \n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "    chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\n\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     project_id= os.environ.get(\"PROJECT_ID\") \n",
    "#     dataset= os.environ.get(\"DATASET\")  \n",
    "#     table= os.environ.get(\"TABLE\") \n",
    "#     region= os.environ.get(\"REGION\") \n",
    "#     metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "#     page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "#     source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "#     separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "#     chunk_size= os.environ.get(\"CHUNK_SIZE\")\n",
    "#     chunk_overlap= 0 if os.environ.get(\"CHUNK_OVERLAP\")==\"\" else os.environ.get(\"CHUNK_OVERLAP\")\n",
    " \n",
    "\n",
    "#     project_id= 'nine-quality-test' \n",
    "#     dataset= 'my_langchain_dataset'\n",
    "#     table= 'doc_and_vectors'\n",
    "#     region= 'us-central1'\n",
    "#     metadata_columns= \"id\".split(\",\")\n",
    "#     page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "#     source_query_str= \"\"\"\n",
    "#     SELECT id,media_type,content,test_metadata \n",
    "#     FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "#     \"\"\"\n",
    "#     separators= \"\\n\\n\"\n",
    "#     chunk_size= 25\n",
    "#     chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "#     message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "#                         metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "#                         source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "#                        chunk_overlap=chunk_overlap)\n",
    "#     return(message)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0424463-a90c-42a9-bb30-99f3a8eded81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b05dbe9-3eda-409d-9821-526491f7b66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "\n",
    "def list_sub_directories(bucket_name, prefix):\n",
    "    \"\"\"Returns a list of sub-directories within the given bucket.\"\"\"\n",
    "    service = googleapiclient.discovery.build('storage', 'v1')\n",
    "\n",
    "    req = service.objects().list(bucket=bucket_name, prefix=prefix, delimiter='/')\n",
    "    \n",
    "    res = req.execute()\n",
    "    \n",
    "    return res['prefixes']\n",
    "\n",
    "# For the example (gs://abc/xyz), bucket_name is 'abc' and the prefix would be 'xyz/'\n",
    "#print(list_sub_directories(bucket_name='raw_nine_files/sub_dir', prefix=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46029a1-d72b-4ee2-a594-848a5bfa4c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023/', '2024/']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_sub_directories(bucket_name=bucket, prefix=prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee98b19e-ae8d-423a-8756-102822b0c081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/1/a/\n",
      "2023/1/b/\n",
      "2023/2/c/\n",
      "2024/3/d/\n"
     ]
    }
   ],
   "source": [
    "bucket='raw_nine_files'\n",
    "prefix=''\n",
    "l=[]\n",
    "for main in list(list_sub_directories(bucket_name=bucket, prefix=prefix)):\n",
    "    for dircty in list(list_sub_directories(bucket_name=bucket, prefix=main )):\n",
    "       for sub_dircty in list(list_sub_directories(bucket_name=bucket, prefix=dircty )):\n",
    "           print(sub_dircty)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "f6048d2c-dd54-46b8-b181-ade334045925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_gcs_objects(bucket_name):\n",
    "    \"\"\"List all objects in a Google Cloud Storage bucket and print their metadata.\"\"\"\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=\"vlt_video_extract\")\n",
    "\n",
    "    print(f\"Objects in bucket '{bucket_name}':\")\n",
    "    for blob in blobs:\n",
    "        print(f\"Name: {blob.name}\")\n",
    "        print(f\"Size: {blob.size} bytes\")\n",
    "        print(f\"Content Type: {blob.content_type}\")\n",
    "        print(f\"Media Link: {blob.public_url}\")  # Get public URL if the object is publicly accessible\n",
    "        print(f\"Created: {blob.time_created}\")\n",
    "        print(f\"Updated: {blob.updated}\")\n",
    "        print(f\"Updated: {blob.self_link}\")\n",
    "        print(f\"Updated: {blob.get}\")\n",
    "        #print(dir(blob))\n",
    "        \n",
    "        print(\"------\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f197a8-25b7-416a-a2a6-f04a6fd5da4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5634.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_duration(\"raw_nine_files\",\"vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced9a6e-a9a9-4f8e-9e39-5d410c5e7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ffcfb4b138e4ddd2456a59167c84bf4021642dff6ad6de28e57d52e-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/2024%2F3%2Fd%2Fscreenshot_directrycontent.png?jk=AdvOr8tEXuKMrGk7hapfYy4ZPHk1yEDCUJKOHuCrx660bHpMtnBM9ujMQYZYlGC3uD9x2FfhMgvBZNcuZcVX6z301E7Zm2RjL3pJkSIu8paY66HCL6H6SLrZ4R1eCjkaxareY7JnsKrMof1amj819hbOcCMgyGfuRm74dLtW501kSwMiUwyd_6yYs6EeoeG38UVmQBiioRHQgCSMdDBu3Ej3hAwXvcIPMfCIhqvjQD7uD1OFecUTJx40xk_o_Yiy-uMA5XNsHRBiPtZ3KnUphFw5BIxArUX3DDMPFpEySCHtPIb67btv3H-FXW8MvV_1HBkw3V-xlDt3lKggVN4WRq3IeS6cF0xFD25jpDSYA-jBKL6f_U0wprpd3918-ldVEE77DysLlD7EnhIn_KErSpWi9qYWnsvqftvC3R-4iJUkehhUjtUiefJrA0uEJsZoSEeevzM_0jwtMY2Rb4LCzroTIYLDwRc4n45ILr13Rev_ATTsSx6ccGzxhfRClVaHrBRY7udek7EXIPJAx8m8MkyQCfKScX4GoWQqmPhn3hNd0LuW44r5xpO-ljrOzsBndeswe0gtwM53ns9ObCMznjoJo3DeGKFxKxsIUxmoSRaLP4bpvz7YgZEIqvrS8Mt1ZxHEXG98_meWlYyPd3A2ir7G7ZLQcxiIBm7-IKms2rmapPc3u5bM69KhjQNR93HYRxCXGzQnIhR8HYXzj9P1_6INOw3UVkBdIYX-jEFsNhkzHHTEIqrmDW_76xQ_wSrr_dOh3Oi8Ri12DXaXksMFBvbJhUGmgETi0QKfqd8EI-ReT3BbZVChbAIJ1RNUe-wd7guT8mKdpxTicWK0KpMnGaU3DL8tlY9GZwpFyRDmJRHwBP7iqVjU4M4jF2KyHwdL589yyxlheVn-fmItOag0vPxkysf4pX0qL52WNP3ch15tN2D8LePFp4S__rQwXD-vdsciDDXBH3DOeLeKWR4-h6lVkYxi6XtHVbLgOE26lurqe1Q0XQQcJYA1usDNdJUNYwuW1qYGiB7VH6PDYSrSHuVj2YB3bmsGxpxOf2vxrhhJZzIhKU3Yxy-BX-tXp0AK8C4oowhu66feVTx6dIl_d3xQlOinsq5kEbsxvb9aSSKPkuNkfFLTbksJhW2-QWHnv4Bwtf7uC6zMLxVfocy-iCZAMTkGa9icvrHJeT3htPvpTQYbHq_kOeTVZUj3Hw-LRcaUQnxQfOTLVWmGXBAyWhWr28l5zblWxYxGMXMP&isca=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b19e03-d39e-40e9-be2e-4aa2ec427834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Duration: 5634.0\n"
     ]
    }
   ],
   "source": [
    "#video_url = \"https://storage.cloud.google.com/raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\"\n",
    "video_url=\"https://ffba87f3998dc58bc084d165a1de6442fbc82dae22183352ca8b1bc-apidata.googleusercontent.com/download/storage/v1/b/raw_nine_files/o/vlt_video_extract%2FMAAT%2FMAAT2024_1_A_HBB.mp4?jk=AdvOr8stvY3hRXD9psznNOnR68vfY46qQuSRwB7uO7udZdeU2LGGz-_54PmfT-yobQGxdyq1OJd1dnjB-KHuYc64Ky3hZ45dZPi3LocJ9kOpF2E7EAfzzbh1tnu855pcL63SAw-jIOe4AKQdfRQcXxO-KhuhEkUxHY9jthAzHPVv7wYH5zPsrKLuKIZzlfqJ-kh2KYLi-HlC9SDa9FtEPZbHXSuJZvlUXZK9exW_8Qnxlo5EtNLP386be6siB0rndlMgdKeHPTcO3r7RpXPlfeFm6hQuWDpe-vAW6wRz-BJzlnXOc1-cdXQ7_2Hz85JrdQ6SO-7Sf9fG7Nhk-wSLWGf_9hzaD4AaaVqhCJqmojJGQ1NID5iLry10QGX6r2Ce23Rp0P0dicWb0ePc6BygOVOPM2NZZWyHQO0OJB63aaXsz5MAFTA7dU0oQsz36-d4NzwcEiIfb4Sv86r5uowgeS_pBBOJrv0Y3NlmF1BJ_Eq5bN-q10S0wJ040NMQNs-fWfDKSSCBd-gWovKWk16r4DLSOvt10-EVfNWs8qHEnI9VJB_3E03XxhrONQxX2VqQoCnPMYcAob1lbfezAefR9dpy6xoXvcoB_GajqVoBBLlF6lk8cX69YxsL_gdSzNVJIbRMwVYfdipLKhXxiipUJM3GuB-OlD9OwKjpbYK2hGCxN0GR_pY5mrRREka8FAsxMpZvmEXEgqwYKwMPRCHmtfRj4uAyubVUPP-WCBEQQ_0r8VG1IBFqhLLrM4rqrWwxM5Vejjo6NPS6QP2Va5sOfptdcyLqTXpaNBesHkrO_hyC4ClOtwkhx1-x8rTPHat8VkUHUcPGHug2aVHp8l-zzTs-puLcF4N24hfkD06MVqbUWslkKHXTUiEbRiIOgg854ObQ976NsZI8GPeDjIIbCYzCZyDGUR9lYcJ98oQlopCEtPZsKrwUE-nV40WvQ2ABXGrW3szWrOlaOPQDwmwCTTSgWvlyxWlqI8txdIV3B1pccav3bkOiqlz7e2BcBc6rxaxu_P4Xy9_ZBGggkFO7dIiBizs0O6kkWoFib5985-bXbEvXNdeClyhuTSG9JML5-EiQqMvfm0_uW69Y9LtAUw9QwlMg6AP9nZytgw1aPBTDJlU6nZHM4ARrS7-Tv9QV3eUIGANS466_mAPP-M5wR3sWIy8fq7LhUCWmVxOqCt-iMgiqP6ZgyyRd6AgY1wAk-lshrySf1uDZV5xgeQrp8WhfNS19HNMexHSqreqPYIs&isca=1\"\n",
    "try:\n",
    "    clip = VideoFileClip(video_url)\n",
    "    print(\"Video Duration:\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba45b01-3702-4224-a881-cedccbd6daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from moviepy.editor import VideoFileClip\n",
    "import io\n",
    "\n",
    " \n",
    "# Initialize the storage client\n",
    "client = storage.Client( )\n",
    "\n",
    "# Set your bucket and file names\n",
    "bucket_name = 'raw_nine_files'\n",
    "object_name = 'vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4'\n",
    "\n",
    "# Create a file-like object from GCS\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(object_name)\n",
    "\n",
    "# Download the blob as a byte stream\n",
    "video_stream = io.BytesIO()\n",
    "blob.download_to_file(video_stream)\n",
    "video_stream.seek(0)  # Reset stream position to the beginning\n",
    "\n",
    "# Use moviepy to get the video length from the stream\n",
    "try:\n",
    "    clip = VideoFileClip(video_stream)\n",
    "    print(\"Video Duration (seconds):\", clip.duration)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2da52ee7-4a76-4d33-81bf-81e624583f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rows have been added.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore,BigQueryLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "    \n",
    "def chunk_and_load(project_id: str= None, dataset: str= None, table: str= None, region: str =None,\\\n",
    "                        metadata_columns: list[str]=None, page_content_columns: list[str]= None, \\\n",
    "                        source_query_str: str= None, separators:  list[str]=None, chunk_size: int=None, \\\n",
    "                       chunk_overlap: int=0):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and generates\n",
    "        text embeddings for them.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "             \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        source_query_str, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "   \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"process_time\"]=now\n",
    "        if prev==split.metadata[\"id\"]:\n",
    "           split.metadata[\"chunk\"] = chunk_idx      \n",
    "        else:\n",
    "            chunk_idx=0\n",
    "            split.metadata[\"chunk\"] = chunk_idx\n",
    "            prev=split.metadata[\"id\"]\n",
    "        chunk_idx +=1\n",
    "\n",
    "        rows_to_insert.append(\n",
    "                           {\"id\": split.metadata[\"id\"], \n",
    "                               \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                               \"content\":  split.page_content,\n",
    "                               \"chunk\": split.metadata[\"chunk\"]\n",
    "                              }\n",
    "                                     )\n",
    "    logging.info (f\"Metadata added to chunks\")\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset}.{table}\"\n",
    "\n",
    " \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors: {errors}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logging.info (f\"Bigquery store info is set -  ProjectID {project_id}, Region {region}, Dataset {dataset}, Table {table}\")\n",
    "\n",
    "\n",
    "#     _=bq_store.add_documents(doc_splits)\n",
    "    \n",
    "#     logging.info (f\"Chunks and embeddings added to the store\")\n",
    "\n",
    "    return 'done'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id= os.environ.get(\"PROJECT_ID\") \n",
    "    dataset= os.environ.get(\"DATASET\")  \n",
    "    table= os.environ.get(\"TABLE\") \n",
    "    region= os.environ.get(\"REGION\") \n",
    "    metadata_columns= str(os.environ.get(\"META_DATA_COLUMNS\")).split(',') \n",
    "    page_content_columns= str(os.environ.get(\"PAGE_CONTENT_COLUMNS\")).split(',') \n",
    "    source_query_str= os.environ.get(\"SOURCE_QUERY_STR\") \n",
    "    separators= None if str(os.environ.get(\"SEPARATORS\"))==\"\" else str(os.environ.get(\"SEPARATORS\")).split(',') \n",
    "    chunk_size= 1000 if str(os.environ.get(\"CHUNK_SIZE\"))==\"None\" else int(str(os.environ.get(\"CHUNK_SIZE\")))  \n",
    "    chunk_overlap= 0 if str(os.environ.get(\"CHUNK_OVERLAP\"))==\"None\" else int(str(os.environ.get(\"CHUNK_OVERLAP\")))\n",
    " \n",
    " \n",
    "\n",
    "    project_id= 'nine-quality-test' \n",
    "    dataset= 'my_langchain_dataset'\n",
    "    table= 'doc_and_vectors'\n",
    "    region= 'us-central1'\n",
    "    metadata_columns= \"id\".split(\",\")\n",
    "    page_content_columns= \"content,media_type,test_metadata\".split(',') \n",
    "    source_query_str= \"\"\"\n",
    "    SELECT id,media_type,content,test_metadata \n",
    "    FROM `nine-quality-test.Nine_Quality_Test.content_embeddings` ;\n",
    "    \"\"\"\n",
    "    separators= \"\\\\n\\\\n\"\n",
    "    chunk_size= 25\n",
    "    chunk_overlap= 5\n",
    "    \n",
    "    \n",
    "    message=chunk_and_load(project_id=project_id, dataset=dataset, table=table, region=region,\\\n",
    "                        metadata_columns= metadata_columns, page_content_columns=page_content_columns, \\\n",
    "                        source_query_str=source_query_str, separators=separators, chunk_size=chunk_size, \\\n",
    "                       chunk_overlap=chunk_overlap)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "ff0ec9a2-e279-4f9e-bd31-64294f9d6a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "       \n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Truncate the table if exist\n",
    "        query = f\"TRUNCATE TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' truncated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error truncating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "        #add partitioning\n",
    "        # table.time_partitioning = bigquery.TimePartitioning(\n",
    "        #     type_=bigquery.TimePartitioningType.DAY,\n",
    "        #     field=\"request_id\",  # This field will be used for partitioning\n",
    "        # )\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5bc05941-78d7-45ae-bc59-77841f2b803f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''\n",
    " \n",
    "    try:\n",
    "        project_id=  request_args['project_id']\n",
    "        dataset_id=  request_args['dataset']\n",
    "        table= request_args['table']\n",
    "        region= request_args['region']\n",
    "        metadata_columns= str(request_args['metadata_columns']).split(',') \n",
    "        page_content_columns= str(request_args['page_content_columns']).split(',') \n",
    "        source_query_str= request_args['source_query_str']\n",
    "        separators= None if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "        chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "        chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap']))  \n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= 'chunked_data'\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [ col.strip() for col in  \"id,content,path,media_type,test_metadata\".split(\",\")]\n",
    "            page_content_columns= [col.strip() for col in \"content,test_metadata\".split(',') ]\n",
    "            source_query_str= \"\"\"\n",
    "            SELECT id,media_type,content,test_metadata, path\n",
    "            FROM `nine-quality-test.Nine_Quality_Test.test_long_artcles` ;\n",
    "            \"\"\"\n",
    "            separators= \"\\n\"\n",
    "            chunk_size= 500\n",
    "            chunk_overlap= 10\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "\n",
    "\n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "\n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"id\"]\n",
    "    rows_to_insert=[]\n",
    "    request_date=datetime.today().strftime('%Y_%m_%d')\n",
    "    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=30000 #maximum number of requests in a batch\n",
    " \n",
    "    max_index=2\n",
    "    record_count=0\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"id\"]\n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = request_date+'_'+str(version)\n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"id\": split.metadata[\"id\"], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\":  split.page_content,\n",
    "                                   \"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   \"media_type\": split.metadata[\"media_type\"],\n",
    "                                   \"path\": split.metadata[\"path\"],\n",
    "                                   \"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{dataset_id}_{request_id}\"\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]\n",
    "                print(job.result())\n",
    "    \n",
    "    return {'status':'SUCCESS', 'record_count':record_count, 'count_of_tables':version+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6bc54925-7ff7-47a9-863f-8bc6f142cd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BigQueryLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchunk_bq_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[176], line 39\u001b[0m, in \u001b[0;36mchunk_bq_content\u001b[0;34m(request_args)\u001b[0m\n\u001b[1;32m     33\u001b[0m         chunk_overlap\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m#return {'record_count':0, 'status':'ERROR- Set required input parameters'}\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m  \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mBigQueryLoader\u001b[49m(\n\u001b[1;32m     40\u001b[0m     query\u001b[38;5;241m=\u001b[39msource_query_str, project\u001b[38;5;241m=\u001b[39mproject_id, metadata_columns\u001b[38;5;241m=\u001b[39mmetadata_columns, page_content_columns\u001b[38;5;241m=\u001b[39mpage_content_columns\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m documents\u001b[38;5;241m.\u001b[39mextend(loader\u001b[38;5;241m.\u001b[39mload())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BigQueryLoader' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_bq_content('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90c12451-fb58-4edd-9d53-8dde1eaa087c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320504668865953792\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "import datetime\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "\n",
    " \n",
    "system_instruction = \"\"\"\n",
    "You are an expert researcher. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at these research papers, and answer the following questions.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(days=2),\n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)\n",
    "# Example response:\n",
    "# 1234567890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df716674-c8cd-4880-91f2-df9ff80f4963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cf3d063-c54f-42ed-ace5-ada7a184ec39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/494586852359/locations/us-central1/cachedContents/1320504668865953792',\n",
       " 'model': 'projects/nine-quality-test/locations/us-central1/publishers/google/models/gemini-1.5-pro-002',\n",
       " 'createTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'updateTime': '2024-10-10T00:05:18.148425Z',\n",
       " 'expireTime': '2024-10-12T00:05:18.129147Z',\n",
       " 'displayName': 'example-cache'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_content.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40be2737-2aaa-4a80-9b25-cd49ee49a3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'createTime': 'llm_response.body.createTime',\n",
       "  'expireTime': 'llm_response.body.expireTime',\n",
       "  'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4',\n",
       "  'model': 'llm_response.body.model',\n",
       "  'name': 'llm_response.body.name',\n",
       "  'updateTime': 'llm_response.body.updateTime',\n",
       "  'usageMetadata': 'llm_response.body.usageMetadata'},\n",
       " {'createTime': 'llm_response.body.createTime',\n",
       "  'expireTime': 'llm_response.body.expireTime',\n",
       "  'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4',\n",
       "  'model': 'llm_response.body.model',\n",
       "  'name': 'llm_response.body.name',\n",
       "  'updateTime': 'llm_response.body.updateTime',\n",
       "  'usageMetadata': 'llm_response.body.usageMetadata'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= data['items']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1db7b32-ad04-468a-809b-96daefab45b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l=list(filter(lambda element: element['gcs_uri'] == 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b0cc048-04d5-499e-b4e8-966969bc39b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][\"gcs_uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0db5932-f361-467b-b6b2-9ef846dad71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2 - Copy.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n",
      "{'createTime': 'llm_response.body.createTime', 'expireTime': 'llm_response.body.expireTime', 'gcs_uri': 'gs://raw_nine_files/vlt_video_extract/OTHERS/splitProcessOutput_vlt_video_extract_MAAT_Full_MAAT2023_10_A_HBB_360p_split2.mp4', 'model': 'llm_response.body.model', 'name': 'llm_response.body.name', 'updateTime': 'llm_response.body.updateTime', 'usageMetadata': 'llm_response.body.usageMetadata'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-10-12T23:45:42.158262Z'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import the Google Cloud client library and JSON library\n",
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('raw_nine_files')\n",
    "blob = bucket.blob('test111.json')\n",
    "\n",
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for x in data['items']:\n",
    "    print(x)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "data[\"cache_expiry_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76829048-51c8-4427-8ca1-475e0553df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "data = json.loads(blob.download_as_string(client=None))\n",
    "for item in enumerate( data.items):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae3296-e97c-46ae-8ad2-2647a3c92c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": {\n",
    "                                            \"contents\": {\n",
    "                                                \"parts\": [\n",
    "                                                    {\n",
    "                                                        \"fileData\": {\n",
    "                                                            \"fileUri\": \"gs://raw_nine_files/60MI23_33_A_HBB.mp4\",\n",
    "                                                            \"mimeType\": \"video/mp4\"\n",
    "                                                        },\n",
    "                                                        \"videoMetadata\": {\n",
    "                                                            \"endOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 240\n",
    "                                                            },\n",
    "                                                            \"startOffset\": {\n",
    "                                                                \"nanos\": 0,\n",
    "                                                                \"seconds\": 120\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                        \"text\": \"You are an assistant tasked with describing videos for retrieval.These descriptions will be embedded and used to retrieve the raw video.\\n Chapterize the video content by grouping the video content into chapters with intervals of 120 seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\nIf there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\n Describe important scenes in the video concisely. If you are not sure about any info, please do not make it up. \\n Only consider video from 120 seconds to 240 seconds. \\nIf it is the last chapter, set the endOffset to 240 instead.\\n For result, follow JSON schema.<JSONSchema>{\\\"description\\\":\\\"A list of chapters\\\",\\\"items\\\":{\\\"properties\\\":{\\\"Content\\\":{\\\"type\\\":\\\"string\\\"},\\\"endOffset\\\":{\\\"type\\\":\\\"integer\\\"},\\\"startOffset\\\":{\\\"type\\\":\\\"integer\\\"}},\\\"required\\\":[\\\"startOffset\\\",\\\"endOffset\\\",\\\"Content\\\"],\\\"type\\\":\\\"object\\\"},\\\"type\\\":\\\"array\\\"}</JSONSchema>\"\n",
    "                                                    }\n",
    "                                                ],\n",
    "                                                \"role\": \"user\"\n",
    "                                            },\n",
    "                                            \"generation_config\": {\n",
    "                                                \"max_output_tokens\": 2048,\n",
    "                                                \"temperature\": 0.5,\n",
    "                                                \"top_k\": 40,\n",
    "                                                \"top_p\": 0.8\n",
    "                                            },\n",
    "                                            \"safety_settings\": [\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        }\n",
    "                                    }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcba9a1d-5832-4e71-83bc-7b13d46c491a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7373483305540255744\n",
      "<vertexai.caching._caching.CachedContent object at 0x7fcf8c5ed3c0>: {\n",
      "  \"name\": \"projects/494586852359/locations/us-central1/cachedContents/7373483305540255744\",\n",
      "  \"model\": \"projects/nine-quality-test/locations/us-central1/publishers/google/models/gemini-1.5-pro-002\",\n",
      "  \"createTime\": \"2024-10-16T05:34:44.833168Z\",\n",
      "  \"updateTime\": \"2024-10-16T05:34:44.833168Z\",\n",
      "  \"expireTime\": \"2024-10-16T07:34:07.699361Z\"\n",
      "}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(l)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m---> 16\u001b[0m cached_content \u001b[38;5;241m=\u001b[39m \u001b[43mcaching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCachedContent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_content_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#cached_content.delete()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/caching/_caching.py:141\u001b[0m, in \u001b[0;36mCachedContent.__init__\u001b[0;34m(self, cached_content_name)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cached_content_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Represents a cached content resource.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    This resource can be used with vertexai.generative_models.GenerativeModel\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m            \"456\".\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_content_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gca_resource(cached_content_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:552\u001b[0m, in \u001b[0;36mVertexAiResourceNoun.__init__\u001b[0;34m(self, project, location, credentials, resource_name)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes class with project, location, and api_client.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    resource_name(str): A fully-qualified resource name or ID.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resource_name:\n\u001b[0;32m--> 552\u001b[0m     project, location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_validate_project_location\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject \u001b[38;5;241m=\u001b[39m project \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mproject\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation \u001b[38;5;241m=\u001b[39m location \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mlocation\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:654\u001b[0m, in \u001b[0;36mVertexAiResourceNoun._get_and_validate_project_location\u001b[0;34m(self, resource_name, project, location)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_validate_project_location\u001b[39m(\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    638\u001b[0m     resource_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    639\u001b[0m     project: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    640\u001b[0m     location: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    641\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    643\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate the project and location for the resource.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m        RuntimeError: If location is different from resource location\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m     fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_resource_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fields:\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m project, location\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:613\u001b[0m, in \u001b[0;36mVertexAiResourceNoun._parse_resource_name\u001b[0;34m(cls, resource_name)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03mParses resource name into its component segments.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    Dictionary of component segments.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m# gets the underlying wrapped gapic client class\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gapic_client_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_resource_name_method\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1beta1/services/gen_ai_cache_service/client.py:212\u001b[0m, in \u001b[0;36mGenAiCacheServiceClient.parse_cached_content_path\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_cached_content_path\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parses a cached_content path into its component segments.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^projects/(?P<project>.+?)/locations/(?P<location>.+?)/cachedContents/(?P<cached_content>.+?)$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroupdict() \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/re.py:190\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "# cache_id = \"your-cache-id\"\n",
    " \n",
    "# cached_content = caching.CachedContent(cached_content_name=cache_id)\n",
    "# cached_content.delete()\n",
    "\n",
    "for x in caching.CachedContent.list():\n",
    "     l=x.name.split(\"/\")[-1]\n",
    "     print(l)\n",
    "     print(x)\n",
    "     cached_content = caching.CachedContent(cached_content_name=x)\n",
    "     #cached_content.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46dedaab-b53c-4f9b-9e17-8037bf254d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VideoRequest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_video_duration\u001b[39m(request: \u001b[43mVideoRequest\u001b[49m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" generates video length\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m   \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m          \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     fs \u001b[38;5;241m=\u001b[39m gcsfs\u001b[38;5;241m.\u001b[39mGCSFileSystem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VideoRequest' is not defined"
     ]
    }
   ],
   "source": [
    "def get_video_duration(request: VideoRequest):\n",
    "    \"\"\" generates video length\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int 1: as SUCCESS\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7b74fb8-6815-4022-bdfc-bd13da79aaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import gcsfs\n",
    "from pymediainfo import MediaInfo\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the input data model\n",
    "class VideoRequest(BaseModel):\n",
    "    url: str  # Input: video URL\n",
    "\n",
    "# Define the output data model\n",
    "class VideoResponse(BaseModel):\n",
    "    duration: float  # Output: Duration in seconds\n",
    "\n",
    "@app.post(\"/get-video-duration\", response_model=VideoResponse)\n",
    "async def get_video_duration(request: VideoRequest):\n",
    "    \"\"\"\n",
    "    get video duration of a given gcs url\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    # Open the file stream using gcsfs\n",
    "    with fs.open(gcsuri, 'rb') as video_file:\n",
    "              # Use pymediainfo to extract metadata directly from the stream\n",
    "              media_info = MediaInfo.parse(video_file)\n",
    "              for track in media_info.tracks:\n",
    "                  if track.track_type == 'Video':\n",
    "                      duration= track.duration / 1000  # Convert ms to seconds\n",
    "                      print(duration)\n",
    "                      break\n",
    " \n",
    "\n",
    "    return VideoResponse(duration=duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c1998d85-7777-4975-8454-6ebe00317d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "                      \n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None, video_metadata_file: str=\"\"):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for video and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str file_pre_fix: prefix of the destination file name\n",
    "            list requests: list of requests\n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "    \n",
    "\n",
    "    # Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    blob = bucket.blob(video_metadata_file)\n",
    "\n",
    "    # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "    data = json.loads(blob.download_as_string(client=None))\n",
    "    video_metadata=data['items']\n",
    "    \n",
    "\n",
    "    segments_to_process=120 #segments duration\n",
    "    intervals=120#intervals\n",
    "    video_start =0 #where from video to start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for blob in blobs:                         \n",
    "        if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         metadata=list(filter(lambda element: element['gcs_uri'] ==gcsuri, video_metadata))[0]\n",
    "                         video_duration=metadata[\"videoOriginalDurationSecond\"]\n",
    "                         cache_id=metadata[\"name\"]\n",
    "                         prev=video_start\n",
    "                         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                                offset={'start':prev, 'end':val}\n",
    "                                \n",
    "                                startOffset=offset['start']\n",
    "                                endOffset=offset['end']\n",
    "                                if endOffset>=video_duration:\n",
    "                                     endOffset=video_duration\n",
    "                                print(offset)\n",
    "                                prev=val \n",
    "                                segment_prompt= f\"Only consider video from {startOffset} seconds to {endOffset} seconds. Ignore analyzing the rest of video.\\n\" \n",
    "                                if index==0:\n",
    "                                    request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                                    rf= open(request_file.name, \"a\") \n",
    "                                \n",
    "                                 \n",
    "                                request_list=[\n",
    "                                       json.dumps(\n",
    "                                              {\n",
    "                                                \"request\": \n",
    "                                                     {\n",
    "                                                     \"cached_content\": cache_id,\n",
    "                                                      \"contents\":  {\n",
    "                                                                        \"parts\": [\n",
    "                                                                            {\n",
    "                                                                            \"fileData\":  {\"fileUri\": gcsuri, \"mimeType\": mimeType},\n",
    "                                                                            \"videoMetadata\": {\n",
    "                                                                                \"endOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": endOffset\n",
    "                                                                                },\n",
    "                                                                                \"startOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": startOffset\n",
    "                                                                                }\n",
    "                                                                            }\n",
    "\n",
    "                                                                            },\n",
    "                                                                            {\n",
    "                                                                            \"text\": prompt_text +\"\\n\"+ segment_prompt\n",
    "                                                                            } \n",
    "\n",
    "                                                                        ],\n",
    "                                                                        \"role\": \"user\"\n",
    "                                                                        }\n",
    "                                                          , \n",
    "                                                          \"generation_config\": \n",
    "                                                               {\"max_output_tokens\": max_output_tokens, \n",
    "                                                                \"temperature\":temperature, \n",
    "                                                                 \"top_k\": top_k, \n",
    "                                                                 \"top_p\": top_p\n",
    "                                                                }\n",
    "                                                          , \n",
    "                                                          \"safety_settings\": \n",
    "                                                           [\n",
    "                                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                             \"threshold\": \"BLOCK_NONE\"\n",
    "                                                             }\n",
    "                                                           , \n",
    "                                                           {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           },\n",
    "                                                           {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           }, \n",
    "                                                           {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                            \"threshold\": \"BLOCK_NONE\"\n",
    "                                                            }\n",
    "\n",
    "\n",
    "                                                           ]\n",
    "                                                     }\n",
    "                                              }\n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "                                rf.writelines(request_list)\n",
    "                                rf.flush()\n",
    "\n",
    "                                if index==(max_index-1):\n",
    "                                        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                        rf.close() \n",
    "                                        versions.append(version)                               \n",
    "                                        index=0\n",
    "                                        version +=1\n",
    "                                        request_list=[]\n",
    "                                        rf=None\n",
    "\n",
    "                                else:                               \n",
    "                                        index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "@functions_framework.http\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "#     request_json = request.get_json(silent=True)\n",
    "#     request_args = request.args\n",
    "\n",
    "#     dest_bucket_name =request_args['destination_bucket']\n",
    "#     source_bucket_name =request_args['source_bucket']\n",
    "#     source_folder_name=request_args['source_folder']\n",
    "#     request_file_prefix =request_args['request_file_prefix']\n",
    "#     request_file_folder =request_args['request_file_folder']\n",
    "#     prompt_text= request_args['prompt_text']\n",
    "#     media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    " \n",
    "#     request_content= request_args['request_content']\n",
    "\n",
    "#     if request_args and 'temperature' in request_args:\n",
    "#         temperature= request_args['temperature']\n",
    "#     else:\n",
    "#       temperature=0.5\n",
    "\n",
    "#     if request_args and 'max_output_tokens' in request_args:\n",
    "#        max_output_tokens= request_args['max_output_tokens'] \n",
    "#     else:\n",
    "#          max_output_tokens=2048\n",
    "\n",
    "#     if request_args and 'top_p' in request_args:\n",
    "#         top_p= request_args['top_p']\n",
    "#     else:\n",
    "#          top_p=0.5\n",
    "\n",
    "#     if request_args and 'top_k' in request_args:\n",
    "#         top_k= request_args['top_k']\n",
    "#     else:\n",
    "#          top_k=50\n",
    "\n",
    "#     if request_args and 'max_request_per_file' in request_args:\n",
    "#         max_request_per_file= request_args['max_request_per_file']\n",
    "#     else:\n",
    "#       max_request_per_file=30000\n",
    "    \n",
    "    dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "    source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "    request_file_prefix = \"video_request\"#request_args['request_file_prefix']\n",
    "    request_file_folder =  \"video_batch_request_fldr\"#request_args['request_file_folder']\n",
    "    prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "    media_types= ['video/mp4']#list(request_args['media_type'])\n",
    "    request_content= \"video\"#request_args['request_content']\n",
    "    source_folder_name=\"vlt_video_extract/OTHERS\"\n",
    "    temperature=0.5\n",
    "    max_output_tokens=2048\n",
    "    top_p=50\n",
    "    top_k=0.5\n",
    "    max_request_per_file=30000\n",
    "    video_metadata_file=\"vlt_video_metadata_fldr/vlt_video_metadata.json\"\n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file\n",
    "                                      )  \n",
    "    if  request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file,\n",
    "                                     video_metadata_file=video_metadata_file\n",
    "                                      ) \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "28e800a6-f02c-49ef-a0fd-9c91330edbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 120}\n",
      "{'start': 120, 'end': 240}\n",
      "{'start': 240, 'end': 360}\n",
      "{'start': 360, 'end': 480}\n",
      "{'start': 480, 'end': 600}\n",
      "{'start': 600, 'end': 720}\n",
      "{'start': 720, 'end': 840}\n",
      "{'start': 840, 'end': 960}\n",
      "{'start': 960, 'end': 1080}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 1}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db570928-ad71-4027-8cc3-13db077eef24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)                              \n",
    "\n",
    "def create_video_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None, video_metadata_file: str=\"\"):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for video and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str source_bucket_name: name of the source  gcs bucket to read files from\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str source_folder_name: name of the source folder name to read files from\n",
    "            str request_file_prefix: prefix of the request file name\n",
    "            list mime_types: list of accepted mime_types\n",
    "            str prompt_text: prompt for Gimini\n",
    "            float temperature: Gimini temprature\n",
    "            float top_p: Gimini top_p\n",
    "            float top_k: Gimini top_k\n",
    "            int max_request_per_file: max number of requests per batch\n",
    "            int max_output_tokens: Gimini max_output_tokens          \n",
    "            str video_metadata_file: name of video metadata\n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"       \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]    \n",
    "\n",
    "    # Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    blob = bucket.blob(video_metadata_file)\n",
    "\n",
    "    # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "    data = json.loads(blob.download_as_string(client=None))\n",
    "    video_metadata=data['items']\n",
    "    \n",
    "\n",
    "    segments_to_process=120 #segments duration\n",
    "    intervals=120#intervals\n",
    "    video_start =0 #where from video to start\n",
    "\n",
    "    for blob in blobs:                         \n",
    "        if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         metadata=list(filter(lambda element: element['gcs_uri'] ==gcsuri, video_metadata))[0]\n",
    "                         video_duration=metadata[\"videoOriginalDurationSecond\"]\n",
    "                         cache_id=metadata[\"name\"]\n",
    "\n",
    "                         prev=video_start\n",
    "                         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                                offset={'start':prev, 'end':val}\n",
    "                                \n",
    "                                startOffset=offset['start']\n",
    "                                endOffset=offset['end']\n",
    "                                if endOffset>=video_duration:\n",
    "                                     endOffset=video_duration\n",
    "                                print(offset)\n",
    "                                prev=val \n",
    "                                segment_prompt= \"Only consider video from\" + str(startOffset)+\" seconds to \"+ str(endOffset)+\" seconds. Ignore analyzing the rest of video.\\n\" \n",
    "                                if index==0:\n",
    "                                    request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                                    rf= open(request_file.name, \"a\") \n",
    "                                \n",
    "                                 \n",
    "                                request_list=[\n",
    "                                       json.dumps(\n",
    "                                              {\n",
    "                                                \"request\": \n",
    "                                                     {\n",
    "                                                      \"cached_content\": cache_id,\n",
    "                                                      \"contents\":  {\n",
    "                                                                        \"parts\": [\n",
    "                                                                            {\n",
    "                                                                            \"fileData\":  {\"fileUri\": gcsuri, \"mimeType\": mimeType},\n",
    "                                                                            \"videoMetadata\": {\n",
    "                                                                                \"endOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": endOffset\n",
    "                                                                                },\n",
    "                                                                                \"startOffset\": {\n",
    "                                                                                \"nanos\": 0,\n",
    "                                                                                \"seconds\": startOffset\n",
    "                                                                                }\n",
    "                                                                            }\n",
    "\n",
    "                                                                            },\n",
    "                                                                            {\n",
    "                                                                            \"text\": prompt_text +\"\\n\"+ segment_prompt\n",
    "                                                                            } \n",
    "\n",
    "                                                                        ],\n",
    "                                                                        \"role\": \"user\"\n",
    "                                                                        }\n",
    "                                                          , \n",
    "                                                          \"generation_config\": \n",
    "                                                               {\"max_output_tokens\": max_output_tokens, \n",
    "                                                                \"temperature\":temperature, \n",
    "                                                                 \"top_k\": top_k, \n",
    "                                                                 \"top_p\": top_p\n",
    "                                                                }\n",
    "                                                          , \n",
    "                                                          \"safety_settings\": \n",
    "                                                           [\n",
    "                                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                             \"threshold\": \"BLOCK_NONE\"\n",
    "                                                             }\n",
    "                                                           , \n",
    "                                                           {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           },\n",
    "                                                           {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                           \"threshold\": \"BLOCK_NONE\"\n",
    "                                                           }, \n",
    "                                                           {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                            \"threshold\": \"BLOCK_NONE\"\n",
    "                                                            }\n",
    "\n",
    "\n",
    "                                                           ]\n",
    "                                                     }\n",
    "                                              }\n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "                                rf.writelines(request_list)\n",
    "                                rf.flush()\n",
    "\n",
    "                                if index==(max_index-1):\n",
    "                                        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                        rf.close() \n",
    "                                        versions.append(version)                               \n",
    "                                        index=0\n",
    "                                        version +=1\n",
    "                                        request_list=[]\n",
    "                                        rf=None\n",
    "\n",
    "                                else:                               \n",
    "                                        index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "def create_image_request_file( dest_bucket_name: str= None, source_bucket_name: str= None, source_folder_name: str=None,\n",
    "                          request_file_prefix: str= None, request_file_folder: str= None, mime_types: list[str]= None,\n",
    "                          prompt_text: str=\"\", temperature: float= None,\n",
    "                          max_output_tokens: int=2048, top_p: float= None, top_k : float= None, max_request_per_file: int =None):\n",
    "\n",
    "    \"\"\"create batch request  file(s) of up to 30000 for videos and store it in gcs\n",
    "   \n",
    "        Args:\n",
    "            str dest_bucket_name: name of the destination gcs bucket to write files to\n",
    "            str source_bucket_name: name of the source  gcs bucket to read files from\n",
    "            str dest_folder_name: name of the destination folder name to write files to\n",
    "            str source_folder_name: name of the source folder name to read files from\n",
    "            str request_file_prefix: prefix of the request file name\n",
    "            list mime_types: list of accepted mime_types\n",
    "            str prompt_text: prompt for Gimini\n",
    "            float temperature: Gimini temprature\n",
    "            float top_p: Gimini top_p\n",
    "            float top_k: Gimini top_k\n",
    "            int max_request_per_file: max number of requests per batch\n",
    "            int max_output_tokens: Gimini max_output_tokens          \n",
    "\n",
    "        Returns:\n",
    "            int : number of generated files\n",
    "          \n",
    "    \"\"\"           \n",
    "         \n",
    "\n",
    "    # Initialize a client for interacting with GCS\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket by name\n",
    "    bucket = storage_client.get_bucket(source_bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder_name)  \n",
    "    version=0\n",
    "    index=0     \n",
    "    max_index=max_request_per_file\n",
    "\n",
    "    now=datetime.strptime(str(datetime.now()),\n",
    "                               '%Y-%m-%d %H:%M:%S.%f')\n",
    "    now=datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    versions=[]\n",
    "\n",
    "    for blob in blobs:                         \n",
    "                    if blob.content_type in mime_types:                            \n",
    "                         gcsuri= \"gs://\"+source_bucket_name+\"/\"+blob.name\n",
    "                         mimeType=blob.content_type\n",
    "                         if index==0:\n",
    "                            request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "                            rf= open(request_file.name, \"a\") \n",
    "                 \n",
    "                         request_list=[\n",
    "                               json.dumps(\n",
    "                                      {\n",
    "                                        \"request\": \n",
    "                                             {\n",
    "                                              \"contents\": \n",
    "                                                  {\"parts\": [{ \"fileData\": \n",
    "                                                                 {\"fileUri\": gcsuri, \"mimeType\": mimeType}\n",
    "                                                              }, \n",
    "                                                              {\"text\": prompt_text\n",
    "                                                              }\n",
    "                                                            ]\n",
    "                                                              , \n",
    "                                                    \"role\": \"user\"\n",
    "                                                  }\n",
    "                                                  , \n",
    "                                                  \"generation_config\": \n",
    "                                                       {\"max_output_tokens\": max_output_tokens, \n",
    "                                                        \"temperature\":temperature, \n",
    "                                                         \"top_k\": top_k, \n",
    "                                                         \"top_p\": top_p\n",
    "                                                        }\n",
    "                                                  , \n",
    "                                                  \"safety_settings\": \n",
    "                                                   [\n",
    "                                                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \n",
    "                                                     \"threshold\": \"BLOCK_NONE\"\n",
    "                                                     }\n",
    "                                                   , \n",
    "                                                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   },\n",
    "                                                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "                                                   \"threshold\": \"BLOCK_NONE\"\n",
    "                                                   }, \n",
    "                                                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                                    \"threshold\": \"BLOCK_NONE\"\n",
    "                                                    }\n",
    "\n",
    "\n",
    "                                                   ]\n",
    "                                             }\n",
    "                                      }\n",
    "                               )  +\"\\n\"\n",
    "                         ]\n",
    "\n",
    "                         rf.writelines(request_list)\n",
    "                         rf.flush()\n",
    " \n",
    "                         if index==(max_index-1):\n",
    "                                upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version, request_file_post_fix=now)\n",
    "                                rf.close() \n",
    "                                versions.append(version)                               \n",
    "                                index=0\n",
    "                                version +=1\n",
    "                                request_list=[]\n",
    "                                rf=None\n",
    "\n",
    "                         else:                               \n",
    "                                index =index+1\n",
    "\n",
    "    if not rf is None: \n",
    "        upload_file(rf,dest_bucket_name=dest_bucket_name,request_file_folder=request_file_folder,request_file_prefix=request_file_prefix,version=version,request_file_post_fix=now)\n",
    "        versions.append(version)   \n",
    " \n",
    "    return len(versions)\n",
    "\n",
    "@functions_framework.http\n",
    "def create_batch_request_file(request):\n",
    "    \"\"\"HTTP Cloud Function.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "        Response object using `make_response`\n",
    "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request_json = request.get_json(silent=True)\n",
    "        request_args = request.args\n",
    "\n",
    "        dest_bucket_name =request_args['destination_bucket']\n",
    "        source_bucket_name =request_args['source_bucket']\n",
    "        source_folder_name=request_args['source_folder']\n",
    "        request_file_prefix =request_args['request_file_prefix']\n",
    "        request_file_folder =request_args['request_file_folder']\n",
    "        prompt_text= request_args['prompt_text']\n",
    "        media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "\n",
    "        request_content= request_args['request_content']\n",
    "\n",
    "        if request_args and 'video_metadata_file' in request_args:\n",
    "            video_metadata_file= request_args['video_metadata_file']\n",
    "        else:\n",
    "          video_metadata_file=\"\"\n",
    "\n",
    "        if request_args and 'temperature' in request_args:\n",
    "            temperature= request_args['temperature']\n",
    "        else:\n",
    "          temperature=1\n",
    "\n",
    "        if request_args and 'max_output_tokens' in request_args:\n",
    "           max_output_tokens= request_args['max_output_tokens'] \n",
    "        else:\n",
    "             max_output_tokens=8192\n",
    "\n",
    "        if request_args and 'top_p' in request_args:\n",
    "            top_p= request_args['top_p']\n",
    "        else:\n",
    "             top_p=0.95\n",
    "\n",
    "        if request_args and 'top_k' in request_args:\n",
    "            top_k= request_args['top_k']\n",
    "        else:\n",
    "             top_k=40\n",
    "\n",
    "        if request_args and 'max_request_per_file' in request_args:\n",
    "            max_request_per_file= request_args['max_request_per_file']\n",
    "        else:\n",
    "          max_request_per_file=30000\n",
    "    except:\n",
    "            dest_bucket_name ='artifacts-nine-quality-test-embeddings' #request_args['destination_bucket']\n",
    "            source_bucket_name ='raw_nine_files'# request_args['source_bucket']\n",
    "            request_file_prefix = \"video_request\"#request_args['request_file_prefix']\n",
    "            request_file_folder =  \"video_batch_request_fldr\"#request_args['request_file_folder']\n",
    "            prompt_text= \"You are an assistant tasked with summarizing images for retrieval. \\\\ These summaries will be embedded and used to retrieve the raw image. \\\\ Give a concise summary of the image that is well optimized for retrieval.\\\\ If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\\ If you are not sure about any info, please do not make it up. Do not add any extra text to the output.\"#request_args['prompt_text']\n",
    "            media_types= ['video/mp4']#list(request_args['media_type'])\n",
    "            request_content= \"video\"#request_args['request_content']\n",
    "            source_folder_name=\"vlt_video_extract/OTHERS\"\n",
    "            temperature=0.5\n",
    "            max_output_tokens=2048\n",
    "            top_p=50\n",
    "            top_k=0.5\n",
    "            max_request_per_file=30000\n",
    "            video_metadata_file=\"vlt_video_metadata_fldr/vlt_video_metadata.json\"\n",
    "\n",
    " \n",
    "\n",
    "    versions=0\n",
    "    if  request_content=='image':\n",
    "      versions=create_image_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file\n",
    "                                      )  \n",
    "    if  request_content=='video':\n",
    "      versions=create_video_request_file(dest_bucket_name=dest_bucket_name,source_bucket_name=source_bucket_name,source_folder_name=source_folder_name,\n",
    "                                      request_file_prefix=request_file_prefix,request_file_folder=request_file_folder,\n",
    "                                      mime_types=media_types, prompt_text=prompt_text,temperature=temperature,\n",
    "                                     max_output_tokens=max_output_tokens,top_p=top_p,top_k=top_k, max_request_per_file=max_request_per_file,\n",
    "                                     video_metadata_file=video_metadata_file\n",
    "                                      ) \n",
    "\n",
    "    return {\"status\":\"SUCCESS\",\"file_count\":versions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b17d6b7-db52-48ef-b9b3-504fdeb11bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 120}\n",
      "{'start': 120, 'end': 240}\n",
      "{'start': 240, 'end': 360}\n",
      "{'start': 360, 'end': 480}\n",
      "{'start': 480, 'end': 600}\n",
      "{'start': 600, 'end': 720}\n",
      "{'start': 720, 'end': 840}\n",
      "{'start': 840, 'end': 960}\n",
      "{'start': 960, 'end': 1080}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'file_count': 1}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batch_request_file('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6f51d-1fc9-4b4a-ba40-1f6b196b8927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7323099284709048320\n"
     ]
    }
   ],
   "source": [
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    \n",
    "    display_name=\"example-cache\",\n",
    ")\n",
    "\n",
    "print(cached_content.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ae371a-c0db-4f4c-86f6-87c86ff0eb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first paper, \"Gemini: A Family of Highly Capable Multimodal Models\", introduces Gemini 1.0, Google's family of multimodal models. It details their architecture, training, capabilities, and responsible deployment.  Gemini 1.0 models come in different sizes (Ultra, Pro, Nano) for various applications.  The paper highlights Gemini Ultra's state-of-the-art performance on many benchmarks, exceeding human expert performance on MMLU and achieving strong results in multimodal reasoning (like MMMU) and coding tasks.  The paper emphasizes responsible AI development, outlining impact assessments, safety mitigations, and evaluations to address potential harms.\n",
      "\n",
      "The second paper, \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\", introduces Gemini 1.5 Pro, an evolution focusing on significantly expanded context length and efficiency.  It details the model's architecture as a mixture-of-experts model and the infrastructure advancements enabling its capabilities.  The paper emphasizes Gemini 1.5 Pro's near-perfect recall on long-context retrieval across modalities (text, video, audio) up to millions of tokens. It highlights improvements over Gemini 1.0 on core benchmarks while requiring less compute, and showcases surprising new capabilities like in-context language learning for low-resource languages with limited data.  Responsible development remains a focus, outlining updated impact assessment, model mitigation, and safety evaluations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from vertexai.preview import caching\n",
    "\n",
    "# TODO(developer): Update and un-comment below lines\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "# cache_id = \"your-cache-id\"\n",
    "\n",
    " \n",
    "cached_content = caching.CachedContent(cached_content_name=\"7323099284709048320\")\n",
    "\n",
    "model = GenerativeModel.from_cached_content(cached_content=cached_content)\n",
    "\n",
    "response = model.generate_content(\"What are the papers about?\")\n",
    "\n",
    "print(response.text)\n",
    "# Example response:\n",
    "# The provided text is about a new family of multimodal models called Gemini, developed by Google.\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc92a70a-f78b-40e1-84d0-82165a547135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GenerativeModel' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mdir\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GenerativeModel' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802f1da-660c-4c80-84b8-8e6eec3961fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " url: ${MM_LLM_ENDPOINT}\n",
    "                                auth:\n",
    "                                    type: OAuth2\n",
    "                                body:\n",
    "                                    \"instances\": [\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         mimeType:  \"${item.mime_type}\",\n",
    "                                                        \"gcsUri\": \"${item.gcs_uri}\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "                            result: llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "decc7737-767c-446c-8513-3fe5bf1fbb92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MM_MODEL= \"multimodalembedding@001\"\n",
    "MM_METHOD= \"predict\"\n",
    "PROJECT='nine-quality-test'\n",
    "LOCATION='us-central1'\n",
    "MM_LLM_ENDPOINT=\"https://\" + 'us-central1' + \"-aiplatform.googleapis.com\" + \"/v1/projects/\" + PROJECT + \"/locations/\" + 'us-central1' + \"/publishers/google/models/\" + MM_MODEL+\":\"+MM_METHOD\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65083e85-cf11-4364-9dab-e0056d461e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'code': 401, 'message': 'Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.', 'status': 'UNAUTHENTICATED', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'CREDENTIALS_MISSING', 'domain': 'googleapis.com', 'metadata': {'method': 'google.cloud.aiplatform.v1.PredictionService.Predict', 'service': 'aiplatform.googleapis.com'}}]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The API endpoint\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Data to be sent\n",
    "data = {\n",
    "     \"instances\": [\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         \"mimeType\":  \"image/png\",\n",
    "                                                        \"gcsUri\": \"gs://raw_nine_files/2023/1/a/816f62dz.png\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "}\n",
    "\n",
    "# A POST request to the API\n",
    "response = requests.post(MM_LLM_ENDPOINT, json=data)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08618712-9ba9-42c1-86e0-1dc6b9e3240c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "api_regional_endpoint= \"us-central1-aiplatform.googleapis.com\"\n",
    "client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "            client_options=client_options\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7f90ed3-d096-4851-b36e-0ca1197a46c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "instances=[\n",
    "                                                    {\n",
    "                                                        \"image\": {\n",
    "                                                         \"mimeType\":  \"image/png\",\n",
    "                                                        \"gcsUri\": \"gs://raw_nine_files/2023/1/a/816f62dz.png\"  \n",
    "                                                        }\n",
    "\n",
    "                                                     } ]\n",
    "\n",
    "endpoint = (\n",
    "           f\"projects/{PROJECT}/locations/{LOCATION}\"\n",
    "           \"/publishers/google/models/multimodalembedding@001\"\n",
    "        )\n",
    "response =client.predict(endpoint=endpoint, instances=instances)\n",
    "response.predictions[0].get(\"imageEmbedding\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07e10cda-9199-49c7-94a5-dd5a0a7f0815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_method= \"generateContent\"\n",
    "content_model= \"gemini-1.5-pro-002\"\n",
    "endpoint=(\"v1beta1/projects/\"  + PROJECT+\"/locations/\" + LOCATION + \"/publishers/google/models/\"+ content_model + \":\" + content_method)\n",
    " \n",
    "instances={\n",
    "                                       \n",
    "                                        \"contents\": [\n",
    "                                            {\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"parts\": [\n",
    "                                                {\n",
    "                                              \n",
    "                                                \n",
    "                                                \"fileData\": {\n",
    "                                                    \"mimeType\": \"video/mp4\",\n",
    "                                                    \"fileUri\": \"gs://raw_nine_files/vlt_video_extract/MAAT/MAAT2024_1_A_HBB.mp4\"\n",
    "                                                },\n",
    "\n",
    "                                                \"videoMetadata\": {\n",
    "                                                    \"startOffset\": {\n",
    "                                                    \"seconds\": 300,\n",
    "                                                    \"nanos\": 0\n",
    "                                                    },\n",
    "                                                    \"endOffset\": {\n",
    "                                                    \"seconds\": 520,\n",
    "                                                    \"nanos\": 0\n",
    "                                                    }\n",
    "                                                }\n",
    "                                                },\n",
    "                                                 { \"text\": \"describe what is happening in this video in the given time frame from offset 300 second to offset 520 second in details and what people are talking about.\"}\n",
    "                                            ]\n",
    "                                            }\n",
    "                                        ] ,\n",
    "                                        \n",
    "                                        \"safetySettings\": [\n",
    "                                            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "                                        ],\n",
    "                                        \"generationConfig\": {\n",
    "                                            \"temperature\": 1,\n",
    "                                            \"topP\": 0.5,\n",
    "                                            \"topK\": 40,\n",
    "                                           # \"candidateCount\": integer,\n",
    "                                            \"maxOutputTokens\": 8192,\n",
    "                                            # \"presencePenalty\": float,\n",
    "                                            # \"frequencyPenalty\": float,\n",
    "                                            # \"stopSequences\": [\n",
    "                                            # string\n",
    "                                            # ],\n",
    "                                            # \"responseMimeType\": string,\n",
    "                                            # \"responseSchema\": schema,\n",
    "                                            # \"seed\": integer,\n",
    "                                            # \"responseLogprobs\": boolean,\n",
    "                                            # \"logprobs\": integer,\n",
    "                                            # \"audioTimestamp\": boolean\n",
    "                                        }#,\n",
    "\n",
    "                                       # ,\"cachedContent\": \"projects/494586852359/locations/us-central1/cachedContents/4906355134671355904\"  \n",
    "                                       \n",
    "} \n",
    "\n",
    "from google.cloud import aiplatform\n",
    "api_regional_endpoint= \"us-central1-aiplatform.googleapis.com\"\n",
    "client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "            client_options=client_options\n",
    "        )\n",
    "\n",
    "import google.cloud.aiplatform.v1beta1  as endpoint       \n",
    "#response =client.generate_content(instances)\n",
    "#response =client.predict(endpoint=endpoint, instances=[instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9890de0e-b31c-4020-90e9-ce20b6429589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClassificationPredictionResult',\n",
       " 'ImageObjectDetectionPredictionResult',\n",
       " 'ImageSegmentationPredictionResult',\n",
       " 'TabularClassificationPredictionResult',\n",
       " 'TabularRegressionPredictionResult',\n",
       " 'TextExtractionPredictionResult',\n",
       " 'TextSentimentPredictionResult',\n",
       " 'TimeSeriesForecastingPredictionResult',\n",
       " 'VideoActionRecognitionPredictionResult',\n",
       " 'VideoClassificationPredictionResult',\n",
       " 'VideoObjectTrackingPredictionResult',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'gapic_version',\n",
       " 'package_version',\n",
       " 'types']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dir(endpoint.schema.predict.prediction_v1beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62c24684-9d19-4409-904c-04afbd84b578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "creds, project = google.auth.default()\n",
    "\n",
    "# creds.valid is False, and creds.token is None\n",
    "# Need to refresh credentials to populate those\n",
    "\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b08ca94e-7e65-4a6c-a02e-a3c5f289137f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m video_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2885\u001b[39m\u001b[38;5;66;03m#math.ceil(get_video_duration(video_file))\u001b[39;00m\n\u001b[1;32m     54\u001b[0m video_chapters\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 55\u001b[0m prev\u001b[38;5;241m=\u001b[39m\u001b[43mvideo_start\u001b[49m\n\u001b[1;32m     56\u001b[0m log\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (segments_to_process,video_duration\u001b[38;5;241m+\u001b[39msegments_to_process,segments_to_process):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_start' is not defined"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import time\n",
    "import typing\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "#libraries to generate image summaries\n",
    "from vertexai.vision_models import Video\n",
    "from vertexai.vision_models import VideoSegmentConfig\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part as GenerativeModelPart,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from typing import Any, Dict, List, Literal, Optional, Union\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    " \n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import initializer as aiplatform_initializer\n",
    "import datetime\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "if 1==1:\n",
    "         generative_multimodal_model= GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "         #generation_config= GenerationConfig(temperature=1, top_k=40,top_p=0.95,max_output_tokens=8192) \n",
    "         generation_config=GenerationConfig(temperature=1, top_k=40,top_p=0.95,max_output_tokens=8192)#, response_mime_type='application/json',\n",
    "\t     # response_schema=json.loads(schema))  \n",
    "         #for video, BLOCK_NONE gives error. So, have to set it to BLOCK_ONLY_HIGH\n",
    "         safety_settings=  {\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    }\n",
    "         stream=False\n",
    "        \n",
    "         video_duration=2885#math.ceil(get_video_duration(video_file))\n",
    "         \n",
    "         video_chapters=[]\n",
    "         prev=0\n",
    "         log=[]\n",
    "        segments_to_process=120\n",
    "         \n",
    "         for val in range (segments_to_process,video_duration+segments_to_process,segments_to_process):\n",
    "                offset={'start':prev, 'end':val}\n",
    "                prev=val    \n",
    "                print(offset)\n",
    "                startOffset=offset['start']\n",
    "                endOffset=offset['end']\n",
    "                chapters=[]  \n",
    "                \n",
    "                if endOffset>=video_duration:\n",
    "                    endOffset=video_duration\n",
    "                    \n",
    "                video_description_prompt=f\"\"\"You are an assistant tasked with summarizing videos for retrieval.\\\n",
    "                     These summaries will be embedded and used to retrieve the raw video.\\\n",
    "                    Chapterize the video content by grouping the video content into chapters \\\n",
    "                    with intervals of {intervals} seconds and providing a concise detail for each chapter that is well optimized for retrieval.\\\n",
    "                    If there is a famous person like politician, celebrity or athlete, indicate their name and describe what they are famous for.\\\n",
    "                    Describe important scenes in the video concisely.\\\n",
    "                    If you are not sure about any info, please do not make it up. \\\n",
    "                    Only consider video from {startOffset} seconds to {endOffset} seconds. Ignore analyzing the rest of video.\\\n",
    "                    If it is the last chapter, set the endOffset to {endOffset} instead.\\ \n",
    "                    If a chapter includes prohibited content, set chapterSummary to \"\".\\\n",
    "                    For result, follow JSON schema.<JSONSchema>{json.dumps(schema)}</JSONSchema>\"\n",
    "                    \"\"\"   \n",
    " \n",
    "        \n",
    "                contents=[parts=[GenerativeModelPart.from_uri(video_file,mime_type=\"video/mp4\", ),\n",
    "                           video_description_prompt,]        \n",
    "\n",
    "                model_response = generative_multimodal_model.generate_content(\n",
    "                                    contents,\n",
    "                                    generation_config=generation_config,\n",
    "                                    stream=stream,\n",
    "                                    safety_settings=safety_settings, )        \n",
    "\n",
    "                response_list = []\n",
    "                    # print(model_response)\n",
    "\n",
    "                print('Called the API, processing the result now...')\n",
    "                prohibited_content=False\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1f66746-d055-4eb5-9d03-f446a35c41b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate_content in module vertexai.generative_models._generative_models:\n",
      "\n",
      "generate_content(contents: Union[List[ForwardRef('Content')], List[Dict[str, Any]], str, ForwardRef('Image'), ForwardRef('Part'), List[Union[str, ForwardRef('Image'), ForwardRef('Part')]]], *, generation_config: Union[ForwardRef('GenerationConfig'), Dict[str, Any], NoneType] = None, safety_settings: Union[List[ForwardRef('SafetySetting')], Dict[google.cloud.aiplatform_v1beta1.types.content.HarmCategory, google.cloud.aiplatform_v1beta1.types.content.SafetySetting.HarmBlockThreshold], NoneType] = None, tools: Optional[List[ForwardRef('Tool')]] = None, tool_config: Optional[ForwardRef('ToolConfig')] = None, stream: bool = False) -> Union[ForwardRef('GenerationResponse'), Iterable[ForwardRef('GenerationResponse')]] method of vertexai.preview.generative_models.GenerativeModel instance\n",
      "    Generates content.\n",
      "    \n",
      "    Args:\n",
      "        contents: Contents to send to the model.\n",
      "            Supports either a list of Content objects (passing a multi-turn conversation)\n",
      "            or a value that can be converted to a single Content object (passing a single message).\n",
      "            Supports\n",
      "            * str, Image, Part,\n",
      "            * List[Union[str, Image, Part]],\n",
      "            * List[Content]\n",
      "        generation_config: Parameters for the generation.\n",
      "        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n",
      "        tools: A list of tools (functions) that the model can try calling.\n",
      "        tool_config: Config shared for all tools provided in the request.\n",
      "        stream: Whether to stream the response.\n",
      "    \n",
      "    Returns:\n",
      "        A single GenerationResponse object if stream == False\n",
      "        A stream of GenerationResponse objects if stream == True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(generative_multimodal_model.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b5c78a9-56c4-4ba2-8f9f-654c35ba45bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: ya29.c.c0ASRK0GazjrHy8T5oxOjgzmUAvpcPg6TrD2uCutKDaCN3dLuPEdjFLtvwDqGKVErmiHO-4iC92m7j-813QysLmaxrDYuXiRI07YF4Fb4YEJx5d8ignk4uyiWgPd7WZnO9CqjQ8HxSB7ZEqZEFuz_WmtVXeSGLyccCDAmgokqzj8CbwV5o0q2Cv2A6R9SMfoLcuYUL7HzUK5QZZLIwb14-cpwKzJzXj-l8UyxGm2Qz2hlhczUKMl--WAIkOmGODPKDCOpdSajsuGRbrKpw13MKrDuZl2HITFRE0nfF1pwClYlqTgt2H_V8OtIN8YYsvNAl-Z3f0bimuUDiA095RSnPipdSqFCN3ECwJrHa-iOmXU9SZhcvQJF7Bg_rrR0ANFRmzKCjuIDly_mHBtgqjjRVH413AshIg25-IsyVXvcg3zxUBkFyJdWI8OwZOSlt8y3Rx95FZsJ8cS7SinQOo2z5wX8Xl832FYb7Jlf1jf-jOUhgraMIk6qget7VmyiJ1me6RV82FMt2mqgx5hrSzQ0o34_03zFvwe2ZpQv0r78JBjZpg9wflsJd6ZqkQqFui8207ozoqRzStc4YU9157lzs3pagOxhV3Ju1OMp32ZROwc97hBys2tSj4ZRiFecXqFYIejMYX399dlZ-qkOrdlS0yqyk3iX3jcIOUox5-rqrOXbB8qh6hrUc4p9uv_WgvMUpYUqii85-4VmXU_czOZ7RYfYoFR6Xm73ZQrMVac3BVkSv73O5vMOrboZFazhcuzJx8c8v458i8nb336m9fdp5BXma4RpiUXVrUQcg7fi0YYJgRv6wcdd78wVBrVbxtMbr0QgZ5ItIBqprY7yv8t6Uc4kcQcMdv7MykUrI_1ObV-5Z9_QgFnt7Z9ouSBnyjyOypetS1hgunf6rU9eQY6U57f16mI-wlhqvJke__mwgkxWoyF0pml9kfrYttXyJZprSb3_wmy1pn95BW-35cF4Rdj9b_M3kBQB9coso3wx_sXYdWtslM7bdZRg\n"
     ]
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "try:\n",
    "    creds = GoogleCredentials.get_application_default()\n",
    "except Exception as e:\n",
    "    quit(e)\n",
    "\n",
    "print(\"Access Token:\", creds.get_access_token().access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aaf3493-2c3c-47be-9166-c64580d2a54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "from google.cloud import storage\n",
    "\n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"idx\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"mime_type\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"gcs_uri\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"media_name\", \"STRING\", mode=\"REQUIRED\")\n",
    "       \n",
    "    ]     \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "\n",
    "@functions_framework.http\n",
    "def get_gcs_info(request):\n",
    "    \"\"\"\n",
    "        loads gcs metadata info into big query table\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "    #request_args = request.args\n",
    " \n",
    "\n",
    "#     project_id=  request_args['project_id']\n",
    "#     dataset_id=  request_args['dataset']\n",
    "#     table= request_args['table']\n",
    "#     region= request_args['region']\n",
    "#     source_bucket= request_args['source_bucket']\n",
    "#     source_folder= request_args['source_folder']\n",
    "#     media_types= [media.strip() for media in  str(request_args['media_types']).strip().replace(\"[\",''). replace(']','').replace(\"'\",'').split(',')]\n",
    "\n",
    "            \n",
    "    project_id=  \"nine-quality-test\"\n",
    "    dataset_id= 'vlt_metadata'\n",
    "    table= \"vlt_image_metadata\"\n",
    "    region= \"us-central1\"\n",
    "    source_bucket= 'raw_nine_files'\n",
    "    source_folder= \"2024\"\n",
    "    media_types= ['image/png']\n",
    "    \n",
    "            \n",
    "\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(source_bucket)\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs(prefix=source_folder)\n",
    "    \n",
    "    # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    now = datetime.now()  \n",
    "    rows_to_insert=[]        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "\n",
    "\n",
    "    job_list=[]\n",
    "    idx=0\n",
    "    for blob in blobs:\n",
    "        if blob.content_type in media_types:\n",
    "            rows_to_insert.append(\n",
    "                                {   \"idx\":  idx  , \n",
    "                                    \"name\": blob.name, \n",
    "                                    \"mime_type\":blob.content_type,\n",
    "                                    \"gcs_uri\":  \"gs://\"+source_bucket+\"/\"+blob.name,\n",
    "                                    \"media_name\":blob.name.split(\"/\")[-1].replace(\".\"+blob.content_type.split(\"/\")[-1],\"\")\n",
    "                                    }\n",
    "                                            )\n",
    "        \n",
    "            idx=idx+1\n",
    "            \n",
    "    print(rows_to_insert)\n",
    "    #create table new if does not exist\n",
    "    table=f\"{table}\" \n",
    "    table_schema=create_table(project_id,dataset_id,table)\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema = table_schema\n",
    "    job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)                \n",
    "    job_list.append(job.job_id)\n",
    "  \n",
    "    return {'status':'SUCCESS', 'record_count':idx,'count_of_tables':1,'table_name_prefix':table,'jobs':job_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f20504-2cf7-4f75-a87c-5762526f0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nine-quality-test\n",
      "Dataset 'vlt_metadata' already exists.\n",
      "[{'idx': 0, 'name': '2024/3/d/screenshot_directrycontent.png', 'mime_type': 'image/png', 'gcs_uri': 'gs://raw_nine_files/2024/3/d/screenshot_directrycontent.png', 'media_name': 'screenshot_directrycontent'}]\n",
      "Table 'nine-quality-test.vlt_metadata.vlt_image_metadata' created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS',\n",
       " 'record_count': 1,\n",
       " 'count_of_tables': 1,\n",
       " 'table_name_prefix': TableReference(DatasetReference('nine-quality-test', 'vlt_metadata'), 'vlt_image_metadata'),\n",
       " 'jobs': ['acdf507c-f282-4d3f-bce7-89aad5c12d4d']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gcs_info('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99233f49-0aa6-4804-90c6-926596200397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtable_schema\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table_schema' is not defined"
     ]
    }
   ],
   "source": [
    "table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ff1c98b0-8cfd-46a2-be07-271b944c01a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE MODEL IF NOT EXISTS `vlt_media_multimodal_embeddings_prelanding.test`\n",
    "REMOTE WITH CONNECTION `us-central1.vlt_multimodal_endpoint`\n",
    "OPTIONS(endpoint = 'multimodalembedding@001') ;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49499f00-1668-47e0-a53b-50eb7c98e5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1852933557.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    CREATE OR REPLACE TABLE `bqml_tutorial.met_image_embeddings`\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "855c1a0f-cf8f-4f2c-9224-6c4d21ef4f23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE EXTERNAL TABLE `vlt_metadata.met_images`\n",
    "WITH CONNECTION `us-central1.vlt_multimodal_endpoint`\n",
    "OPTIONS\n",
    "  ( object_metadata = 'SIMPLE',\n",
    "    uris = ['gs://raw_nine_files/vlt_video_extract/OTHERS/*']\n",
    "    )\n",
    "\"\"\"\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a236e34f-ddc6-482b-bd15-60a29072fd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings`\n",
    "AS\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.GENERATE_EMBEDDING(\n",
    "    MODEL `vlt_metadata.multimodal`,\n",
    "    (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT 1000));\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.query(create_model_sql)\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb5bb44c-6dda-445b-9c47-13aa382ceb32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your SQL CREATE MODEL statement\n",
    "create_model_sql = \"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings1`\n",
    "AS\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.GENERATE_EMBEDDING(\n",
    "    MODEL `vlt_metadata.multimodal`,\n",
    "    (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT 1000));\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Execute the CREATE MODEL statement\n",
    "query_job = client.create_job( job_config={\n",
    "        \"query\": {\n",
    "            \"query\": create_model_sql,\n",
    "        },\n",
    "        \"labels\": {\"example-label\": \"example-value\"},\n",
    "        \"maximum_bytes_billed\": 10000000,\n",
    "    })\n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result()\n",
    "\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed19a49-a4c3-42f5-9a68-7c506bde87d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bd806266-d3e0-4264-b7f1-737ece17ae8d'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_job.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7a2be05-5af6-4375-9795-3bd3e0ad5267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Initialize the loop variables\n",
    "offset = 0\n",
    "idx=0\n",
    "more_results = True\n",
    "job_list=[]\n",
    "row_count=1\n",
    "while more_results:\n",
    "    # Define your SQL CREATE MODEL statement\n",
    "    generate_mm_embedding_sql = f\"\"\"CREATE OR REPLACE TABLE `vlt_metadata.met_image_embeddings_{idx}`\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM\n",
    "      ML.GENERATE_EMBEDDING(\n",
    "        MODEL `vlt_metadata.multimodal`,\n",
    "        (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images` LIMIT {batch_size} OFFSET {offset}));\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the CREATE MODEL statement\n",
    "    job = client.create_job( job_config={\n",
    "            \"query\": {\n",
    "                \"query\": generate_mm_embedding_sql,\n",
    "            },\n",
    "            #\"labels\": {\"example-label\": \"example-value\"},\n",
    "            \"maximum_bytes_billed\": 10000000,\n",
    "        })\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job_list.append(job.job_id)\n",
    " \n",
    "    # Update offset for the next batch\n",
    "    offset += batch_size\n",
    "    idx =idx+1\n",
    "        \n",
    "    # Check if there are any results\n",
    "    if offset>row_count:\n",
    "        more_results = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bd66a-53a6-4487-b072-b76c7f52ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM ML.GENERATE_EMBEDDING(\n",
    "  MODEL `PROJECT_ID.DATASET_ID.MODEL_NAME`,\n",
    "  TABLE PROJECT_ID.DATASET_ID.TABLE_NAME,\n",
    "  STRUCT(TRUE AS flatten_json_output,\n",
    "    START_SECOND AS start_second,\n",
    "    END_SECOND AS end_second,\n",
    "    INTERVAL_SECONDS AS interval_seconds)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a1282-a56f-415e-af3a-99f2c4a981da",
   "metadata": {},
   "outputs": [],
   "source": [
    "${query+ \" SELECT *  FROM  ML.GENERATE_EMBEDDING ( MODEL\" +\"`\"+ METADATA_DATASET+\".\"+MODEL_NAME+\"`,\"+\n",
    "                                                    \" ( SELECT EVM.*  FROM `\"+METADATA_DATASET+\".\"+METADATA_TABLE+\"` EVM \"+\n",
    "                                                       \" INNER JOIN `\"+METADATA_DATASET+\".\"+VIDEO_METADATA_TABLE+\"` VM \"+\n",
    "                                                        \" ON EVM.uri=VM.gcs_uri \"+\n",
    "                                                        \" WHERE VW.duration>=\" + string(startSegment) +                                                    \n",
    "                                                        \" LIMIT \" +string(limit) +\" OFFSET \"+ string(offset) +\")),\"+\n",
    "                                                         \" STRUCT(TRUE AS flatten_json_output,\"+\n",
    "                                                            string(startSegment)+ \"  AS start_second,\"+\n",
    "                                                            string(endSegment)+ \" AS end_second,\"+\n",
    "                                                            string(INTERVALS) + \" AS interval_seconds)\"\n",
    "                                                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08ec5e-dfc0-49dc-b1aa-f4d1fc24ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    # Define your SQL CREATE MODEL statement\n",
    "    generate_mm_embedding_sql = f\"\"\"CREATE OR REPLACE TABLE `vlt_media_multimodal_embeddings_prelanding.vlt_video_multimodal_embeddings_test`\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM\n",
    "      ML.GENERATE_EMBEDDING(\n",
    "        MODEL `vlt_metadata.multimodal`,\n",
    "        (SELECT  * FROM `nine-quality-test.vlt_metadata.met_images`\n",
    "         \n",
    "        \n",
    "        \n",
    "        LIMIT {batch_size} OFFSET {offset}));\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the CREATE MODEL statement\n",
    "    job = client.create_job( job_config={\n",
    "            \"query\": {\n",
    "                \"query\": generate_mm_embedding_sql,\n",
    "            },\n",
    "            #\"labels\": {\"example-label\": \"example-value\"},\n",
    "            \"maximum_bytes_billed\": 10000000,\n",
    "        })\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job_list.append(job.job_id)\n",
    " \n",
    "    # Update offset for the next batch\n",
    "    offset += batch_size\n",
    "    idx =idx+1\n",
    "        \n",
    "    # Check if there are any results\n",
    "    if offset>row_count:\n",
    "        more_results = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "742390ce-a138-4e55-9497-594eb38c85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document loaded.....\n",
      "document splitted.....\n",
      "nine-quality-test\n",
      "Dataset 'langchain_dataset' already exists.\n",
      "chunk done\n",
      "chucked_content_data_2024_10_24T014648231552Z_0\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0' created successfully.\n",
      "DONE\n",
      "Job 050f7e12-6091-4a84-a4d8-e069292d94ad completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_0\n",
      "chucked_content_data_2024_10_24T014648231552Z_1\n",
      "Table 'nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_1' created successfully.\n",
      "DONE\n",
      "Job 90253b51-df5f-4d35-a93a-10f6d77c6925 completed successfully. For nine-quality-test.langchain_dataset.chucked_content_data_2024_10_24T014648231552Z_1\n"
     ]
    }
   ],
   "source": [
    "import functions_framework\n",
    "\n",
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "\n",
    "\n",
    "def create_log_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a log table to keep the track of chunkings \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table. \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"asset_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema \n",
    "    \n",
    "    \n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"asset_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    "  \n",
    "   \n",
    "  \n",
    "\n",
    "        \n",
    "    try:\n",
    "            project_id=  request_args['project_id']\n",
    "            dataset_id=  request_args['dataset']\n",
    "            table= request_args['table']\n",
    "            region= request_args['region']\n",
    "            metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "            page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "            source_query_str= request_args['source_query_str']\n",
    "            #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "            chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "            chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "            max_prompt_count_limit=30000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "\n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= \"chucked_content_data_2024_10_24T014648231552Z\"\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [\"asset_id\"]\n",
    "            page_content_columns= [\"HeadLine\",\"content\"]\n",
    "            source_query_str= \"\"\"\n",
    "           SELECT asset_id, headline as HeadLine, plain_text_column as Content  FROM `nine-quality-test.vlt_media_content_prelanding.vlt_article_content` ;\n",
    "            \"\"\"\n",
    "   \n",
    "            chunk_size= 1000\n",
    "            chunk_overlap= 100\n",
    "            max_prompt_count_limit=25000\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "             \n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print('document loaded.....')\n",
    "   # print(documents)\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "    \n",
    "    print('document splitted.....')\n",
    "\n",
    "    # # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    # now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"asset_id\"]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d')    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    print('chunk done')\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[\"asset_id\"]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx  \n",
    "           \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[\"asset_id\"]\n",
    "                      \n",
    "             \n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = prefix+'_'+str(version)\n",
    "            #print(split.page_content)#.replace(page_content_columns[0]+\":\", \"\", 1).strip())\n",
    "            # print(chunk_idx)\n",
    "            # print()\n",
    "            if chunk_idx==1:                \n",
    "                 content=split.page_content  \n",
    "            else:                  \n",
    "                  content=page_content_columns[1]+\": \"+split.page_content \n",
    "                \n",
    "           # print(content)\n",
    "    \n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"asset_id\": split.metadata[\"asset_id\"], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\": content,#.replace(page_content_columns[0]+\":\", \"\", 1).strip(),\n",
    "                                   #\"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   #\"media_type\": split.metadata[\"media_type\"],\n",
    "                                   #\"path\": split.metadata[\"path\"],\n",
    "                                   #\"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:\n",
    "                \n",
    "                create_load_job()\n",
    "               \n",
    "                #create table new if does not exist\n",
    "                table=f\"{prefix}_{version}\"\n",
    "                print(table)\n",
    "                table_schema=create_table(project_id,dataset_id,table)\n",
    "                #push the data into the table\n",
    "                table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "                dataset  = client.dataset(dataset_id)\n",
    "                table = dataset.table(table)\n",
    "                job_config = bigquery.LoadJobConfig()\n",
    "                job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "                job_config.schema = table_schema\n",
    "                job = client.load_table_from_json(rows_to_insert, table, job_config = job_config)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]              \n",
    "                #wait for the job to finish\n",
    "                job.result()\n",
    "                # Get job status \n",
    "                \n",
    "                print(job.state)\n",
    "                \n",
    "                if job.state == 'DONE':\n",
    "                    if job.error_result:\n",
    "                        print(f\"Job {job.job_id} for {table_id} failed with error: {job.error_result}\")\n",
    "                        job_execution_result['error_result']=job.error_result\n",
    "                        raise Exception(\"Sorry, no numbers below zero\")\n",
    "                    else:\n",
    "                        print(f\"Job {job.job_id} completed successfully. For \"+ table_id)\n",
    "                else:\n",
    "                    print(f\"Job {job.job_id} for {table_id} is still in progress.\")\n",
    "                \n",
    "                job_list.append(job_execution_result)\n",
    "                \n",
    "    if len(rows_to_insert)>0:\n",
    "        \n",
    "  \n",
    "    return   {'status':'SUCCESS', 'record_count':record_count,'count_of_tables':version+1 }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "\n",
    "        request_args={}\n",
    "    \n",
    "        if  'project_id' in os.environ:\n",
    "            request_args['project_id']= os.environ.get('project_id')\n",
    "            \n",
    "        if  'dataset' in os.environ:\n",
    "            request_args['dataset']= os.environ.get('dataset')\n",
    "            \n",
    "        if  'table' in os.environ:\n",
    "            request_args['table']= os.environ.get('table')\n",
    "            \n",
    "        if  'region' in os.environ:\n",
    "            request_args['region']= os.environ.get('region')\n",
    "            \n",
    "        if  'metadata_columns' in os.environ:\n",
    "            request_args['metadata_columns']= os.environ.get('metadata_columns')\n",
    "            \n",
    "        if  'page_content_columns' in os.environ:\n",
    "            request_args['page_content_columns']= os.environ.get('page_content_columns')\n",
    "            \n",
    "        if  'source_query_str' in os.environ:\n",
    "            request_args['source_query_str']= os.environ.get('source_query_str')\n",
    "            \n",
    "        if  'chunk_size' in os.environ:\n",
    "            request_args['chunk_size']= os.environ.get('chunk_size')\n",
    "            \n",
    "        if  'chunk_overlap' in os.environ:\n",
    "            request_args['chunk_overlap']= os.environ.get('chunk_overlap')\n",
    "            \n",
    "        if  'max_prompt_count_limit' in os.environ:\n",
    "            request_args['max_prompt_count_limit']= os.environ.get('max_prompt_count_limit')\n",
    "        \n",
    "        chunk_bq_content(request_args)\n",
    "            \n",
    "          \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c1e1b6b5-dc20-4dd1-9130-e629afefce24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document loaded.....\n",
      "document splitted.....\n",
      "nine-quality-test\n",
      "Dataset 'langchain_dataset' already exists.\n",
      "chunk done\n"
     ]
    }
   ],
   "source": [
    "if 1==1:\n",
    "    \n",
    "    status=''  \n",
    "  \n",
    "   \n",
    "  \n",
    "\n",
    "        \n",
    "    try:\n",
    "            project_id=  request_args['project_id']\n",
    "            dataset_id=  request_args['dataset']\n",
    "            table= request_args['table']\n",
    "            region= request_args['region']\n",
    "            metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "            page_content_columns= [col.strip() for col in str(request_args['page_content_columns']).split(',') ]\n",
    "            source_query_str= request_args['source_query_str']\n",
    "            #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "            chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "            chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "            max_prompt_count_limit=30000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "\n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= \"chucked_content_data_2024_10_24T014648231552Z\"\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [\"asset_id\"]\n",
    "            page_content_columns= [\"HeadLine\",\"Content\"]\n",
    "            source_query_str= \"\"\"\n",
    "           SELECT asset_id, headline as HeadLine, plain_text_column as Content  FROM `nine-quality-test.vlt_media_content_prelanding.vlt_article_content` ;\n",
    "            \"\"\"\n",
    "   \n",
    "            chunk_size= 1000\n",
    "            chunk_overlap= 100\n",
    "            max_prompt_count_limit=25000\n",
    "            #return {'record_count':0, 'status':'ERROR- Set required input parameters'}\n",
    "             \n",
    "\n",
    "     # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print('document loaded.....')\n",
    "   # print(documents)\n",
    "    \n",
    "    logging.info (f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)    \n",
    "     \n",
    "    logging.info (f\"Documents splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\")\n",
    "    \n",
    "    print('document splitted.....')\n",
    "\n",
    "    # # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    # now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[\"asset_id\"]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d')    \n",
    "        \n",
    "    client = bigquery.Client(project_id)    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    print('chunk done')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "f859bfd5-552f-446a-a8c4-df8596306fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62210"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c13ab960-3d64-460c-8c98-c3bf5d886a31",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "   \n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"combined_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "def load_into_bq(table_prefix: str,project_id: str,dataset_id: str, region: str, data: list,  version: int):  \n",
    "    \"\"\"\n",
    "        Create a job to load data into big query table and wait for the job to finish \n",
    "        \n",
    "        Args:\n",
    "           str table_prefix: the prefix of biquery table name\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           list(json) data: list of json records that should be inserted into table.\n",
    "     \n",
    "             \n",
    "    \"\"\"    \n",
    "    client = bigquery.Client(project_id)    \n",
    "   \n",
    "    \n",
    "    #create table new if does not exist\n",
    "    table=f\"{table_prefix}_{version}\"\n",
    "    table_schema=create_table(project_id,dataset_id,table)\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema = table_schema\n",
    "    job = client.load_table_from_json(data, table, job_config = job_config)\n",
    "              \n",
    "    #wait for the job to finish\n",
    "    job.result()\n",
    "    # Get job status   \n",
    "                \n",
    "    if job.state == 'DONE':\n",
    "        if job.error_result:\n",
    "             print(f\"Job {job.job_id} for {table_id} failed with error: {job.error_result}\")\n",
    "             raise Exception(\"Sorry, no numbers below zero\")\n",
    "        else:\n",
    "             print(f\"Job {job.job_id} completed successfully. For \"+ table_id)\n",
    "    else:\n",
    "            print(f\"Job {job.job_id} for {table_id} is still in progress.\")\n",
    "                \n",
    "\n",
    "def load_and_split_docs (source_query_str: str,project_id: str, metadata_columns: list[str], page_content_columns: list[str], chunk_size: int,chunk_overlap: int ):\n",
    "    \n",
    "      # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print(f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)   \n",
    "    \n",
    "    return doc_splits\n",
    "    \n",
    "\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''  \n",
    " \n",
    " \n",
    "\n",
    "    try:\n",
    "            project_id=  request_args['project_id']\n",
    "            dataset_id=  request_args['dataset']\n",
    "            table= request_args['table']\n",
    "            region= request_args['region']\n",
    "            metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "            article_page_content_columns= [col.strip() for col in str(request_args['article_page_content_columns']).split(',') ]\n",
    "            video_page_content_columns= [col.strip() for col in str(request_args['video_page_content_columns']).split(',') ]\n",
    "            image_page_content_columns= [col.strip() for col in str(request_args['image_page_content_columns']).split(',') ]\n",
    "            article_source_query_str= request_args['article_source_query_str']\n",
    "            video_source_query_str= request_args['video_source_query_str']\n",
    "            image_source_query_str= request_args['image_source_query_str']\n",
    "            #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "            chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "            chunk_overlap= 0 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "            max_prompt_count_limit=30000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "\n",
    "\n",
    "    except:        \n",
    "            project_id= 'nine-quality-test' \n",
    "            dataset_id= 'langchain_dataset'\n",
    "            table= \"chucked_content_data_2024_10_24T014648231552Z\"\n",
    "            region= 'us-central1'\n",
    "            metadata_columns= [\"combined_id\"]            \n",
    "            article_page_content_columns= [col.strip() for col in str(\"HeadLine, Content\").split(',') ]\n",
    "            video_page_content_columns= [col.strip() for col in str( \"Content\").split(',') ]\n",
    "            image_page_content_columns= [col.strip() for col in str( \"Content\").split(',') ]\n",
    "            article_source_query_str= \"SELECT  'article' as asset_type, combined_id ,headline as HeadLine, description as Content   FROM `nine-quality-test.vlt_media_content_prelanding.vlt_combined_media_content` WHERE LOWER(asset_type) LIKE '%article%' ;\" \n",
    "            video_source_query_str= \"SELECT 'video' as asset_type, combined_id  , description as Content   FROM `nine-quality-test.vlt_media_content_prelanding.vlt_combined_media_content` WHERE LOWER(asset_type) LIKE '%video%' and 1=0 ;\" \n",
    "            image_source_query_str=  \"SELECT 'image' as asset_type,combined_id ,  description as Content   FROM `nine-quality-test.vlt_media_content_prelanding.vlt_combined_media_content` WHERE LOWER(asset_type) LIKE '%image%' and 1=0 ;\"    \n",
    "              \n",
    "            chunk_size= 1000\n",
    "            chunk_overlap= 100\n",
    "            max_prompt_count_limit=25000\n",
    "     \n",
    "   \n",
    "    doc_splits= load_and_split_docs(article_source_query_str,project_id, metadata_columns, article_page_content_columns, chunk_size,chunk_overlap )        \n",
    "    print(f\"Article contents loaded and splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\\n\")\n",
    " \n",
    "    doc_splits= doc_splits+ load_and_split_docs(video_source_query_str,project_id, metadata_columns, video_page_content_columns, chunk_size,chunk_overlap )        \n",
    "    print(f\"Video contents loaded and splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\\n\")\n",
    "   \n",
    "    doc_splits= doc_splits+ load_and_split_docs(image_source_query_str,project_id, metadata_columns, image_page_content_columns, chunk_size,chunk_overlap )        \n",
    "    print(f\"Image contents loaded and splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\\n\")\n",
    "\n",
    "    print(len(doc_splits))\n",
    "    # # get current processing time to add it to metadata, datetime object containing current date and time\n",
    "    # now = datetime.now()\n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[metadata_columns[0]]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d') \n",
    "    now = datetime.now()    \n",
    "    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "   \n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[metadata_columns[0]]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[metadata_columns[0]]\n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = prefix+'_'+str(version)\n",
    "            \n",
    "            if chunk_idx==1:                \n",
    "                 content=split.page_content  \n",
    "            else: \n",
    "              \n",
    "                  content=\"Content\"+\": \"+split.page_content \n",
    "\n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"combined_id\": split.metadata[metadata_columns[0]], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\": content,#.replace(page_content_columns[0]+\":\", \"\", 1).strip(),\n",
    "                                   #\"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   #\"media_type\": split.metadata[\"media_type\"],\n",
    "                                   #\"path\": split.metadata[\"path\"],\n",
    "                                   #\"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:  \n",
    "                load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]   \n",
    "               \n",
    "                \n",
    "    if  len(rows_to_insert)>0:       \n",
    "        load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "        #moving to next batch\n",
    "        record_count=record_count+len(rows_to_insert)\n",
    "        rows_to_insert=[]   \n",
    "                \n",
    "    \n",
    "  \n",
    "    return   {'status':'SUCCESS', 'record_count':record_count,'count_of_tables':version+1 }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "\n",
    "        request_args={}\n",
    "        if  'project_id' in os.environ:\n",
    "            request_args['project_id']= os.environ.get('project_id')\n",
    "            \n",
    "        if  'dataset' in os.environ:\n",
    "            request_args['dataset']= os.environ.get('dataset')\n",
    "            \n",
    "        if  'table' in os.environ:\n",
    "            request_args['table']= os.environ.get('table')\n",
    "            \n",
    "        if  'region' in os.environ:\n",
    "            request_args['region']= os.environ.get('region')\n",
    "            \n",
    "        if  'metadata_columns' in os.environ:\n",
    "            request_args['metadata_columns']= os.environ.get('metadata_columns')\n",
    "            \n",
    "        if  'page_content_columns' in os.environ:\n",
    "            request_args['page_content_columns']= os.environ.get('page_content_columns')\n",
    "            \n",
    "        if  'source_query_str' in os.environ:\n",
    "            request_args['source_query_str']= os.environ.get('source_query_str')\n",
    "            \n",
    "        if  'chunk_size' in os.environ:\n",
    "            request_args['chunk_size']= os.environ.get('chunk_size')\n",
    "            \n",
    "        if  'chunk_overlap' in os.environ:\n",
    "            request_args['chunk_overlap']= os.environ.get('chunk_overlap')\n",
    "            \n",
    "        if  'max_prompt_count_limit' in os.environ:\n",
    "            request_args['max_prompt_count_limit']= os.environ.get('max_prompt_count_limit')\n",
    "        \n",
    "        chunk_bq_content(request_args)\n",
    "            \n",
    "          \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0fd3ea7c-59a5-41ec-a979-d84db3e2bc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='HeadLine: Gobsmacked by Barilaro’s behaviour? Join the club\n",
      "Content: Just what ClubsNSW needs: as it is trying to convince NSW voters that it has a good heart, a bid by John Barilaro to take over the helm (“Barilaro bids to become clubs boss”, February 3). Here is a man who achieved infamy by repeatedly demonstrating questionable judgment throughout his political life. Barilaro’s attempt to take over from Josh Landis should be consigned to the same rubbish bin that holds his bid for appointment as NSW trade commissioner to New York. Kim Woo, Mascot'\n",
      "\n",
      "\n",
      "page_content='I read the article about John Barilaro’s utterly unconscionable behaviour, which effectively deprived most of the Labor electorates of their emergency bushfire funding, with smoke coming out of my ears (“Barilaro’s office in fire funding storm”, February 3). The admitted pork barrelling by this government was a vote-buying strategy. Such reprehensible action needs way more than the usual slap on the wrist, so it’ll be interesting to see what happens next. Anne Ring, Coogee\n",
      " To exacerbate people’s misery, and politicise it, is the lowest of the low. The relief recovery program is there to ensure all affected by the Black Summer fires get assistance. Barilaro shows us – again and again – how little compassion he has for the people of NSW. Daniela Catalano, Haberfield'\n",
      "\n",
      "\n",
      "page_content='Sadly, the revelations show two things: first, that the egregious former minister for John Barilaro appears ready to continue to serve the people of NSW in any capacity – provided it has a six-figure salary attached. Second, that while in office he didn’t have the nous to understand how pork-barrelling works. The idea is to get people to vote for you, so it is logically directed at seats you don’t have, rather than ones you do. Unless, of course, he realised the Coalition was (is) so on the nose that it needed to shore-up those seats it already held. Or maybe both of these cases are true. Greg Oehm, Robertson\n",
      " Why is there no punishment for pork barrelling? In public life, fraudulent behaviour is punishable under law. For politicians, it’s business as usual. Forget about improved sporting facilities, safer car parks and appropriate disaster relief. So long as these actions come without consequence, nothing will change. Sam Kent, Hunters Hill'\n",
      "\n",
      "\n",
      "page_content='I pondered long and hard in an attempt to predict what Barilaro might include in his job application for ClubsNSW. All I could dredge up were greyhounds, koalas, brumbies, threats to blow up the government, overseas trade appointments and, then, Thursday’s revelation of his interference in the bushfire recovery program. When will our failed politicians realise that they are unemployable? Even by ClubsNSW. Rhonda Seymour, Castle Hill\n",
      " Quite appropriate really – a man who directed desperately needed bushfire recovery funds away from ALP seats now wants to run an organisation that exists on the back of problem gamblers and illegal money laundering. Sounds like the ethics will be a perfect fit. Michael McMullan, Avoca Beach\n",
      " Is it any wonder that Barilaro denied bushfire recovery funding to the Blue Mountains? Apart from it being a Labor seat, he once referred to koalas as tree rats. The man has no love of the bush. Sally Spurr, Lane Cove'\n",
      "\n",
      "\n",
      "page_content='The former deputy premier is reportedly proud of his nickname Pork Barilaro. His arrogance is breathtaking, and the revelation that the joint federal and state emergency bushfire funds were distributed on partisan grounds under his direction is contemptuous. Why am I not surprised that he has now put his hand up for the recently vacated ClubsNSW job? His intervention in the emergency bushfire recovery program needs to be immediately reviewed by ICAC. Peter Neufeld, Mosman\n",
      " I suggest that regardless of which party wins in March it considers appointing a Chief Pub Tester for major policy and decision-making. The beauty of the role is that it could be filled by 99 per cent of the population, and the only equipment the chief would require is an egg timer, a bar stool and a beer or G&T. Monique Darcy, Davidson\n",
      " The integrity and transparency of the Perrottet government can be summed up in one word: Barilaro. Michael Petras, Thornleigh'\n",
      "\n",
      "\n",
      "page_content='It’s hard to decide whether I feel utter contempt for the cynical interference in disaster relief or just bewilderment at such delusional ambition? Has Barilaro considered putting his hand up for POTUS in 2024? Phil Bradshaw, Naremburn\n",
      " Good old “Pork Barilaro”, the gift that keeps giving to NSW Labor. Rob Phillips, North Epping\n",
      " I’m buying up popcorn. The Barilaro ICAC will be a corker. Dorothy Kamaker, Whale Beach'\n",
      "\n",
      "\n",
      "page_content='Divisive Pell not a martyr, and no saint\n",
      " Tony Abbott called Cardinal George Pell “a saint of our times” and a victim of a modern-day crucifixion (“As Pell the man is laid to rest, Pell the martyr rises”, February 3). As I understand from my 13 years of Catholic education, Jesus was crucified for threatening and challenging the entrenched power structure and calcified ideologies and attitudes of the church hierarchy, not for propping it up. Monica Oppen, Stanmore \n",
      " Abbott’s grasp of the requirements for sainthood and of the array of great Catholics that Australia has produced helps me to understand even further his time as prime minister. Sister Susan Connelly, Lakemba \n",
      " My condolences to anyone praying for any ongoing meaningful contribution from Abbott to Australia. Ben Dryza, Newtown \n",
      " Abbott said “George Pell was the greatest man I’ve ever known”. Yet no knighthood. Mark Kilminster, Castle Hill'\n",
      "\n",
      "\n",
      "page_content='“Warrior”, “soldier of truth”, “saint”, “hero”. Gandhi? Nelson Mandela? Martin Luther King? No. George Pell. Seriously? Julia Booth, Westleigh \n",
      " By far the most significant comment at the funeral was Abbott’s; that Pell should never have been investigated without a complaint; he should never have been charged without corroborating evidence, and should never have been convicted without a plausible case, as the High Court eventually realised. Laurie Le Claire, Epping \n",
      " Such delusions of grandeur. The deliberately divisive Pell was no more a saint than Abbott was a leader. Anne Garvan, Chatswood West\n",
      " The last time I checked, Saint Mary Mackillop is clearly more highly revered than a late cardinal. Saint Mary Mackillop is the greatest Catholic Australia has ever produced. Zara Tai, Minchinbury\n",
      " The question is, has Pell travelled northwards or southwards? David Boyd, Bondi Beach'\n",
      "\n",
      "\n",
      "page_content='The question is, has Pell travelled northwards or southwards? David Boyd, Bondi Beach\n",
      " Pell incites strong opinions among his supporters and haters, even in death. Let us all move on with his passing, but may the Catholic Church move with integrity, compassion and justice. If the Church always tries to put the institution above the people, the institution, even after 2000 years, will not survive. Vincent Wong, Killara\n",
      " Dear Tony Abbott,How dare you turn a wilful blind eye to Cardinal Pell’s own wilful and well documented blind eye to the abuse of children. Sue Burton, Mosman\n",
      " The man Abbott describes as the “greatest I’ve ever known” was someone found by a royal commission to have protected paedophiles. What does this say about Abbott?'\n",
      "\n",
      "\n",
      "page_content='Still, it’s no surprise to see a conga line of Liberal luminaries now waxing so lyrical about Cardinal Pell. In Pell they found a fellow right wing political operative par excellence, who brought the power and prestige of the Church to the project of reactionary conservatism. Michael Hinchey, New Lambton \n",
      " Clearly, Abbott lives in a parallel universe to most of us. Peter Kamenyitzky, Castle Hill'\n",
      "\n",
      "\n",
      "page_content='Clearly, Abbott lives in a parallel universe to most of us. Peter Kamenyitzky, Castle Hill \n",
      " Interesting to see the fawning adoration by Tony Abbott at the memorial service for George Pell. Including of course that he was a victim of a media witch hunt against him and the Catholic Church in general. George Pell presided over an administration that did little to redress the victims of sexual abuse nor did it acknowledge what future safeguards have been put in place. His administration was a failure. His only interest was protecting the Church and its money. His line at the royal commission, “It was of little interest to me”, perhaps shows the true colours of the man. Stephen Trevarrow, New Farm (Qld)'\n",
      "\n",
      "\n",
      "page_content='How do we reconcile Tony Abbott’s proclamation that Cardinal George Pell is a “saint for our times” with Pell’s own statement that multiple claims of child sexual abuse perpetrated by Catholic priests under his watch were of not much interest to him? The man is hardly a martyr if the definition is “a person who voluntarily suffers death as the penalty of witnessing to and refusing to renounce a religion”. Meg Pickup, Ballina \n",
      " Not one letter writer has shown even a drop of human kindness (Letters, February 3). The man, a beacon of endeavour and achievement, was an early crusader in the fight against sexual abuse in the church, subsequently deemed inadequate, but revolutionary in its time. Pell endured horrifically biased treatment from the ABC and the Victorian police, tolerated a dud rap as a criminal, and was eventually totally vindicated, yet, rather than bitterness or hatred, he displayed only his enduring faith in his God.'\n",
      "\n",
      "\n",
      "page_content='How was Pell not a giant of a man? Flawed yes, but none of us is perfect, and he didn’t ever deserve the viciousness he faced. Convicted serial killers are shown more tolerance. Rosemary O’Brien, Ashfield'\n",
      "\n",
      "\n",
      "page_content='Bernini explained that since St. Peter’s is “the mother church of nearly all the others, it had to have colonnades, which would show it as if stretching out its arms maternally to receive Catholics, so as to confirm them in their faith, heretics to reunite them to the church, and infidels, to enlighten them in the true faith.” Pell’s funeral made plain for all the church in Australia has closed its arms to the wounded, those it harmed, ignored their cries of pain, and in so doing, failed to follow Jesus’ admonition to welcome and comfort the injured and hold them close, invite them in to St Mary’s Cathedral and acknowledge its crimes against the young innocents. Instead, it denied the findings of a royal commission, denied their very existence. The beginning of the road to redemption is to first admit your sins. This week, the Catholic Church did the opposite, it said nothing happened, there were no victims and proved it is too arrogant and out of touch, it can only deny its acts by'\n",
      "\n",
      "\n",
      "page_content='there were no victims and proved it is too arrogant and out of touch, it can only deny its acts by washing away the blood of the young innocent. Philip Drew, Annandale'\n",
      "\n",
      "\n",
      "page_content='Thank goodness for former PM John Howard, Tony Abbott, Peter Dutton, Matt Canavan, and Dan Tehan for standing up to witness the passing of Cardinal Pell.It is disappointing our other current political leaders were too small-minded, self-serving, and “go with the flow” to honour Pell who was used as a scapegoat, and suffered 404 days in gaol for a crime he did not commit. But he is in a better place now, thank God. Elizabeth Vickers, Maroubra'\n",
      "\n",
      "\n",
      "page_content='Nevermind the monarchists, get the royals off our notes\n",
      " Does the decision to replace the late Queen’s head with an Indigenous design on a single banknote really constitute a poke in the eye to monarchists, or is it simply an attempt to acknowledge the nation’s First People on our currency (“Timing of new $5 note without King could be better”, February 3)? It seems an over reaction to a minor change that will probably go unnoticed by most of the population. After all, how many of us could accurately describe exactly what is on each banknote without looking? Merona Martin, Meroo Meadow\n",
      " I don’t understand why the dysfunctional Windsor family are on Australia’s currency anyway. James Duggan, Hunters Hill'\n",
      "\n",
      "\n",
      "page_content='The monarchists seem so sad over the $5 note business. Why not cheer them up by having more royals on our money, reminding us colonials how great the monarchy has been. Put Charles on the fiver and Andrew on the $10. The $20 could have the former Edward VIII and Wallace. The $50 would be perfect for Henry VIII, circled by his five wives. Nothing less than the $100 note would do for Prince Harry. Richard Macey, Pendle Hill \n",
      " I applaud the decision not to issue a $5 note with King Charles on it. They are obviously waiting until after the coronation; any balding man looks better wearing a hat or a large crown. Noel Mills, Avalon Beach'\n",
      "\n",
      "\n",
      "page_content='Better pay the best cure for GP crisis\n",
      " The government’s approach is novel: leave the cause untreated and see if the patient survives (“Doctor pay unaddressed in report”, February 3). When the main cause of the crisis in general practice is that the bulk-billing Medicare rebate is about half of where it should have been had it kept pace with inflation over the last 30 years – making bulk-billing economically unviable and leading medical graduates to pursue other paths where their expertise is more valued – the government seems to want to waste money on doing everything it can to avoid actually treating the cause. The prognosis isn’t good. Alan Garrity, North Narrabeen'\n",
      "\n",
      "\n",
      "page_content='A close family friend has had his very successful inner-city GP practice up for sale for three years with no buyers and has now been forced to simply shut it down. Payments to GPs have not increased markedly for many years unlike their business overheads, resulting in marginal profitability for doctors often working in excess of 60 hours a week. What a sad commentary on our modern society when highly trained and respected GPs, whose opinions often determine the length and quality of our lives, struggle to make a living while tradies earn $150 an hour and drive around in $100,000 utes. Tony Snellgrove, Tumbulgum'\n",
      "\n",
      "\n",
      "page_content='Inequality needs new thinking\n",
      " Decades of neo-liberal economics along with the slow burn of fiscal austerity have driven the relentless selling-off of government instrumentalities and the ransacking of many vital government departments (“Shrill critics of Chalmers’ essay are missing the point,” February 3). It provided previous governments with the revenue to win votes through tax cuts and nepotistic funding rorts. It made winners of the well-heeled, while more of us became steadily less and less well off.\n",
      " Inequality has now reached epidemic proportions and areas such as education, health, housing, aged care and transport are in fiscal crisis and desperate for funding. For decades, we have been sold the lie that taxes are a burden on our economy and to be feared. It is time we considered a more equitable and inclusive way of improving the wealth of our nation. Bruce Spence, Balmain'\n",
      "\n",
      "\n",
      "page_content='We now have a treasurer who writes an essay offering new ideas to create a fairer, better society for all Australians. His ideas are progressive, thought-provoking and challenging. The voices that criticise him, immediately label him a communist and a real danger to corporate Australia. The attitude by those that have, against those that have not, are predictable, archaic, self-centred and selfish. Jim Chalmers is thinking about our future not the next election, surely that’s worth consideration. Geoff Hermon, Maraylya'\n",
      "\n",
      "\n",
      "page_content='Baffling gas decision\n",
      " One of the few good decisions made by Scott Morrison is likely to be overturned (“Scott Morrison’s decision to stop controversial NSW gas field to be overturned”, smh.com.au, February 3). Whatever Morrison’s motives, this was a decision which went some way to address climate change concerns. It is now likely to be overturned with the help of a federal government that claims to care about climate change and global warming. The reason for this decision by the federal government is, apparently, the potential cost of compensation to the gas companies. Once again money trumps public interest.As a long-term Labor supporter I am baffled and deeply troubled by their approach to this issue. Edward Quinn, St Ives'\n",
      "\n",
      "\n",
      "page_content='The Wounded Kangaroo\n",
      " Alan Joyce’s perpetual apologies for Qantas failings wear thin when your QF27 luggage does not arrive in Buenos Aires three days before departing an Antarctic voyage (“Despite the hype, Qantas turnbacks are a sign of strong safety”, February 3). No apologies. No explanation. And no expedition gear. Just anxiety.\n",
      " Unfortunately, Qantas no longer brings an offering to market which is as certain as it may be safe.\n",
      " It’s time for a management rethink because, right now, the Spirit of Australia days are over. Any airline will do. Stephen Goddard, Bowral'\n",
      "\n",
      "\n",
      "page_content='Classroom conundrum\n",
      " Decisions are complex, not binary. Should single sex schools exist? It depends (“Enrolments at public boys’ high schools in steep decline”, February 3).\n",
      " For some children they work. For others not. It is about the makeup of the individual, not social engineering. My son went to Homebush Boys High and it worked for him. Would he have done better in a co-ed school? We will never know.\n",
      " The pendulum is swinging towards co-ed schools. I predict in a couple of decades it will start swinging the other way. Why do we think there has to be one answer? Neville Turbit, Russell Lea'\n",
      "\n",
      "\n",
      "page_content='Dutton must grow up\n",
      " Ossified, obsolete and irrelevant, the Liberal Party, with or without the Duttons of this world, must soon begin to transform to 21st century life or face extinction (“Dutton must pass Harbour test”, February 3). The fresh Labor government has shaped up well so far but needs a viable and intelligent opposition for our democracy to function as we all would wish. Why is no one rising to the challenge? Brian Haisman, Winmalee\n",
      " Nick Bryant is one among many who point out the difficulties ahead for Peter Dutton as leader of the Liberal party. I wonder if a 2023 version of Don Chipp will emerge from the moderate malcontents of the party. John Bailey, Canterbury'\n",
      "\n",
      "\n",
      "page_content='Does Peter Dutton know what’s really “an attack on our society”? He plays the divide and conquer card while aiming to appeal to the tone-deaf. Trying to ignite a fire when none need to exist. It’s well past time to pay our due to our First Nations on whose land we live. It’s time to grow up. Francesca Stahlut, Armidale'\n",
      "\n",
      "\n",
      "page_content='Only stand up for a standing ovation\n",
      " Theatregoers standing for the curtain call ovation are a minor irritation (Letters, February 3). The real problem punters are the concertgoers who stand in their seats for half of the show. John Swanton, Coogee\n",
      " Over the decades I have attended countless music concerts and many had the audience give a standing ovation. There was one concert where the crowd got to its feet at the end not in appreciation but to demand the band return to the stage as their performance had been so short.\n",
      " When it appeared they had no intention of playing more, the audience took to throwing cushions and ripping seats out to throw at the stage in disgust. This was Elvis Costello’s first concert in Sydney in 1978 at the Regent theatre; his future ones were much longer. Con Vaitsas, Ashbury'\n",
      "\n",
      "\n",
      "page_content='Looking for a three-headed emu in a haystack\n",
      " Good news that the radioactive pod was located in Western Australia, but I await the first appearance of a three-headed emu (Letters, February 3). Mark Berg, Caringbah South\n",
      " \n",
      "\n",
      "Put ‘woke’ back to sleep\n",
      " Could we please put “woke” back to sleep. It has become another boring, pointless word that spoils a good argument or description. Kath Maher, Lidcombe'\n",
      "\n",
      "\n",
      "page_content='Postscript\n",
      " Kim Crawford of Springwood was one of a number of letter writers who claimed to have double-checked the date on Friday’s Herald front page. So confounding were the headlines - Cardinal George Pell described as a “martyr” and a “saint of our times”; the former NSW deputy premier eyeing off the newly vacated job of ClubsNSW chairman, and living up to his nickname of “pork Barilaro” - that correspondents thought they’d woken up on April 1.\n",
      " George Fishman of Vaucluse was one of only a few who wrote that “whatever side you are on, the funeral oration by Tony Abbott hailing George Pell was one for the ages”. Not surprisingly, the majority of writers were scathing of the former PM’s statement that “Pell was the greatest man I’ve ever known”, many agreeing with Peter Kamenyitzky of Castle Hill that Abbott “clearly lives in a parallel universe to most of us”, while others suggested he needs to meet more men.'\n",
      "\n",
      "\n",
      "page_content='Others, like Sue Burton of Mosman, were critical of Abbott turning “a wilful blind eye to Pell’s own wilful and well documented blind eye to the abuse of children”.\n",
      " “I respectfully suggest that John Barilaro now be awarded the title of ‘the human headline’ (with apologies to Derryn Hinch),“wrote Lorraine Hickey of Green Point. “So many juicy stories, so little time.” Phil Bradshaw of Naremburn was so bewildered at the former minister’s ambition, he wondered whether “Pork Barilaro” would consider putting his hand up for POTUS in 2024. Pat Stringa, letters editor\n",
      " To submit a letter to The Sydney Morning Herald, email letters@smh.com.au. Click here for tips on how to submit letters. The Opinion newsletter is a weekly wrap of views that will challenge, champion and inform. Sign up here.'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# page_content='HeadLine: Gobsmacked by Barilaro’s behaviour? Join the club\\nContent: Just what ClubsNSW needs: as it is trying to convince NSW voters that it has a good heart, a bid by John Barilaro to take over the helm (“Barilaro bids to become clubs boss”, February 3). Here is a man who achieved infamy by repeatedly demonstrating questionable judgment throughout his political life. Barilaro’s attempt to take over from Josh Landis should be consigned to the same rubbish bin that holds his bid for appointment as NSW trade commissioner to New York. Kim Woo, Mascot\\n I read the article about John Barilaro’s utterly unconscionable behaviour, which effectively deprived most of the Labor electorates of their emergency bushfire funding, with smoke coming out of my ears (“Barilaro’s office in fire funding storm”, February 3). The admitted pork barrelling by this government was a vote-buying strategy. Such reprehensible action needs way more than the usual slap on the wrist, so it’ll be interesting to see what happens next. Anne Ring, Coogee\\n To exacerbate people’s misery, and politicise it, is the lowest of the low. The relief recovery program is there to ensure all affected by the Black Summer fires get assistance. Barilaro shows us – again and again – how little compassion he has for the people of NSW. Daniela Catalano, Haberfield\\n Sadly, the revelations show two things: first, that the egregious former minister for John Barilaro appears ready to continue to serve the people of NSW in any capacity – provided it has a six-figure salary attached. Second, that while in office he didn’t have the nous to understand how pork-barrelling works. The idea is to get people to vote for you, so it is logically directed at seats you don’t have, rather than ones you do. Unless, of course, he realised the Coalition was (is) so on the nose that it needed to shore-up those seats it already held. Or maybe both of these cases are true. Greg Oehm, Robertson\\n Why is there no punishment for pork barrelling? In public life, fraudulent behaviour is punishable under law. For politicians, it’s business as usual. Forget about improved sporting facilities, safer car parks and appropriate disaster relief. So long as these actions come without consequence, nothing will change. Sam Kent, Hunters Hill\\n I pondered long and hard in an attempt to predict what Barilaro might include in his job application for ClubsNSW. All I could dredge up were greyhounds, koalas, brumbies, threats to blow up the government, overseas trade appointments and, then, Thursday’s revelation of his interference in the bushfire recovery program. When will our failed politicians realise that they are unemployable? Even by ClubsNSW. Rhonda Seymour, Castle Hill\\n Quite appropriate really – a man who directed desperately needed bushfire recovery funds away from ALP seats now wants to run an organisation that exists on the back of problem gamblers and illegal money laundering. Sounds like the ethics will be a perfect fit. Michael McMullan, Avoca Beach\\n Is it any wonder that Barilaro denied bushfire recovery funding to the Blue Mountains? Apart from it being a Labor seat, he once referred to koalas as tree rats. The man has no love of the bush. Sally Spurr, Lane Cove\\n The former deputy premier is reportedly proud of his nickname Pork Barilaro. His arrogance is breathtaking, and the revelation that the joint federal and state emergency bushfire funds were distributed on partisan grounds under his direction is contemptuous. Why am I not surprised that he has now put his hand up for the recently vacated ClubsNSW job? His intervention in the emergency bushfire recovery program needs to be immediately reviewed by ICAC. Peter Neufeld, Mosman\\n I suggest that regardless of which party wins in March it considers appointing a Chief Pub Tester for major policy and decision-making. The beauty of the role is that it could be filled by 99 per cent of the population, and the only equipment the chief would require is an egg timer, a bar stool and a beer or G&T. Monique Darcy, Davidson\\n The integrity and transparency of the Perrottet government can be summed up in one word: Barilaro. Michael Petras, Thornleigh\\n It’s hard to decide whether I feel utter contempt for the cynical interference in disaster relief or just bewilderment at such delusional ambition? Has Barilaro considered putting his hand up for POTUS in 2024? Phil Bradshaw, Naremburn\\n Good old “Pork Barilaro”, the gift that keeps giving to NSW Labor. Rob Phillips, North Epping\\n I’m buying up popcorn. The Barilaro ICAC will be a corker. Dorothy Kamaker, Whale Beach\\n Divisive Pell not a martyr, and no saint\\n Tony Abbott called Cardinal George Pell “a saint of our times” and a victim of a modern-day crucifixion (“As Pell the man is laid to rest, Pell the martyr rises”, February 3). As I understand from my 13 years of Catholic education, Jesus was crucified for threatening and challenging the entrenched power structure and calcified ideologies and attitudes of the church hierarchy, not for propping it up. Monica Oppen, Stanmore \\n Abbott’s grasp of the requirements for sainthood and of the array of great Catholics that Australia has produced helps me to understand even further his time as prime minister. Sister Susan Connelly, Lakemba \\n My condolences to anyone praying for any ongoing meaningful contribution from Abbott to Australia. Ben Dryza, Newtown \\n Abbott said “George Pell was the greatest man I’ve ever known”. Yet no knighthood. Mark Kilminster, Castle Hill \\n “Warrior”, “soldier of truth”, “saint”, “hero”. Gandhi? Nelson Mandela? Martin Luther King? No. George Pell. Seriously? Julia Booth, Westleigh \\n By far the most significant comment at the funeral was Abbott’s; that Pell should never have been investigated without a complaint; he should never have been charged without corroborating evidence, and should never have been convicted without a plausible case, as the High Court eventually realised. Laurie Le Claire, Epping \\n Such delusions of grandeur. The deliberately divisive Pell was no more a saint than Abbott was a leader. Anne Garvan, Chatswood West\\n The last time I checked, Saint Mary Mackillop is clearly more highly revered than a late cardinal. Saint Mary Mackillop is the greatest Catholic Australia has ever produced. Zara Tai, Minchinbury\\n The question is, has Pell travelled northwards or southwards? David Boyd, Bondi Beach\\n Pell incites strong opinions among his supporters and haters, even in death. Let us all move on with his passing, but may the Catholic Church move with integrity, compassion and justice. If the Church always tries to put the institution above the people, the institution, even after 2000 years, will not survive. Vincent Wong, Killara\\n Dear Tony Abbott,How dare you turn a wilful blind eye to Cardinal Pell’s own wilful and well documented blind eye to the abuse of children. Sue Burton, Mosman\\n The man Abbott describes as the “greatest I’ve ever known” was someone found by a royal commission to have protected paedophiles. What does this say about Abbott?\\n Still, it’s no surprise to see a conga line of Liberal luminaries now waxing so lyrical about Cardinal Pell. In Pell they found a fellow right wing political operative par excellence, who brought the power and prestige of the Church to the project of reactionary conservatism. Michael Hinchey, New Lambton \\n Clearly, Abbott lives in a parallel universe to most of us. Peter Kamenyitzky, Castle Hill \\n Interesting to see the fawning adoration by Tony Abbott at the memorial service for George Pell. Including of course that he was a victim of a media witch hunt against him and the Catholic Church in general. George Pell presided over an administration that did little to redress the victims of sexual abuse nor did it acknowledge what future safeguards have been put in place. His administration was a failure. His only interest was protecting the Church and its money. His line at the royal commission, “It was of little interest to me”, perhaps shows the true colours of the man. Stephen Trevarrow, New Farm (Qld)\\n How do we reconcile Tony Abbott’s proclamation that Cardinal George Pell is a “saint for our times” with Pell’s own statement that multiple claims of child sexual abuse perpetrated by Catholic priests under his watch were of not much interest to him? The man is hardly a martyr if the definition is “a person who voluntarily suffers death as the penalty of witnessing to and refusing to renounce a religion”. Meg Pickup, Ballina \\n Not one letter writer has shown even a drop of human kindness (Letters, February 3). The man, a beacon of endeavour and achievement, was an early crusader in the fight against sexual abuse in the church, subsequently deemed inadequate, but revolutionary in its time. Pell endured horrifically biased treatment from the ABC and the Victorian police, tolerated a dud rap as a criminal, and was eventually totally vindicated, yet, rather than bitterness or hatred, he displayed only his enduring faith in his God.\\n How was Pell not a giant of a man? Flawed yes, but none of us is perfect, and he didn’t ever deserve the viciousness he faced. Convicted serial killers are shown more tolerance. Rosemary O’Brien, Ashfield\\n Bernini explained that since St. Peter’s is “the mother church of nearly all the others, it had to have colonnades, which would show it as if stretching out its arms maternally to receive Catholics, so as to confirm them in their faith, heretics to reunite them to the church, and infidels, to enlighten them in the true faith.” Pell’s funeral made plain for all the church in Australia has closed its arms to the wounded, those it harmed, ignored their cries of pain, and in so doing, failed to follow Jesus’ admonition to welcome and comfort the injured and hold them close, invite them in to St Mary’s Cathedral and acknowledge its crimes against the young innocents. Instead, it denied the findings of a royal commission, denied their very existence. The beginning of the road to redemption is to first admit your sins. This week, the Catholic Church did the opposite, it said nothing happened, there were no victims and proved it is too arrogant and out of touch, it can only deny its acts by washing away the blood of the young innocent. Philip Drew, Annandale\\n Thank goodness for former PM John Howard, Tony Abbott, Peter Dutton, Matt Canavan, and Dan Tehan for standing up to witness the passing of Cardinal Pell.It is disappointing our other current political leaders were too small-minded, self-serving, and “go with the flow” to honour Pell who was used as a scapegoat, and suffered 404 days in gaol for a crime he did not commit. But he is in a better place now, thank God. Elizabeth Vickers, Maroubra\\n \\n Nevermind the monarchists, get the royals off our notes\\n Does the decision to replace the late Queen’s head with an Indigenous design on a single banknote really constitute a poke in the eye to monarchists, or is it simply an attempt to acknowledge the nation’s First People on our currency (“Timing of new $5 note without King could be better”, February 3)? It seems an over reaction to a minor change that will probably go unnoticed by most of the population. After all, how many of us could accurately describe exactly what is on each banknote without looking? Merona Martin, Meroo Meadow\\n I don’t understand why the dysfunctional Windsor family are on Australia’s currency anyway. James Duggan, Hunters Hill\\n The monarchists seem so sad over the $5 note business. Why not cheer them up by having more royals on our money, reminding us colonials how great the monarchy has been. Put Charles on the fiver and Andrew on the $10. The $20 could have the former Edward VIII and Wallace. The $50 would be perfect for Henry VIII, circled by his five wives. Nothing less than the $100 note would do for Prince Harry. Richard Macey, Pendle Hill \\n I applaud the decision not to issue a $5 note with King Charles on it. They are obviously waiting until after the coronation; any balding man looks better wearing a hat or a large crown. Noel Mills, Avalon Beach\\n Better pay the best cure for GP crisis\\n The government’s approach is novel: leave the cause untreated and see if the patient survives (“Doctor pay unaddressed in report”, February 3). When the main cause of the crisis in general practice is that the bulk-billing Medicare rebate is about half of where it should have been had it kept pace with inflation over the last 30 years – making bulk-billing economically unviable and leading medical graduates to pursue other paths where their expertise is more valued – the government seems to want to waste money on doing everything it can to avoid actually treating the cause. The prognosis isn’t good. Alan Garrity, North Narrabeen \\n A close family friend has had his very successful inner-city GP practice up for sale for three years with no buyers and has now been forced to simply shut it down. Payments to GPs have not increased markedly for many years unlike their business overheads, resulting in marginal profitability for doctors often working in excess of 60 hours a week. What a sad commentary on our modern society when highly trained and respected GPs, whose opinions often determine the length and quality of our lives, struggle to make a living while tradies earn $150 an hour and drive around in $100,000 utes. Tony Snellgrove, Tumbulgum\\n \\n Inequality needs new thinking\\n Decades of neo-liberal economics along with the slow burn of fiscal austerity have driven the relentless selling-off of government instrumentalities and the ransacking of many vital government departments (“Shrill critics of Chalmers’ essay are missing the point,” February 3). It provided previous governments with the revenue to win votes through tax cuts and nepotistic funding rorts. It made winners of the well-heeled, while more of us became steadily less and less well off.\\n Inequality has now reached epidemic proportions and areas such as education, health, housing, aged care and transport are in fiscal crisis and desperate for funding. For decades, we have been sold the lie that taxes are a burden on our economy and to be feared. It is time we considered a more equitable and inclusive way of improving the wealth of our nation. Bruce Spence, Balmain\\n We now have a treasurer who writes an essay offering new ideas to create a fairer, better society for all Australians. His ideas are progressive, thought-provoking and challenging. The voices that criticise him, immediately label him a communist and a real danger to corporate Australia. The attitude by those that have, against those that have not, are predictable, archaic, self-centred and selfish. Jim Chalmers is thinking about our future not the next election, surely that’s worth consideration. Geoff Hermon, Maraylya\\n Baffling gas decision\\n One of the few good decisions made by Scott Morrison is likely to be overturned (“Scott Morrison’s decision to stop controversial NSW gas field to be overturned”, smh.com.au, February 3). Whatever Morrison’s motives, this was a decision which went some way to address climate change concerns. It is now likely to be overturned with the help of a federal government that claims to care about climate change and global warming. The reason for this decision by the federal government is, apparently, the potential cost of compensation to the gas companies. Once again money trumps public interest.As a long-term Labor supporter I am baffled and deeply troubled by their approach to this issue. Edward Quinn, St Ives \\n The Wounded Kangaroo\\n Alan Joyce’s perpetual apologies for Qantas failings wear thin when your QF27 luggage does not arrive in Buenos Aires three days before departing an Antarctic voyage (“Despite the hype, Qantas turnbacks are a sign of strong safety”, February 3). No apologies. No explanation. And no expedition gear. Just anxiety.\\n Unfortunately, Qantas no longer brings an offering to market which is as certain as it may be safe.\\n It’s time for a management rethink because, right now, the Spirit of Australia days are over. Any airline will do. Stephen Goddard, Bowral\\n Classroom conundrum\\n Decisions are complex, not binary. Should single sex schools exist? It depends (“Enrolments at public boys’ high schools in steep decline”, February 3).\\n For some children they work. For others not. It is about the makeup of the individual, not social engineering. My son went to Homebush Boys High and it worked for him. Would he have done better in a co-ed school? We will never know.\\n The pendulum is swinging towards co-ed schools. I predict in a couple of decades it will start swinging the other way. Why do we think there has to be one answer? Neville Turbit, Russell Lea\\n Dutton must grow up\\n Ossified, obsolete and irrelevant, the Liberal Party, with or without the Duttons of this world, must soon begin to transform to 21st century life or face extinction (“Dutton must pass Harbour test”, February 3). The fresh Labor government has shaped up well so far but needs a viable and intelligent opposition for our democracy to function as we all would wish. Why is no one rising to the challenge? Brian Haisman, Winmalee\\n Nick Bryant is one among many who point out the difficulties ahead for Peter Dutton as leader of the Liberal party. I wonder if a 2023 version of Don Chipp will emerge from the moderate malcontents of the party. John Bailey, Canterbury\\n Does Peter Dutton know what’s really “an attack on our society”? He plays the divide and conquer card while aiming to appeal to the tone-deaf. Trying to ignite a fire when none need to exist. It’s well past time to pay our due to our First Nations on whose land we live. It’s time to grow up. Francesca Stahlut, Armidale\\n Only stand up for a standing ovation\\n Theatregoers standing for the curtain call ovation are a minor irritation (Letters, February 3). The real problem punters are the concertgoers who stand in their seats for half of the show. John Swanton, Coogee\\n Over the decades I have attended countless music concerts and many had the audience give a standing ovation. There was one concert where the crowd got to its feet at the end not in appreciation but to demand the band return to the stage as their performance had been so short.\\n When it appeared they had no intention of playing more, the audience took to throwing cushions and ripping seats out to throw at the stage in disgust. This was Elvis Costello’s first concert in Sydney in 1978 at the Regent theatre; his future ones were much longer. Con Vaitsas, Ashbury\\n \\n Looking for a three-headed emu in a haystack\\n Good news that the radioactive pod was located in Western Australia, but I await the first appearance of a three-headed emu (Letters, February 3). Mark Berg, Caringbah South\\n \\n\\nPut ‘woke’ back to sleep\\n Could we please put “woke” back to sleep. It has become another boring, pointless word that spoils a good argument or description. Kath Maher, Lidcombe\\n Postscript\\n Kim Crawford of Springwood was one of a number of letter writers who claimed to have double-checked the date on Friday’s Herald front page. So confounding were the headlines - Cardinal George Pell described as a “martyr” and a “saint of our times”; the former NSW deputy premier eyeing off the newly vacated job of ClubsNSW chairman, and living up to his nickname of “pork Barilaro” - that correspondents thought they’d woken up on April 1.\\n George Fishman of Vaucluse was one of only a few who wrote that “whatever side you are on, the funeral oration by Tony Abbott hailing George Pell was one for the ages”. Not surprisingly, the majority of writers were scathing of the former PM’s statement that “Pell was the greatest man I’ve ever known”, many agreeing with Peter Kamenyitzky of Castle Hill that Abbott “clearly lives in a parallel universe to most of us”, while others suggested he needs to meet more men.\\n Others, like Sue Burton of Mosman, were critical of Abbott turning “a wilful blind eye to Pell’s own wilful and well documented blind eye to the abuse of children”.\\n “I respectfully suggest that John Barilaro now be awarded the title of ‘the human headline’ (with apologies to Derryn Hinch),“wrote Lorraine Hickey of Green Point. “So many juicy stories, so little time.” Phil Bradshaw of Naremburn was so bewildered at the former minister’s ambition, he wondered whether “Pork Barilaro” would consider putting his hand up for POTUS in 2024. Pat Stringa, letters editor\\n To submit a letter to The Sydney Morning Herald, email letters@smh.com.au. Click here for tips on how to submit letters. The Opinion newsletter is a weekly wrap of views that will challenge, champion and inform. Sign up here.\\n'\n",
    "\n",
    "page_content='HeadLine: Gobsmacked by Barilaro’s behaviour? Join the club\\nContent: Just what ClubsNSW needs: as it is trying to convince NSW voters that it has a good heart, a bid by John Barilaro to take over the helm (“Barilaro bids to become clubs boss”, February 3). Here is a man who achieved infamy by repeatedly demonstrating questionable judgment throughout his political life. Barilaro’s attempt to take over from Josh Landis should be consigned to the same rubbish bin that holds his bid for appointment as NSW trade commissioner to New York. Kim Woo, Mascot\\n I read the article about John Barilaro’s utterly unconscionable behaviour, which effectively deprived most of the Labor electorates of their emergency bushfire funding, with smoke coming out of my ears (“Barilaro’s office in fire funding storm”, February 3). The admitted pork barrelling by this government was a vote-buying strategy. Such reprehensible action needs way more than the usual slap on the wrist, so it’ll be interesting to see what happens next. Anne Ring, Coogee\\n To exacerbate people’s misery, and politicise it, is the lowest of the low. The relief recovery program is there to ensure all affected by the Black Summer fires get assistance. Barilaro shows us – again and again – how little compassion he has for the people of NSW. Daniela Catalano, Haberfield\\n Sadly, the revelations show two things: first, that the egregious former minister for John Barilaro appears ready to continue to serve the people of NSW in any capacity – provided it has a six-figure salary attached. Second, that while in office he didn’t have the nous to understand how pork-barrelling works. The idea is to get people to vote for you, so it is logically directed at seats you don’t have, rather than ones you do. Unless, of course, he realised the Coalition was (is) so on the nose that it needed to shore-up those seats it already held. Or maybe both of these cases are true. Greg Oehm, Robertson\\n Why is there no punishment for pork barrelling? In public life, fraudulent behaviour is punishable under law. For politicians, it’s business as usual. Forget about improved sporting facilities, safer car parks and appropriate disaster relief. So long as these actions come without consequence, nothing will change. Sam Kent, Hunters Hill\\n I pondered long and hard in an attempt to predict what Barilaro might include in his job application for ClubsNSW. All I could dredge up were greyhounds, koalas, brumbies, threats to blow up the government, overseas trade appointments and, then, Thursday’s revelation of his interference in the bushfire recovery program. When will our failed politicians realise that they are unemployable? Even by ClubsNSW. Rhonda Seymour, Castle Hill\\n Quite appropriate really – a man who directed desperately needed bushfire recovery funds away from ALP seats now wants to run an organisation that exists on the back of problem gamblers and illegal money laundering. Sounds like the ethics will be a perfect fit. Michael McMullan, Avoca Beach\\n Is it any wonder that Barilaro denied bushfire recovery funding to the Blue Mountains? Apart from it being a Labor seat, he once referred to koalas as tree rats. The man has no love of the bush. Sally Spurr, Lane Cove\\n The former deputy premier is reportedly proud of his nickname Pork Barilaro. His arrogance is breathtaking, and the revelation that the joint federal and state emergency bushfire funds were distributed on partisan grounds under his direction is contemptuous. Why am I not surprised that he has now put his hand up for the recently vacated ClubsNSW job? His intervention in the emergency bushfire recovery program needs to be immediately reviewed by ICAC. Peter Neufeld, Mosman\\n I suggest that regardless of which party wins in March it considers appointing a Chief Pub Tester for major policy and decision-making. The beauty of the role is that it could be filled by 99 per cent of the population, and the only equipment the chief would require is an egg timer, a bar stool and a beer or G&T. Monique Darcy, Davidson\\n The integrity and transparency of the Perrottet government can be summed up in one word: Barilaro. Michael Petras, Thornleigh\\n It’s hard to decide whether I feel utter contempt for the cynical interference in disaster relief or just bewilderment at such delusional ambition? Has Barilaro considered putting his hand up for POTUS in 2024? Phil Bradshaw, Naremburn\\n Good old “Pork Barilaro”, the gift that keeps giving to NSW Labor. Rob Phillips, North Epping\\n I’m buying up popcorn. The Barilaro ICAC will be a corker. Dorothy Kamaker, Whale Beach\\n \\n\\nDivisive Pell not a martyr, and no saint\\n Tony Abbott called Cardinal George Pell “a saint of our times” and a victim of a modern-day crucifixion (“As Pell the man is laid to rest, Pell the martyr rises”, February 3). As I understand from my 13 years of Catholic education, Jesus was crucified for threatening and challenging the entrenched power structure and calcified ideologies and attitudes of the church hierarchy, not for propping it up. Monica Oppen, Stanmore \\n Abbott’s grasp of the requirements for sainthood and of the array of great Catholics that Australia has produced helps me to understand even further his time as prime minister. Sister Susan Connelly, Lakemba \\n My condolences to anyone praying for any ongoing meaningful contribution from Abbott to Australia. Ben Dryza, Newtown \\n Abbott said “George Pell was the greatest man I’ve ever known”. Yet no knighthood. Mark Kilminster, Castle Hill \\n “Warrior”, “soldier of truth”, “saint”, “hero”. Gandhi? Nelson Mandela? Martin Luther King? No. George Pell. Seriously? Julia Booth, Westleigh \\n By far the most significant comment at the funeral was Abbott’s; that Pell should never have been investigated without a complaint; he should never have been charged without corroborating evidence, and should never have been convicted without a plausible case, as the High Court eventually realised. Laurie Le Claire, Epping \\n Such delusions of grandeur. The deliberately divisive Pell was no more a saint than Abbott was a leader. Anne Garvan, Chatswood West\\n The last time I checked, Saint Mary Mackillop is clearly more highly revered than a late cardinal. Saint Mary Mackillop is the greatest Catholic Australia has ever produced. Zara Tai, Minchinbury\\n The question is, has Pell travelled northwards or southwards? David Boyd, Bondi Beach\\n Pell incites strong opinions among his supporters and haters, even in death. Let us all move on with his passing, but may the Catholic Church move with integrity, compassion and justice. If the Church always tries to put the institution above the people, the institution, even after 2000 years, will not survive. Vincent Wong, Killara\\n Dear Tony Abbott,How dare you turn a wilful blind eye to Cardinal Pell’s own wilful and well documented blind eye to the abuse of children. Sue Burton, Mosman\\n The man Abbott describes as the “greatest I’ve ever known” was someone found by a royal commission to have protected paedophiles. What does this say about Abbott?\\n Still, it’s no surprise to see a conga line of Liberal luminaries now waxing so lyrical about Cardinal Pell. In Pell they found a fellow right wing political operative par excellence, who brought the power and prestige of the Church to the project of reactionary conservatism. Michael Hinchey, New Lambton \\n Clearly, Abbott lives in a parallel universe to most of us. Peter Kamenyitzky, Castle Hill \\n Interesting to see the fawning adoration by Tony Abbott at the memorial service for George Pell. Including of course that he was a victim of a media witch hunt against him and the Catholic Church in general. George Pell presided over an administration that did little to redress the victims of sexual abuse nor did it acknowledge what future safeguards have been put in place. His administration was a failure. His only interest was protecting the Church and its money. His line at the royal commission, “It was of little interest to me”, perhaps shows the true colours of the man. Stephen Trevarrow, New Farm (Qld)\\n How do we reconcile Tony Abbott’s proclamation that Cardinal George Pell is a “saint for our times” with Pell’s own statement that multiple claims of child sexual abuse perpetrated by Catholic priests under his watch were of not much interest to him? The man is hardly a martyr if the definition is “a person who voluntarily suffers death as the penalty of witnessing to and refusing to renounce a religion”. Meg Pickup, Ballina \\n Not one letter writer has shown even a drop of human kindness (Letters, February 3). The man, a beacon of endeavour and achievement, was an early crusader in the fight against sexual abuse in the church, subsequently deemed inadequate, but revolutionary in its time. Pell endured horrifically biased treatment from the ABC and the Victorian police, tolerated a dud rap as a criminal, and was eventually totally vindicated, yet, rather than bitterness or hatred, he displayed only his enduring faith in his God.\\n How was Pell not a giant of a man? Flawed yes, but none of us is perfect, and he didn’t ever deserve the viciousness he faced. Convicted serial killers are shown more tolerance. Rosemary O’Brien, Ashfield\\n Bernini explained that since St. Peter’s is “the mother church of nearly all the others, it had to have colonnades, which would show it as if stretching out its arms maternally to receive Catholics, so as to confirm them in their faith, heretics to reunite them to the church, and infidels, to enlighten them in the true faith.” Pell’s funeral made plain for all the church in Australia has closed its arms to the wounded, those it harmed, ignored their cries of pain, and in so doing, failed to follow Jesus’ admonition to welcome and comfort the injured and hold them close, invite them in to St Mary’s Cathedral and acknowledge its crimes against the young innocents. Instead, it denied the findings of a royal commission, denied their very existence. The beginning of the road to redemption is to first admit your sins. This week, the Catholic Church did the opposite, it said nothing happened, there were no victims and proved it is too arrogant and out of touch, it can only deny its acts by washing away the blood of the young innocent. Philip Drew, Annandale\\n Thank goodness for former PM John Howard, Tony Abbott, Peter Dutton, Matt Canavan, and Dan Tehan for standing up to witness the passing of Cardinal Pell.It is disappointing our other current political leaders were too small-minded, self-serving, and “go with the flow” to honour Pell who was used as a scapegoat, and suffered 404 days in gaol for a crime he did not commit. But he is in a better place now, thank God. Elizabeth Vickers, Maroubra\\n \\n Nevermind the monarchists, get the royals off our notes\\n Does the decision to replace the late Queen’s head with an Indigenous design on a single banknote really constitute a poke in the eye to monarchists, or is it simply an attempt to acknowledge the nation’s First People on our currency (“Timing of new $5 note without King could be better”, February 3)? It seems an over reaction to a minor change that will probably go unnoticed by most of the population. After all, how many of us could accurately describe exactly what is on each banknote without looking? Merona Martin, Meroo Meadow\\n I don’t understand why the dysfunctional Windsor family are on Australia’s currency anyway. James Duggan, Hunters Hill\\n The monarchists seem so sad over the $5 note business. Why not cheer them up by having more royals on our money, reminding us colonials how great the monarchy has been. Put Charles on the fiver and Andrew on the $10. The $20 could have the former Edward VIII and Wallace. The $50 would be perfect for Henry VIII, circled by his five wives. Nothing less than the $100 note would do for Prince Harry. Richard Macey, Pendle Hill \\n I applaud the decision not to issue a $5 note with King Charles on it. They are obviously waiting until after the coronation; any balding man looks better wearing a hat or a large crown. Noel Mills, Avalon Beach\\n Better pay the best cure for GP crisis\\n The government’s approach is novel: leave the cause untreated and see if the patient survives (“Doctor pay unaddressed in report”, February 3). When the main cause of the crisis in general practice is that the bulk-billing Medicare rebate is about half of where it should have been had it kept pace with inflation over the last 30 years – making bulk-billing economically unviable and leading medical graduates to pursue other paths where their expertise is more valued – the government seems to want to waste money on doing everything it can to avoid actually treating the cause. The prognosis isn’t good. Alan Garrity, North Narrabeen \\n A close family friend has had his very successful inner-city GP practice up for sale for three years with no buyers and has now been forced to simply shut it down. Payments to GPs have not increased markedly for many years unlike their business overheads, resulting in marginal profitability for doctors often working in excess of 60 hours a week. What a sad commentary on our modern society when highly trained and respected GPs, whose opinions often determine the length and quality of our lives, struggle to make a living while tradies earn $150 an hour and drive around in $100,000 utes. Tony Snellgrove, Tumbulgum\\n \\n Inequality needs new thinking\\n Decades of neo-liberal economics along with the slow burn of fiscal austerity have driven the relentless selling-off of government instrumentalities and the ransacking of many vital government departments (“Shrill critics of Chalmers’ essay are missing the point,” February 3). It provided previous governments with the revenue to win votes through tax cuts and nepotistic funding rorts. It made winners of the well-heeled, while more of us became steadily less and less well off.\\n Inequality has now reached epidemic proportions and areas such as education, health, housing, aged care and transport are in fiscal crisis and desperate for funding. For decades, we have been sold the lie that taxes are a burden on our economy and to be feared. It is time we considered a more equitable and inclusive way of improving the wealth of our nation. Bruce Spence, Balmain\\n We now have a treasurer who writes an essay offering new ideas to create a fairer, better society for all Australians. His ideas are progressive, thought-provoking and challenging. The voices that criticise him, immediately label him a communist and a real danger to corporate Australia. The attitude by those that have, against those that have not, are predictable, archaic, self-centred and selfish. Jim Chalmers is thinking about our future not the next election, surely that’s worth consideration. Geoff Hermon, Maraylya\\n Baffling gas decision\\n One of the few good decisions made by Scott Morrison is likely to be overturned (“Scott Morrison’s decision to stop controversial NSW gas field to be overturned”, smh.com.au, February 3). Whatever Morrison’s motives, this was a decision which went some way to address climate change concerns. It is now likely to be overturned with the help of a federal government that claims to care about climate change and global warming. The reason for this decision by the federal government is, apparently, the potential cost of compensation to the gas companies. Once again money trumps public interest.As a long-term Labor supporter I am baffled and deeply troubled by their approach to this issue. Edward Quinn, St Ives \\n The Wounded Kangaroo\\n Alan Joyce’s perpetual apologies for Qantas failings wear thin when your QF27 luggage does not arrive in Buenos Aires three days before departing an Antarctic voyage (“Despite the hype, Qantas turnbacks are a sign of strong safety”, February 3). No apologies. No explanation. And no expedition gear. Just anxiety.\\n Unfortunately, Qantas no longer brings an offering to market which is as certain as it may be safe.\\n It’s time for a management rethink because, right now, the Spirit of Australia days are over. Any airline will do. Stephen Goddard, Bowral\\n Classroom conundrum\\n Decisions are complex, not binary. Should single sex schools exist? It depends (“Enrolments at public boys’ high schools in steep decline”, February 3).\\n For some children they work. For others not. It is about the makeup of the individual, not social engineering. My son went to Homebush Boys High and it worked for him. Would he have done better in a co-ed school? We will never know.\\n The pendulum is swinging towards co-ed schools. I predict in a couple of decades it will start swinging the other way. Why do we think there has to be one answer? Neville Turbit, Russell Lea\\n Dutton must grow up\\n Ossified, obsolete and irrelevant, the Liberal Party, with or without the Duttons of this world, must soon begin to transform to 21st century life or face extinction (“Dutton must pass Harbour test”, February 3). The fresh Labor government has shaped up well so far but needs a viable and intelligent opposition for our democracy to function as we all would wish. Why is no one rising to the challenge? Brian Haisman, Winmalee\\n Nick Bryant is one among many who point out the difficulties ahead for Peter Dutton as leader of the Liberal party. I wonder if a 2023 version of Don Chipp will emerge from the moderate malcontents of the party. John Bailey, Canterbury\\n Does Peter Dutton know what’s really “an attack on our society”? He plays the divide and conquer card while aiming to appeal to the tone-deaf. Trying to ignite a fire when none need to exist. It’s well past time to pay our due to our First Nations on whose land we live. It’s time to grow up. Francesca Stahlut, Armidale\\n Only stand up for a standing ovation\\n Theatregoers standing for the curtain call ovation are a minor irritation (Letters, February 3). The real problem punters are the concertgoers who stand in their seats for half of the show. John Swanton, Coogee\\n Over the decades I have attended countless music concerts and many had the audience give a standing ovation. There was one concert where the crowd got to its feet at the end not in appreciation but to demand the band return to the stage as their performance had been so short.\\n When it appeared they had no intention of playing more, the audience took to throwing cushions and ripping seats out to throw at the stage in disgust. This was Elvis Costello’s first concert in Sydney in 1978 at the Regent theatre; his future ones were much longer. Con Vaitsas, Ashbury\\n \\n Looking for a three-headed emu in a haystack\\n Good news that the radioactive pod was located in Western Australia, but I await the first appearance of a three-headed emu (Letters, February 3). Mark Berg, Caringbah South\\n Put ‘woke’ back to sleep\\n Could we please put “woke” back to sleep. It has become another boring, pointless word that spoils a good argument or description. Kath Maher, Lidcombe\\n Postscript\\n Kim Crawford of Springwood was one of a number of letter writers who claimed to have double-checked the date on Friday’s Herald front page. So confounding were the headlines - Cardinal George Pell described as a “martyr” and a “saint of our times”; the former NSW deputy premier eyeing off the newly vacated job of ClubsNSW chairman, and living up to his nickname of “pork Barilaro” - that correspondents thought they’d woken up on April 1.\\n George Fishman of Vaucluse was one of only a few who wrote that “whatever side you are on, the funeral oration by Tony Abbott hailing George Pell was one for the ages”. Not surprisingly, the majority of writers were scathing of the former PM’s statement that “Pell was the greatest man I’ve ever known”, many agreeing with Peter Kamenyitzky of Castle Hill that Abbott “clearly lives in a parallel universe to most of us”, while others suggested he needs to meet more men.\\n Others, like Sue Burton of Mosman, were critical of Abbott turning “a wilful blind eye to Pell’s own wilful and well documented blind eye to the abuse of children”.\\n “I respectfully suggest that John Barilaro now be awarded the title of ‘the human headline’ (with apologies to Derryn Hinch),“wrote Lorraine Hickey of Green Point. “So many juicy stories, so little time.” Phil Bradshaw of Naremburn was so bewildered at the former minister’s ambition, he wondered whether “Pork Barilaro” would consider putting his hand up for POTUS in 2024. Pat Stringa, letters editor\\n To submit a letter to The Sydney Morning Herald, email letters@smh.com.au. Click here for tips on how to submit letters. The Opinion newsletter is a weekly wrap of views that will challenge, champion and inform. Sign up here.\\n'\n",
    "page_content='HeadLine: Gobsmacked by Barilaro’s behaviour? Join the club\\nContent: Just what ClubsNSW needs: as it is trying to convince NSW voters that it has a good heart, a bid by John Barilaro to take over the helm (“Barilaro bids to become clubs boss”, February 3). Here is a man who achieved infamy by repeatedly demonstrating questionable judgment throughout his political life. Barilaro’s attempt to take over from Josh Landis should be consigned to the same rubbish bin that holds his bid for appointment as NSW trade commissioner to New York. Kim Woo, Mascot\\n I read the article about John Barilaro’s utterly unconscionable behaviour, which effectively deprived most of the Labor electorates of their emergency bushfire funding, with smoke coming out of my ears (“Barilaro’s office in fire funding storm”, February 3). The admitted pork barrelling by this government was a vote-buying strategy. Such reprehensible action needs way more than the usual slap on the wrist, so it’ll be interesting to see what happens next. Anne Ring, Coogee\\n To exacerbate people’s misery, and politicise it, is the lowest of the low. The relief recovery program is there to ensure all affected by the Black Summer fires get assistance. Barilaro shows us – again and again – how little compassion he has for the people of NSW. Daniela Catalano, Haberfield\\n Sadly, the revelations show two things: first, that the egregious former minister for John Barilaro appears ready to continue to serve the people of NSW in any capacity – provided it has a six-figure salary attached. Second, that while in office he didn’t have the nous to understand how pork-barrelling works. The idea is to get people to vote for you, so it is logically directed at seats you don’t have, rather than ones you do. Unless, of course, he realised the Coalition was (is) so on the nose that it needed to shore-up those seats it already held. Or maybe both of these cases are true. Greg Oehm, Robertson\\n Why is there no punishment for pork barrelling? In public life, fraudulent behaviour is punishable under law. For politicians, it’s business as usual. Forget about improved sporting facilities, safer car parks and appropriate disaster relief. So long as these actions come without consequence, nothing will change. Sam Kent, Hunters Hill\\n I pondered long and hard in an attempt to predict what Barilaro might include in his job application for ClubsNSW. All I could dredge up were greyhounds, koalas, brumbies, threats to blow up the government, overseas trade appointments and, then, Thursday’s revelation of his interference in the bushfire recovery program. When will our failed politicians realise that they are unemployable? Even by ClubsNSW. Rhonda Seymour, Castle Hill\\n Quite appropriate really – a man who directed desperately needed bushfire recovery funds away from ALP seats now wants to run an organisation that exists on the back of problem gamblers and illegal money laundering. Sounds like the ethics will be a perfect fit. Michael McMullan, Avoca Beach\\n Is it any wonder that Barilaro denied bushfire recovery funding to the Blue Mountains? Apart from it being a Labor seat, he once referred to koalas as tree rats. The man has no love of the bush. Sally Spurr, Lane Cove\\n The former deputy premier is reportedly proud of his nickname Pork Barilaro. His arrogance is breathtaking, and the revelation that the joint federal and state emergency bushfire funds were distributed on partisan grounds under his direction is contemptuous. Why am I not surprised that he has now put his hand up for the recently vacated ClubsNSW job? His intervention in the emergency bushfire recovery program needs to be immediately reviewed by ICAC. Peter Neufeld, Mosman\\n I suggest that regardless of which party wins in March it considers appointing a Chief Pub Tester for major policy and decision-making. The beauty of the role is that it could be filled by 99 per cent of the population, and the only equipment the chief would require is an egg timer, a bar stool and a beer or G&T. Monique Darcy, Davidson\\n The integrity and transparency of the Perrottet government can be summed up in one word: Barilaro. Michael Petras, Thornleigh\\n It’s hard to decide whether I feel utter contempt for the cynical interference in disaster relief or just bewilderment at such delusional ambition? Has Barilaro considered putting his hand up for POTUS in 2024? Phil Bradshaw, Naremburn\\n Good old “Pork Barilaro”, the gift that keeps giving to NSW Labor. Rob Phillips, North Epping\\n I’m buying up popcorn. The Barilaro ICAC will be a corker. Dorothy Kamaker, Whale Beach\\n \\n\\nDivisive Pell not a martyr, and no saint\\n Tony Abbott called Cardinal George Pell “a saint of our times” and a victim of a modern-day crucifixion (“As Pell the man is laid to rest, Pell the martyr rises”, February 3). As I understand from my 13 years of Catholic education, Jesus was crucified for threatening and challenging the entrenched power structure and calcified ideologies and attitudes of the church hierarchy, not for propping it up. Monica Oppen, Stanmore \\n Abbott’s grasp of the requirements for sainthood and of the array of great Catholics that Australia has produced helps me to understand even further his time as prime minister. Sister Susan Connelly, Lakemba \\n My condolences to anyone praying for any ongoing meaningful contribution from Abbott to Australia. Ben Dryza, Newtown \\n Abbott said “George Pell was the greatest man I’ve ever known”. Yet no knighthood. Mark Kilminster, Castle Hill \\n “Warrior”, “soldier of truth”, “saint”, “hero”. Gandhi? Nelson Mandela? Martin Luther King? No. George Pell. Seriously? Julia Booth, Westleigh \\n By far the most significant comment at the funeral was Abbott’s; that Pell should never have been investigated without a complaint; he should never have been charged without corroborating evidence, and should never have been convicted without a plausible case, as the High Court eventually realised. Laurie Le Claire, Epping \\n Such delusions of grandeur. The deliberately divisive Pell was no more a saint than Abbott was a leader. Anne Garvan, Chatswood West\\n The last time I checked, Saint Mary Mackillop is clearly more highly revered than a late cardinal. Saint Mary Mackillop is the greatest Catholic Australia has ever produced. Zara Tai, Minchinbury\\n The question is, has Pell travelled northwards or southwards? David Boyd, Bondi Beach\\n Pell incites strong opinions among his supporters and haters, even in death. Let us all move on with his passing, but may the Catholic Church move with integrity, compassion and justice. If the Church always tries to put the institution above the people, the institution, even after 2000 years, will not survive. Vincent Wong, Killara\\n Dear Tony Abbott,How dare you turn a wilful blind eye to Cardinal Pell’s own wilful and well documented blind eye to the abuse of children. Sue Burton, Mosman\\n The man Abbott describes as the “greatest I’ve ever known” was someone found by a royal commission to have protected paedophiles. What does this say about Abbott?\\n Still, it’s no surprise to see a conga line of Liberal luminaries now waxing so lyrical about Cardinal Pell. In Pell they found a fellow right wing political operative par excellence, who brought the power and prestige of the Church to the project of reactionary conservatism. Michael Hinchey, New Lambton \\n Clearly, Abbott lives in a parallel universe to most of us. Peter Kamenyitzky, Castle Hill \\n Interesting to see the fawning adoration by Tony Abbott at the memorial service for George Pell. Including of course that he was a victim of a media witch hunt against him and the Catholic Church in general. George Pell presided over an administration that did little to redress the victims of sexual abuse nor did it acknowledge what future safeguards have been put in place. His administration was a failure. His only interest was protecting the Church and its money. His line at the royal commission, “It was of little interest to me”, perhaps shows the true colours of the man. Stephen Trevarrow, New Farm (Qld)\\n How do we reconcile Tony Abbott’s proclamation that Cardinal George Pell is a “saint for our times” with Pell’s own statement that multiple claims of child sexual abuse perpetrated by Catholic priests under his watch were of not much interest to him? The man is hardly a martyr if the definition is “a person who voluntarily suffers death as the penalty of witnessing to and refusing to renounce a religion”. Meg Pickup, Ballina \\n Not one letter writer has shown even a drop of human kindness (Letters, February 3). The man, a beacon of endeavour and achievement, was an early crusader in the fight against sexual abuse in the church, subsequently deemed inadequate, but revolutionary in its time. Pell endured horrifically biased treatment from the ABC and the Victorian police, tolerated a dud rap as a criminal, and was eventually totally vindicated, yet, rather than bitterness or hatred, he displayed only his enduring faith in his God.\\n How was Pell not a giant of a man? Flawed yes, but none of us is perfect, and he didn’t ever deserve the viciousness he faced. Convicted serial killers are shown more tolerance. Rosemary O’Brien, Ashfield\\n Bernini explained that since St. Peter’s is “the mother church of nearly all the others, it had to have colonnades, which would show it as if stretching out its arms maternally to receive Catholics, so as to confirm them in their faith, heretics to reunite them to the church, and infidels, to enlighten them in the true faith.” Pell’s funeral made plain for all the church in Australia has closed its arms to the wounded, those it harmed, ignored their cries of pain, and in so doing, failed to follow Jesus’ admonition to welcome and comfort the injured and hold them close, invite them in to St Mary’s Cathedral and acknowledge its crimes against the young innocents. Instead, it denied the findings of a royal commission, denied their very existence. The beginning of the road to redemption is to first admit your sins. This week, the Catholic Church did the opposite, it said nothing happened, there were no victims and proved it is too arrogant and out of touch, it can only deny its acts by washing away the blood of the young innocent. Philip Drew, Annandale\\n Thank goodness for former PM John Howard, Tony Abbott, Peter Dutton, Matt Canavan, and Dan Tehan for standing up to witness the passing of Cardinal Pell.It is disappointing our other current political leaders were too small-minded, self-serving, and “go with the flow” to honour Pell who was used as a scapegoat, and suffered 404 days in gaol for a crime he did not commit. But he is in a better place now, thank God. Elizabeth Vickers, Maroubra\\n \\n\\nNevermind the monarchists, get the royals off our notes\\n Does the decision to replace the late Queen’s head with an Indigenous design on a single banknote really constitute a poke in the eye to monarchists, or is it simply an attempt to acknowledge the nation’s First People on our currency (“Timing of new $5 note without King could be better”, February 3)? It seems an over reaction to a minor change that will probably go unnoticed by most of the population. After all, how many of us could accurately describe exactly what is on each banknote without looking? Merona Martin, Meroo Meadow\\n I don’t understand why the dysfunctional Windsor family are on Australia’s currency anyway. James Duggan, Hunters Hill\\n The monarchists seem so sad over the $5 note business. Why not cheer them up by having more royals on our money, reminding us colonials how great the monarchy has been. Put Charles on the fiver and Andrew on the $10. The $20 could have the former Edward VIII and Wallace. The $50 would be perfect for Henry VIII, circled by his five wives. Nothing less than the $100 note would do for Prince Harry. Richard Macey, Pendle Hill \\n I applaud the decision not to issue a $5 note with King Charles on it. They are obviously waiting until after the coronation; any balding man looks better wearing a hat or a large crown. Noel Mills, Avalon Beach\\n \\n\\nBetter pay the best cure for GP crisis\\n The government’s approach is novel: leave the cause untreated and see if the patient survives (“Doctor pay unaddressed in report”, February 3). When the main cause of the crisis in general practice is that the bulk-billing Medicare rebate is about half of where it should have been had it kept pace with inflation over the last 30 years – making bulk-billing economically unviable and leading medical graduates to pursue other paths where their expertise is more valued – the government seems to want to waste money on doing everything it can to avoid actually treating the cause. The prognosis isn’t good. Alan Garrity, North Narrabeen \\n A close family friend has had his very successful inner-city GP practice up for sale for three years with no buyers and has now been forced to simply shut it down. Payments to GPs have not increased markedly for many years unlike their business overheads, resulting in marginal profitability for doctors often working in excess of 60 hours a week. What a sad commentary on our modern society when highly trained and respected GPs, whose opinions often determine the length and quality of our lives, struggle to make a living while tradies earn $150 an hour and drive around in $100,000 utes. Tony Snellgrove, Tumbulgum\\n \\n\\nInequality needs new thinking\\n Decades of neo-liberal economics along with the slow burn of fiscal austerity have driven the relentless selling-off of government instrumentalities and the ransacking of many vital government departments (“Shrill critics of Chalmers’ essay are missing the point,” February 3). It provided previous governments with the revenue to win votes through tax cuts and nepotistic funding rorts. It made winners of the well-heeled, while more of us became steadily less and less well off.\\n Inequality has now reached epidemic proportions and areas such as education, health, housing, aged care and transport are in fiscal crisis and desperate for funding. For decades, we have been sold the lie that taxes are a burden on our economy and to be feared. It is time we considered a more equitable and inclusive way of improving the wealth of our nation. Bruce Spence, Balmain\\n We now have a treasurer who writes an essay offering new ideas to create a fairer, better society for all Australians. His ideas are progressive, thought-provoking and challenging. The voices that criticise him, immediately label him a communist and a real danger to corporate Australia. The attitude by those that have, against those that have not, are predictable, archaic, self-centred and selfish. Jim Chalmers is thinking about our future not the next election, surely that’s worth consideration. Geoff Hermon, Maraylya\\n \\n\\nBaffling gas decision\\n One of the few good decisions made by Scott Morrison is likely to be overturned (“Scott Morrison’s decision to stop controversial NSW gas field to be overturned”, smh.com.au, February 3). Whatever Morrison’s motives, this was a decision which went some way to address climate change concerns. It is now likely to be overturned with the help of a federal government that claims to care about climate change and global warming. The reason for this decision by the federal government is, apparently, the potential cost of compensation to the gas companies. Once again money trumps public interest.As a long-term Labor supporter I am baffled and deeply troubled by their approach to this issue. Edward Quinn, St Ives \\n \\n\\nThe Wounded Kangaroo\\n Alan Joyce’s perpetual apologies for Qantas failings wear thin when your QF27 luggage does not arrive in Buenos Aires three days before departing an Antarctic voyage (“Despite the hype, Qantas turnbacks are a sign of strong safety”, February 3). No apologies. No explanation. And no expedition gear. Just anxiety.\\n Unfortunately, Qantas no longer brings an offering to market which is as certain as it may be safe.\\n It’s time for a management rethink because, right now, the Spirit of Australia days are over. Any airline will do. Stephen Goddard, Bowral\\n \\n\\nClassroom conundrum\\n Decisions are complex, not binary. Should single sex schools exist? It depends (“Enrolments at public boys’ high schools in steep decline”, February 3).\\n For some children they work. For others not. It is about the makeup of the individual, not social engineering. My son went to Homebush Boys High and it worked for him. Would he have done better in a co-ed school? We will never know.\\n The pendulum is swinging towards co-ed schools. I predict in a couple of decades it will start swinging the other way. Why do we think there has to be one answer? Neville Turbit, Russell Lea\\n \\n\\nDutton must grow up\\n Ossified, obsolete and irrelevant, the Liberal Party, with or without the Duttons of this world, must soon begin to transform to 21st century life or face extinction (“Dutton must pass Harbour test”, February 3). The fresh Labor government has shaped up well so far but needs a viable and intelligent opposition for our democracy to function as we all would wish. Why is no one rising to the challenge? Brian Haisman, Winmalee\\n Nick Bryant is one among many who point out the difficulties ahead for Peter Dutton as leader of the Liberal party. I wonder if a 2023 version of Don Chipp will emerge from the moderate malcontents of the party. John Bailey, Canterbury\\n Does Peter Dutton know what’s really “an attack on our society”? He plays the divide and conquer card while aiming to appeal to the tone-deaf. Trying to ignite a fire when none need to exist. It’s well past time to pay our due to our First Nations on whose land we live. It’s time to grow up. Francesca Stahlut, Armidale\\n \\n\\nOnly stand up for a standing ovation\\n Theatregoers standing for the curtain call ovation are a minor irritation (Letters, February 3). The real problem punters are the concertgoers who stand in their seats for half of the show. John Swanton, Coogee\\n Over the decades I have attended countless music concerts and many had the audience give a standing ovation. There was one concert where the crowd got to its feet at the end not in appreciation but to demand the band return to the stage as their performance had been so short.\\n When it appeared they had no intention of playing more, the audience took to throwing cushions and ripping seats out to throw at the stage in disgust. This was Elvis Costello’s first concert in Sydney in 1978 at the Regent theatre; his future ones were much longer. Con Vaitsas, Ashbury\\n \\n \\n\\nLooking for a three-headed emu in a haystack\\n Good news that the radioactive pod was located in Western Australia, but I await the first appearance of a three-headed emu (Letters, February 3). Mark Berg, Caringbah South\\n \\n\\nPut ‘woke’ back to sleep\\n Could we please put “woke” back to sleep. It has become another boring, pointless word that spoils a good argument or description. Kath Maher, Lidcombe\\n \\n\\nPostscript\\n Kim Crawford of Springwood was one of a number of letter writers who claimed to have double-checked the date on Friday’s Herald front page. So confounding were the headlines - Cardinal George Pell described as a “martyr” and a “saint of our times”; the former NSW deputy premier eyeing off the newly vacated job of ClubsNSW chairman, and living up to his nickname of “pork Barilaro” - that correspondents thought they’d woken up on April 1.\\n George Fishman of Vaucluse was one of only a few who wrote that “whatever side you are on, the funeral oration by Tony Abbott hailing George Pell was one for the ages”. Not surprisingly, the majority of writers were scathing of the former PM’s statement that “Pell was the greatest man I’ve ever known”, many agreeing with Peter Kamenyitzky of Castle Hill that Abbott “clearly lives in a parallel universe to most of us”, while others suggested he needs to meet more men.\\n Others, like Sue Burton of Mosman, were critical of Abbott turning “a wilful blind eye to Pell’s own wilful and well documented blind eye to the abuse of children”.\\n “I respectfully suggest that John Barilaro now be awarded the title of ‘the human headline’ (with apologies to Derryn Hinch),“wrote Lorraine Hickey of Green Point. “So many juicy stories, so little time.” Phil Bradshaw of Naremburn was so bewildered at the former minister’s ambition, he wondered whether “Pork Barilaro” would consider putting his hand up for POTUS in 2024. Pat Stringa, letters editor\\n To submit a letter to The Sydney Morning Herald, email letters@smh.com.au. Click here for tips on how to submit letters. The Opinion newsletter is a weekly wrap of views that will challenge, champion and inform. Sign up here.\\n'\n",
    "\n",
    "texts = text_splitter.create_documents([page_content])\n",
    "for t in texts:\n",
    "    print(t)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dc6aee4-fe0d-4575-8e2f-a65214fa8af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#Split the documents into chunks\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m      4\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      5\u001b[0m         chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      6\u001b[0m        \u001b[38;5;66;03m#separators=separators,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m     doc_splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(\u001b[43mdocuments\u001b[49m)   \n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "if 1==1:\n",
    "    #Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0da75dc3-343c-4d1e-bac0-18ec4b8770c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtruncate_text_tokens\u001b[39m(text, encoding_name\u001b[38;5;241m=\u001b[39mEMBEDDING_ENCODING, max_tokens\u001b[38;5;241m=\u001b[39mEMBEDDING_CTX_LENGTH):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Truncate a string to have `max_tokens` according to the given encoding.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def truncate_text_tokens(text, encoding_name=EMBEDDING_ENCODING, max_tokens=EMBEDDING_CTX_LENGTH):\n",
    "    \"\"\"Truncate a string to have `max_tokens` according to the given encoding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return encoding.encode(text)[:max_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12c32108-df64-4a10-8d2d-befc6ffecd18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2df52ded-2611-486d-ad0d-899e3f692aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while (batch := tuple(islice(it, n))):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97216996-2086-4fc8-b198-7351258f9aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import tiktoken\n",
    "def chunked_tokens(text, encoding_name, chunk_length):\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks_iterator = batched(tokens, chunk_length)\n",
    "    yield from chunks_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "679259f2-6815-46e2-b025-db90967ff1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_MODEL = 'textembedding-gecko'\n",
    "EMBEDDING_CTX_LENGTH = 2048\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "\n",
    "\n",
    "def len_safe_get_embedding(text, model=EMBEDDING_MODEL, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING, average=True):\n",
    "    chunk_embeddings = []\n",
    "    chunk_lens = []\n",
    "    for chunk in chunked_tokens(text, encoding_name=encoding_name, chunk_length=max_tokens):\n",
    "        #chunk_embeddings.append(get_embedding(chunk, model=model))\n",
    "        chunk_lens.append(len(chunk))\n",
    "       \n",
    "    \n",
    "    # if average:\n",
    "    #     chunk_embeddings = np.average(chunk_embeddings, axis=0, weights=chunk_lens)\n",
    "    #     chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings)  # normalizes length to 1\n",
    "    #     chunk_embeddings = chunk_embeddings.tolist()\n",
    "    return chunk_lens #chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3aaa802-5e38-49f4-adee-18b12db18bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text='HeadLine: Gobsmacked by Barilaro’s behaviour? Join the club\\nContent: Just what ClubsNSW needs: as it is trying to convince NSW voters that it has a good heart, a bid by John Barilaro to take over the helm (“Barilaro bids to become clubs boss”, February 3). Here is a man who achieved infamy by repeatedly demonstrating questionable judgment throughout his political life. Barilaro’s attempt to take over from Josh Landis should be consigned to the same rubbish bin that holds his bid for appointment as NSW trade commissioner to New York. Kim Woo, Mascot\\n I read the article about John Barilaro’s utterly unconscionable behaviour, which effectively deprived most of the Labor electorates of their emergency bushfire funding, with smoke coming out of my ears (“Barilaro’s office in fire funding storm”, February 3). The admitted pork barrelling by this government was a vote-buying strategy. Such reprehensible action needs way more than the usual slap on the wrist, so it’ll be interesting to see what happens next. Anne Ring, Coogee\\n To exacerbate people’s misery, and politicise it, is the lowest of the low. The relief recovery program is there to ensure all affected by the Black Summer fires get assistance. Barilaro shows us – again and again – how little compassion he has for the people of NSW. Daniela Catalano, Haberfield\\n Sadly, the revelations show two things: first, that the egregious former minister for John Barilaro appears ready to continue to serve the people of NSW in any capacity – provided it has a six-figure salary attached. Second, that while in office he didn’t have the nous to understand how pork-barrelling works. The idea is to get people to vote for you, so it is logically directed at seats you don’t have, rather than ones you do. Unless, of course, he realised the Coalition was (is) so on the nose that it needed to shore-up those seats it already held. Or maybe both of these cases are true. Greg Oehm, Robertson\\n Why is there no punishment for pork barrelling? In public life, fraudulent behaviour is punishable under law. For politicians, it’s business as usual. Forget about improved sporting facilities, safer car parks and appropriate disaster relief. So long as these actions come without consequence, nothing will change. Sam Kent, Hunters Hill\\n I pondered long and hard in an attempt to predict what Barilaro might include in his job application for ClubsNSW. All I could dredge up were greyhounds, koalas, brumbies, threats to blow up the government, overseas trade appointments and, then, Thursday’s revelation of his interference in the bushfire recovery program. When will our failed politicians realise that they are unemployable? Even by ClubsNSW. Rhonda Seymour, Castle Hill\\n Quite appropriate really – a man who directed desperately needed bushfire recovery funds away from ALP seats now wants to run an organisation that exists on the back of problem gamblers and illegal money laundering. Sounds like the ethics will be a perfect fit. Michael McMullan, Avoca Beach\\n Is it any wonder that Barilaro denied bushfire recovery funding to the Blue Mountains? Apart from it being a Labor seat, he once referred to koalas as tree rats. The man has no love of the bush. Sally Spurr, Lane Cove\\n The former deputy premier is reportedly proud of his nickname Pork Barilaro. His arrogance is breathtaking, and the revelation that the joint federal and state emergency bushfire funds were distributed on partisan grounds under his direction is contemptuous. Why am I not surprised that he has now put his hand up for the recently vacated ClubsNSW job? His intervention in the emergency bushfire recovery program needs to be immediately reviewed by ICAC. Peter Neufeld, Mosman\\n I suggest that regardless of which party wins in March it considers appointing a Chief Pub Tester for major policy and decision-making. The beauty of the role is that it could be filled by 99 per cent of the population, and the only equipment the chief would require is an egg timer, a bar stool and a beer or G&T. Monique Darcy, Davidson\\n The integrity and transparency of the Perrottet government can be summed up in one word: Barilaro. Michael Petras, Thornleigh\\n It’s hard to decide whether I feel utter contempt for the cynical interference in disaster relief or just bewilderment at such delusional ambition? Has Barilaro considered putting his hand up for POTUS in 2024? Phil Bradshaw, Naremburn\\n Good old “Pork Barilaro”, the gift that keeps giving to NSW Labor. Rob Phillips, North Epping\\n I’m buying up popcorn. The Barilaro ICAC will be a corker. Dorothy Kamaker, Whale Beach\\n Divisive Pell not a martyr, and no saint Tony Abbott called Cardinal George Pell “a saint of our times” and a victim of a modern-day crucifixion (“As Pell the man is laid to rest, Pell the martyr rises”, February 3). As I understand from my 13 years of Catholic education, Jesus was crucified for threatening and challenging the entrenched power structure and calcified ideologies and attitudes of the church hierarchy, not for propping it up. Monica Oppen, Stanmore \\n Abbott’s grasp of the requirements for sainthood and of the array of great Catholics that Australia has produced helps me to understand even further his time as prime minister. Sister Susan Connelly, Lakemba \\n My condolences to anyone praying for any ongoing meaningful contribution from Abbott to Australia. Ben Dryza, Newtown \\n Abbott said “George Pell was the greatest man I’ve ever known”. Yet no knighthood. Mark Kilminster, Castle Hill \\n “Warrior”, “soldier of truth”, “saint”, “hero”. Gandhi? Nelson Mandela? Martin Luther King? No. George Pell. Seriously? Julia Booth, Westleigh \\n By far the most significant comment at the funeral was Abbott’s; that Pell should never have been investigated without a complaint; he should never have been charged without corroborating evidence, and should never have been convicted without a plausible case, as the High Court eventually realised. Laurie Le Claire, Epping \\n Such delusions of grandeur. The deliberately divisive Pell was no more a saint than Abbott was a leader. Anne Garvan, Chatswood West\\n The last time I checked, Saint Mary Mackillop is clearly more highly revered than a late cardinal. Saint Mary Mackillop is the greatest Catholic Australia has ever produced. Zara Tai, Minchinbury\\n The question is, has Pell travelled northwards or southwards? David Boyd, Bondi Beach\\n Pell incites strong opinions among his supporters and haters, even in death. Let us all move on with his passing, but may the Catholic Church move with integrity, compassion and justice. If the Church always tries to put the institution above the people, the institution, even after 2000 years, will not survive. Vincent Wong, Killara\\n Dear Tony Abbott,How dare you turn a wilful blind eye to Cardinal Pell’s own wilful and well documented blind eye to the abuse of children. Sue Burton, Mosman\\n The man Abbott describes as the “greatest I’ve ever known” was someone found by a royal commission to have protected paedophiles. What does this say about Abbott?\\n Still, it’s no surprise to see a conga line of Liberal luminaries now waxing so lyrical about Cardinal Pell. In Pell they found a fellow right wing political operative par excellence, who brought the power and prestige of the Church to the project of reactionary conservatism. Michael Hinchey, New Lambton \\n Clearly, Abbott lives in a parallel universe to most of us. Peter Kamenyitzky, Castle Hill \\n Interesting to see the fawning adoration by Tony Abbott at the memorial service for George Pell. Including of course that he was a victim of a media witch hunt against him and the Catholic Church in general. George Pell presided over an administration that did little to redress the victims of sexual abuse nor did it acknowledge what future safeguards have been put in place. His administration was a failure. His only interest was protecting the Church and its money. His line at the royal commission, “It was of little interest to me”, perhaps shows the true colours of the man. Stephen Trevarrow, New Farm (Qld)\\n How do we reconcile Tony Abbott’s proclamation that Cardinal George Pell is a “saint for our times” with Pell’s own statement that multiple claims of child sexual abuse perpetrated by Catholic priests under his watch were of not much interest to him? The man is hardly a martyr if the definition is “a person who voluntarily suffers death as the penalty of witnessing to and refusing to renounce a religion”. Meg Pickup, Ballina \\n Not one letter writer has shown even a drop of human kindness (Letters, February 3). The man, a beacon of endeavour and achievement, was an early crusader in the fight against sexual abuse in the church, subsequently deemed inadequate, but revolutionary in its time. Pell endured horrifically biased treatment from the ABC and the Victorian police, tolerated a dud rap as a criminal, and was eventually totally vindicated, yet, rather than bitterness or hatred, he displayed only his enduring faith in his God.\\n How was Pell not a giant of a man? Flawed yes, but none of us is perfect, and he didn’t ever deserve the viciousness he faced. Convicted serial killers are shown more tolerance. Rosemary O’Brien, Ashfield\\n Bernini explained that since St. Peter’s is “the mother church of nearly all the others, it had to have colonnades, which would show it as if stretching out its arms maternally to receive Catholics, so as to confirm them in their faith, heretics to reunite them to the church, and infidels, to enlighten them in the true faith.” Pell’s funeral made plain for all the church in Australia has closed its arms to the wounded, those it harmed, ignored their cries of pain, and in so doing, failed to follow Jesus’ admonition to welcome and comfort the injured and hold them close, invite them in to St Mary’s Cathedral and acknowledge its crimes against the young innocents. Instead, it denied the findings of a royal commission, denied their very existence. The beginning of the road to redemption is to first admit your sins. This week, the Catholic Church did the opposite, it said nothing happened, there were no victims and proved it is too arrogant and out of touch, it can only deny its acts by washing away the blood of the young innocent. Philip Drew, Annandale\\n Thank goodness for former PM John Howard, Tony Abbott, Peter Dutton, Matt Canavan, and Dan Tehan for standing up to witness the passing of Cardinal Pell.It is disappointing our other current political leaders were too small-minded, self-serving, and “go with the flow” to honour Pell who was used as a scapegoat, and suffered 404 days in gaol for a crime he did not commit. But he is in a better place now, thank God. Elizabeth Vickers, Maroubra\\n  Nevermind the monarchists, get the royals off our notes Does the decision to replace the late Queen’s head with an Indigenous design on a single banknote really constitute a poke in the eye to monarchists, or is it simply an attempt to acknowledge the nation’s First People on our currency (“Timing of new $5 note without King could be better”, February 3)? It seems an over reaction to a minor change that will probably go unnoticed by most of the population. After all, how many of us could accurately describe exactly what is on each banknote without looking? Merona Martin, Meroo Meadow\\n I don’t understand why the dysfunctional Windsor family are on Australia’s currency anyway. James Duggan, Hunters Hill\\n The monarchists seem so sad over the $5 note business. Why not cheer them up by having more royals on our money, reminding us colonials how great the monarchy has been. Put Charles on the fiver and Andrew on the $10. The $20 could have the former Edward VIII and Wallace. The $50 would be perfect for Henry VIII, circled by his five wives. Nothing less than the $100 note would do for Prince Harry. Richard Macey, Pendle Hill \\n I applaud the decision not to issue a $5 note with King Charles on it. They are obviously waiting until after the coronation; any balding man looks better wearing a hat or a large crown. Noel Mills, Avalon Beach\\n Better pay the best cure for GP crisis The government’s approach is novel: leave the cause untreated and see if the patient survives (“Doctor pay unaddressed in report”, February 3). When the main cause of the crisis in general practice is that the bulk-billing Medicare rebate is about half of where it should have been had it kept pace with inflation over the last 30 years – making bulk-billing economically unviable and leading medical graduates to pursue other paths where their expertise is more valued – the government seems to want to waste money on doing everything it can to avoid actually treating the cause. The prognosis isn’t good. Alan Garrity, North Narrabeen \\n A close family friend has had his very successful inner-city GP practice up for sale for three years with no buyers and has now been forced to simply shut it down. Payments to GPs have not increased markedly for many years unlike their business overheads, resulting in marginal profitability for doctors often working in excess of 60 hours a week. What a sad commentary on our modern society when highly trained and respected GPs, whose opinions often determine the length and quality of our lives, struggle to make a living while tradies earn $150 an hour and drive around in $100,000 utes. Tony Snellgrove, Tumbulgum\\n  Inequality needs new thinking Decades of neo-liberal economics along with the slow burn of fiscal austerity have driven the relentless selling-off of government instrumentalities and the ransacking of many vital government departments (“Shrill critics of Chalmers’ essay are missing the point,” February 3). It provided previous governments with the revenue to win votes through tax cuts and nepotistic funding rorts. It made winners of the well-heeled, while more of us became steadily less and less well off.\\n Inequality has now reached epidemic proportions and areas such as education, health, housing, aged care and transport are in fiscal crisis and desperate for funding. For decades, we have been sold the lie that taxes are a burden on our economy and to be feared. It is time we considered a more equitable and inclusive way of improving the wealth of our nation. Bruce Spence, Balmain\\n We now have a treasurer who writes an essay offering new ideas to create a fairer, better society for all Australians. His ideas are progressive, thought-provoking and challenging. The voices that criticise him, immediately label him a communist and a real danger to corporate Australia. The attitude by those that have, against those that have not, are predictable, archaic, self-centred and selfish. Jim Chalmers is thinking about our future not the next election, surely that’s worth consideration. Geoff Hermon, Maraylya\\n Baffling gas decision One of the few good decisions made by Scott Morrison is likely to be overturned (“Scott Morrison’s decision to stop controversial NSW gas field to be overturned”, smh.com.au, February 3). Whatever Morrison’s motives, this was a decision which went some way to address climate change concerns. It is now likely to be overturned with the help of a federal government that claims to care about climate change and global warming. The reason for this decision by the federal government is, apparently, the potential cost of compensation to the gas companies. Once again money trumps public interest.As a long-term Labor supporter I am baffled and deeply troubled by their approach to this issue. Edward Quinn, St Ives \\n The Wounded Kangaroo Alan Joyce’s perpetual apologies for Qantas failings wear thin when your QF27 luggage does not arrive in Buenos Aires three days before departing an Antarctic voyage (“Despite the hype, Qantas turnbacks are a sign of strong safety”, February 3). No apologies. No explanation. And no expedition gear. Just anxiety.\\n Unfortunately, Qantas no longer brings an offering to market which is as certain as it may be safe.\\n It’s time for a management rethink because, right now, the Spirit of Australia days are over. Any airline will do. Stephen Goddard, Bowral\\n Classroom conundrum Decisions are complex, not binary. Should single sex schools exist? It depends (“Enrolments at public boys’ high schools in steep decline”, February 3).\\n For some children they work. For others not. It is about the makeup of the individual, not social engineering. My son went to Homebush Boys High and it worked for him. Would he have done better in a co-ed school? We will never know.\\n The pendulum is swinging towards co-ed schools. I predict in a couple of decades it will start swinging the other way. Why do we think there has to be one answer? Neville Turbit, Russell Lea\\n Dutton must grow up Ossified, obsolete and irrelevant, the Liberal Party, with or without the Duttons of this world, must soon begin to transform to 21st century life or face extinction (“Dutton must pass Harbour test”, February 3). The fresh Labor government has shaped up well so far but needs a viable and intelligent opposition for our democracy to function as we all would wish. Why is no one rising to the challenge? Brian Haisman, Winmalee\\n Nick Bryant is one among many who point out the difficulties ahead for Peter Dutton as leader of the Liberal party. I wonder if a 2023 version of Don Chipp will emerge from the moderate malcontents of the party. John Bailey, Canterbury\\n Does Peter Dutton know what’s really “an attack on our society”? He plays the divide and conquer card while aiming to appeal to the tone-deaf. Trying to ignite a fire when none need to exist. It’s well past time to pay our due to our First Nations on whose land we live. It’s time to grow up. Francesca Stahlut, Armidale\\n Only stand up for a standing ovation Theatregoers standing for the curtain call ovation are a minor irritation (Letters, February 3). The real problem punters are the concertgoers who stand in their seats for half of the show. John Swanton, Coogee\\n Over the decades I have attended countless music concerts and many had the audience give a standing ovation. There was one concert where the crowd got to its feet at the end not in appreciation but to demand the band return to the stage as their performance had been so short.\\n When it appeared they had no intention of playing more, the audience took to throwing cushions and ripping seats out to throw at the stage in disgust. This was Elvis Costello’s first concert in Sydney in 1978 at the Regent theatre; his future ones were much longer. Con Vaitsas, Ashbury\\n  Looking for a three-headed emu in a haystack Good news that the radioactive pod was located in Western Australia, but I await the first appearance of a three-headed emu (Letters, February 3). Mark Berg, Caringbah South\\n Put ‘woke’ back to sleep Could we please put “woke” back to sleep. It has become another boring, pointless word that spoils a good argument or description. Kath Maher, Lidcombe\\n Postscript Kim Crawford of Springwood was one of a number of letter writers who claimed to have double-checked the date on Friday’s Herald front page. So confounding were the headlines - Cardinal George Pell described as a “martyr” and a “saint of our times”; the former NSW deputy premier eyeing off the newly vacated job of ClubsNSW chairman, and living up to his nickname of “pork Barilaro” - that correspondents thought they’d woken up on April 1.\\n George Fishman of Vaucluse was one of only a few who wrote that “whatever side you are on, the funeral oration by Tony Abbott hailing George Pell was one for the ages”. Not surprisingly, the majority of writers were scathing of the former PM’s statement that “Pell was the greatest man I’ve ever known”, many agreeing with Peter Kamenyitzky of Castle Hill that Abbott “clearly lives in a parallel universe to most of us”, while others suggested he needs to meet more men.\\n Others, like Sue Burton of Mosman, were critical of Abbott turning “a wilful blind eye to Pell’s own wilful and well documented blind eye to the abuse of children”.\\n “I respectfully suggest that John Barilaro now be awarded the title of ‘the human headline’ (with apologies to Derryn Hinch),“wrote Lorraine Hickey of Green Point. “So many juicy stories, so little time.” Phil Bradshaw of Naremburn was so bewildered at the former minister’s ambition, he wondered whether “Pork Barilaro” would consider putting his hand up for POTUS in 2024. Pat Stringa, letters editor\\n To submit a letter to The Sydney Morning Herald, email letters@smh.com.au. Click here for tips on how to submit letters. The Opinion newsletter is a weekly wrap of views that will challenge, champion and inform. Sign up here.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1df15d2-3039-49d3-ad93-17ac017709b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=len_safe_get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d3d97bb-cf37-41f2-a198-c3862acb24ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2048, 2048, 387]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "080d22a5-2af1-49a2-9cac-fbd476a096c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.batch_prediction import BatchPredictionJob\n",
    "for job in BatchPredictionJob.list():\n",
    "    print(job.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9686700-f084-46b6-b44a-d7d9297f49a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b2439ab8-533e-4352-8030-08b6e9581de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 13) (2251090377.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[289], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Job ID: {job.job_id}, State: {job.state}, Created: {job.created},  Ended: {job.ended}   )\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 13)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "# List jobs for the project\n",
    "jobs = client.list_jobs()\n",
    "\n",
    "for job in jobs:\n",
    "    # try:\n",
    "    #     print(job.statement_type)\n",
    "    # except:\n",
    "    #     continue\n",
    "    if (job.job_type == 'query' and job.statement_type == 'ML.GENERATE_EMBEDDING') or (job.job_type == 'query' and 'ML.GENERATE_EMBEDDING' in job.query):\n",
    "        print(f\"Job ID: {job.job_id}, State: {job.state}, Created: {job.created},  Ended: {job.ended}   )\n",
    "              break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17da75bd-1d51-46d3-b85f-f8ac8984113f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'project_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 334\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_prompt_count_limit\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    331\u001b[0m     request_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_prompt_count_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_prompt_count_limit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 334\u001b[0m \u001b[43mchunk_bq_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[85], line 191\u001b[0m, in \u001b[0;36mchunk_bq_content\u001b[0;34m(request_args)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m         \u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m status\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m   \n\u001b[0;32m--> 191\u001b[0m project_id\u001b[38;5;241m=\u001b[39m  \u001b[43mrequest_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproject_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    192\u001b[0m dataset_id\u001b[38;5;241m=\u001b[39m  request_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    193\u001b[0m table\u001b[38;5;241m=\u001b[39m request_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'project_id'"
     ]
    }
   ],
   "source": [
    "import google.cloud.bigquery as bq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "import os, json\n",
    "   \n",
    "def create_dataset(project_id,dataset_id,region_id):\n",
    "    \"\"\"\n",
    "        Create a dataset\n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str region_id:  name of the region under whcih the dataset should be created.               \n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    print(project_id)\n",
    "    # Check if the dataset exists\n",
    "    try:\n",
    "        dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "    except :\n",
    "        # If the dataset does not exist, create it\n",
    "        \n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "        dataset.location = region_id # Set your desired location\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = client.create_dataset(dataset)  # Make an API request.\n",
    "        print(f\"Dataset '{dataset_id}' created successfully.\")\n",
    "        \n",
    " \n",
    "def create_table(project_id,dataset_id,table_id):\n",
    "    \n",
    "    \"\"\"\n",
    "        Create a table \n",
    "        \n",
    "        Args:\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           str table_id:  name of the table.\n",
    "        Returns:\n",
    "            list[bigquery.SchemaField] schema: table schema- list of columns\n",
    "             \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    \n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"request_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        #bigquery.SchemaField(\"original_content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"combined_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "       # bigquery.SchemaField(\"media_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        #bigquery.SchemaField(\"path\", \"STRING\", mode=\"NULLABLE\"),\n",
    "       # bigquery.SchemaField(\"test_metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"chunk\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"process_time\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "                \n",
    "\n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        table = client.get_table(full_table_id)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' already exists.\")\n",
    "        \n",
    "        # Drop the table if exist\n",
    "        query = f\"DROP TABLE `{full_table_id}`\"\n",
    "\n",
    "        # Execute the query\n",
    "        try:\n",
    "            client.query(query).result()  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' dropped successfully.\")\n",
    "            \n",
    "            # recreate the table\n",
    "            table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "            # Create the table\n",
    "            table = client.create_table(table)  # Make an API request.\n",
    "            print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping/ recreating table '{full_table_id}': {e}\")\n",
    "    except :\n",
    "        # If the table does not exist, create it\n",
    "        table = bigquery.Table(full_table_id, schema=schema)\n",
    "\n",
    "        # Create the table\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(f\"Table '{full_table_id}' created successfully.\")\n",
    "        \n",
    "    return schema\n",
    "\n",
    "\n",
    "def load_and_split_docs (source_query_str: str,project_id: str, metadata_columns: list[str], page_content_columns: list[str], chunk_size: int,chunk_overlap: int ):\n",
    "    \n",
    "    \"\"\"\n",
    "        Load data from biquery table and chunk them \n",
    "        \n",
    "        Args:\n",
    "           str source_query_str: source query string\n",
    "           str project_id: project id\n",
    "           list[str] metadata_columns: list of columns from the source table \n",
    "           list[str] page_content_columns:  list of page content columns from the source table\n",
    "           int chunk_size: chunk size in character\n",
    "           int chunk_overlap: chunk overlap in character\n",
    "           \n",
    "        Returns:\n",
    "            list[documents] doc_splits: list of splitted documents\n",
    "             \n",
    "    \"\"\"\n",
    "    \n",
    "      # Load the data\n",
    "    loader = BigQueryLoader(\n",
    "        query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "    print(f\"Data Loaded from source - {source_query_str}\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "       #separators=separators,\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)   \n",
    "    \n",
    "    return doc_splits\n",
    "\n",
    "def load_into_bq(table_prefix: str,project_id: str,dataset_id: str, region: str, data: list,  version: int):  \n",
    "    \"\"\"\n",
    "        Create a job to load data into big query table and wait for the job to finish \n",
    "        \n",
    "        Args:\n",
    "           str table_prefix: the prefix of biquery table name\n",
    "           str project_id: project id\n",
    "           str dataset_id: name of the dataset under which the table should be created.\n",
    "           list(json) data: list of json records that should be inserted into table.\n",
    "     \n",
    "             \n",
    "    \"\"\"    \n",
    "    client = bigquery.Client(project_id)    \n",
    "   \n",
    "    \n",
    "    #create table new if does not exist\n",
    "    table=f\"{table_prefix}_{version}\"\n",
    "    table_schema=create_table(project_id,dataset_id,table)\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema = table_schema\n",
    "    job = client.load_table_from_json(data, table, job_config = job_config)\n",
    "              \n",
    "    #wait for the job to finish\n",
    "    job.result()\n",
    "    # Get job status   \n",
    "                \n",
    "    if job.state == 'DONE':\n",
    "        if job.error_result:\n",
    "             print(f\"Job {job.job_id} for {table_id} failed with error: {job.error_result}\")\n",
    "             raise Exception(\"Sorry, no numbers below zero\")\n",
    "        else:\n",
    "             print(f\"Job {job.job_id} completed successfully. For \"+ table_id)\n",
    "    else:\n",
    "            print(f\"Job {job.job_id} for {table_id} is still in progress.\")\n",
    "                \n",
    "\n",
    "\n",
    "def chunk_bq_content(request_args):\n",
    "    \"\"\"\n",
    "        Chunks the combination of page_content_columns from a given bigquery source string and load the result into bigquery\n",
    "             \n",
    "    \"\"\"\n",
    "    status=''   \n",
    "\n",
    "\n",
    "    project_id=  request_args['project_id']\n",
    "    dataset_id=  request_args['dataset']\n",
    "    table= request_args['table']\n",
    "    region= request_args['region']\n",
    "    metadata_columns= [col.strip() for col in  str(request_args['metadata_columns']).split(',') ]\n",
    "    article_page_content_columns= [col.strip() for col in str(request_args['article_page_content_columns']).split(',') ]\n",
    "    video_page_content_columns= [col.strip() for col in str(request_args['video_page_content_columns']).split(',') ]\n",
    "    image_page_content_columns= [col.strip() for col in str(request_args['image_page_content_columns']).split(',') ]\n",
    "    article_source_query_str= request_args['article_source_query_str']\n",
    "    video_source_query_str= request_args['video_source_query_str']\n",
    "    image_source_query_str= request_args['image_source_query_str']            \n",
    "    #separators= \"\\n\" if str(request_args['separators'])==\"\" else str(request_args['separators']).split(',') \n",
    "    chunk_size= 1000 if str(request_args['chunk_size']) in [\"None\",\"\"] else int(str(request_args['chunk_size']))  \n",
    "    chunk_overlap=100 if str(request_args['chunk_overlap']) in [\"None\",\"\"] else int(str(request_args['chunk_overlap'])) \n",
    "    max_prompt_count_limit=20000 if str(request_args['max_prompt_count_limit']) in [\"None\",\"\"] else int(str(request_args['max_prompt_count_limit'])) \n",
    "  \n",
    "\n",
    "    #load and split articles       \n",
    "    doc_splits= load_and_split_docs(article_source_query_str,project_id, metadata_columns, article_page_content_columns, chunk_size,chunk_overlap )       \n",
    "    print(f\"Article contents loaded and splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\\n\")\n",
    " \n",
    "    #load and split videos    \n",
    "    doc_splits= doc_splits+ load_and_split_docs(video_source_query_str,project_id, metadata_columns, video_page_content_columns, chunk_size,chunk_overlap )        \n",
    "    print(f\"Video contents loaded and splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\\n\")\n",
    "   \n",
    "    #load and split images \n",
    "    doc_splits= doc_splits+ load_and_split_docs(image_source_query_str,project_id, metadata_columns, image_page_content_columns, chunk_size,chunk_overlap )        \n",
    "    print(f\"Image contents loaded and splitted - chunk_size {chunk_size}, chunk_overlap {chunk_overlap}\\n\")\n",
    "\n",
    "    \n",
    "    # Add chunk number to metadata\n",
    "    chunk_idx=0\n",
    "    prev=doc_splits[0].metadata[metadata_columns[0]]\n",
    "    rows_to_insert=[]\n",
    "    #request_date=datetime.today().strftime('%Y_%m_%d') \n",
    "    now = datetime.now()    \n",
    "    \n",
    "    #create data set if does not exist\n",
    "    create_dataset(project_id,dataset_id,region)\n",
    "   \n",
    "    max_index=max_prompt_count_limit #maximum number of requests in a batch\n",
    "    record_count=0\n",
    "    prefix=f\"{table}\" \n",
    "    job_list=[]\n",
    "    job_execution_result={}\n",
    "    #laod data into bigquery table\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "            split.metadata[\"process_time\"]=now\n",
    "            if prev==split.metadata[metadata_columns[0]]:\n",
    "               split.metadata[\"chunk\"] = chunk_idx      \n",
    "            else:\n",
    "                chunk_idx=0\n",
    "                split.metadata[\"chunk\"] = chunk_idx\n",
    "                prev=split.metadata[metadata_columns[0]]\n",
    "                \n",
    "            chunk_idx +=1\n",
    "            version=idx // max_index\n",
    "            request_id = prefix+'_'+str(version)\n",
    "            \n",
    "            if chunk_idx==1:                \n",
    "                 content=split.page_content  \n",
    "            else:                  \n",
    "                  content=\"Content\"+\": \"+split.page_content \n",
    "                \n",
    "            rows_to_insert.append(\n",
    "                               {  \"request_id\":  request_id  , \n",
    "                                   \"combined_id\": split.metadata[metadata_columns[0]], \n",
    "                                   \"process_time\":split.metadata[\"process_time\"].isoformat(),\n",
    "                                   \"content\": content,#.replace(page_content_columns[0]+\":\", \"\", 1).strip(),\n",
    "                                   #\"original_content\": split.metadata[\"content\"],\n",
    "                                   \"chunk\": split.metadata[\"chunk\"],\n",
    "                                   #\"media_type\": split.metadata[\"media_type\"],\n",
    "                                   #\"path\": split.metadata[\"path\"],\n",
    "                                   #\"test_metadata\": split.metadata[\"test_metadata\"]                            \n",
    "\n",
    "                                  }\n",
    "                                         )\n",
    "            \n",
    "            if (idx+1) % max_index==0:  \n",
    "                load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "                #moving to next batch\n",
    "                record_count=record_count+len(rows_to_insert)\n",
    "                rows_to_insert=[]   \n",
    "               \n",
    "                \n",
    "    if  len(rows_to_insert)>0:       \n",
    "        load_into_bq(prefix,project_id,dataset_id, region, rows_to_insert,version)\n",
    "        #moving to next batch\n",
    "        record_count=record_count+len(rows_to_insert)\n",
    "        rows_to_insert=[]   \n",
    "                \n",
    "    \n",
    "  \n",
    "    return   {'status':'SUCCESS', 'record_count':record_count,'count_of_tables':version+1 }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "\n",
    "        request_args={}\n",
    "        if  'project_id' in os.environ:\n",
    "            request_args['project_id']= os.environ.get('project_id')\n",
    "            \n",
    "        if  'dataset' in os.environ:\n",
    "            request_args['dataset']= os.environ.get('dataset')\n",
    "            \n",
    "        if  'table' in os.environ:\n",
    "            request_args['table']= os.environ.get('table')\n",
    "            \n",
    "        if  'region' in os.environ:\n",
    "            request_args['region']= os.environ.get('region')\n",
    "            \n",
    "        if  'metadata_columns' in os.environ:\n",
    "            request_args['metadata_columns']= os.environ.get('metadata_columns')\n",
    "            \n",
    "        if  'article_page_content_columns' in os.environ:\n",
    "            request_args['article_page_content_columns']= os.environ.get('article_page_content_columns')\n",
    "            \n",
    "        if  'video_page_content_columns' in os.environ:\n",
    "            request_args['video_page_content_columns']= os.environ.get('video_page_content_columns')\n",
    "        \n",
    "        if  'image_page_content_columns' in os.environ:\n",
    "            request_args['image_page_content_columns']= os.environ.get('image_page_content_columns')\n",
    "            \n",
    "        if  'article_source_query_str' in os.environ:\n",
    "            request_args['article_source_query_str']= os.environ.get('article_source_query_str')\n",
    "            \n",
    "        if  'video_source_query_str' in os.environ:\n",
    "            request_args['video_source_query_str']= os.environ.get('video_source_query_str')\n",
    "           \n",
    "        if  'image_source_query_str' in os.environ:\n",
    "            request_args['image_source_query_str']= os.environ.get('image_source_query_str')\n",
    "            \n",
    "        if  'chunk_size' in os.environ:\n",
    "            request_args['chunk_size']= os.environ.get('chunk_size')\n",
    "            \n",
    "        if  'chunk_overlap' in os.environ:\n",
    "            request_args['chunk_overlap']= os.environ.get('chunk_overlap')\n",
    "            \n",
    "        if  'max_prompt_count_limit' in os.environ:\n",
    "            request_args['max_prompt_count_limit']= os.environ.get('max_prompt_count_limit')\n",
    "\n",
    "    \n",
    "        chunk_bq_content(request_args)\n",
    "            \n",
    "          \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba9da94f-031a-4cd3-99ab-9b9df566a66a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe this video from period --- seconds to --- seconds.\n",
      " Focus on key themes, what people are talking about, names of people who appeared and who are discussed and key locations. Highlight any brands, company names or logos that you see. Give me at least 4000 words describing it.  Do not add any extra text to the description. Organize the description in the following format: \n",
      "**Category**\n",
      " \n",
      "**DetailedDescriptionOfEventsAndConversations**\n",
      " \n",
      "**BrandsCompanyNamesLogos**\n",
      " \n",
      "**KeyLocationsAndScenes**\n",
      " \n",
      "**KeyThemes**\n",
      " \n",
      "**PeopleAppearingAndMentioned**\n",
      " When describing the DetailedDescriptionOfEventsAndConversations, consider the following instructions for specific video types: * **News:** Pay close attention to transitions, graphics, and on-screen text. * **TV Shows:** Describe facial expressions, body language, appearances, and overall mood. * **Live Sports Events:** Focus on key moments, like goals or fouls, and describe the overall flow and momentum of the game. * **News Analyses:** Identify different perspectives, arguments, and supporting evidence.\n"
     ]
    }
   ],
   "source": [
    "print(\"Describe this video from period --- seconds to --- seconds.\\n Focus on key themes, what people are talking about, names of people who appeared and who are discussed and key locations. Highlight any brands, company names or logos that you see. Give me at least 4000 words describing it.  Do not add any extra text to the description. Organize the description in the following format: \\n**Category**\\n \\n**DetailedDescriptionOfEventsAndConversations**\\n \\n**BrandsCompanyNamesLogos**\\n \\n**KeyLocationsAndScenes**\\n \\n**KeyThemes**\\n \\n**PeopleAppearingAndMentioned**\\n When describing the DetailedDescriptionOfEventsAndConversations, consider the following instructions for specific video types: * **News:** Pay close attention to transitions, graphics, and on-screen text. * **TV Shows:** Describe facial expressions, body language, appearances, and overall mood. * **Live Sports Events:** Focus on key moments, like goals or fouls, and describe the overall flow and momentum of the game. * **News Analyses:** Identify different perspectives, arguments, and supporting evidence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3751d9d-12b2-4ffd-ba2c-814c7d6f1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from google.cloud import iam_admin_v1\n",
    "from google.cloud.iam_admin_v1 import types\n",
    "\n",
    "\n",
    "def list_service_accounts(project_id: str) -> List[iam_admin_v1.ServiceAccount]:\n",
    "    \"\"\"\n",
    "    Get list of project service accounts.\n",
    "\n",
    "    project_id: ID or number of the Google Cloud project you want to use.\n",
    "    \"\"\"\n",
    "\n",
    "    iam_admin_client = iam_admin_v1.IAMClient()\n",
    "    request = types.ListServiceAccountsRequest()\n",
    "    request.name = f\"projects/{project_id}\"\n",
    "\n",
    "    accounts = iam_admin_client.list_service_accounts(request=request)\n",
    "    return accounts.accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5aa75f2-9085-48ff-981f-96dc16c550e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"projects/nine-quality-test/serviceAccounts/494586852359-compute@developer.gserviceaccount.com\"\n",
       "project_id: \"nine-quality-test\"\n",
       "unique_id: \"117381591874063009905\"\n",
       "email: \"494586852359-compute@developer.gserviceaccount.com\"\n",
       "display_name: \"Compute Engine default service account\"\n",
       "etag: \"01021920\"\n",
       "oauth2_client_id: \"117381591874063009905\"\n",
       ", name: \"projects/nine-quality-test/serviceAccounts/nine-quality-test@appspot.gserviceaccount.com\"\n",
       "project_id: \"nine-quality-test\"\n",
       "unique_id: \"113671129977640505121\"\n",
       "email: \"nine-quality-test@appspot.gserviceaccount.com\"\n",
       "display_name: \"App Engine default service account\"\n",
       "etag: \"01021920\"\n",
       "oauth2_client_id: \"113671129977640505121\"\n",
       ", name: \"projects/nine-quality-test/serviceAccounts/workflowrunner@nine-quality-test.iam.gserviceaccount.com\"\n",
       "project_id: \"nine-quality-test\"\n",
       "unique_id: \"102292657621919090032\"\n",
       "email: \"workflowrunner@nine-quality-test.iam.gserviceaccount.com\"\n",
       "display_name: \"WorkflowRunner\"\n",
       "etag: \"01021920\"\n",
       "description: \"service account to run workflow\"\n",
       "oauth2_client_id: \"102292657621919090032\"\n",
       ", name: \"projects/nine-quality-test/serviceAccounts/functionrunner@nine-quality-test.iam.gserviceaccount.com\"\n",
       "project_id: \"nine-quality-test\"\n",
       "unique_id: \"101307792552361059252\"\n",
       "email: \"functionrunner@nine-quality-test.iam.gserviceaccount.com\"\n",
       "display_name: \"functionrunner\"\n",
       "etag: \"01021920\"\n",
       "description: \"Service account to run cloud functions\"\n",
       "oauth2_client_id: \"101307792552361059252\"\n",
       ", name: \"projects/nine-quality-test/serviceAccounts/vertex-ai-caller@nine-quality-test.iam.gserviceaccount.com\"\n",
       "project_id: \"nine-quality-test\"\n",
       "unique_id: \"116081603469594577885\"\n",
       "email: \"vertex-ai-caller@nine-quality-test.iam.gserviceaccount.com\"\n",
       "display_name: \"Cloud Run to access Vertex AI APIs\"\n",
       "etag: \"01021920\"\n",
       "oauth2_client_id: \"116081603469594577885\"\n",
       "]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_service_accounts('nine-quality-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c647527a-7d7c-40a7-acb8-97bd045bc7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `gcloud projects get-iam-policy $(gcloud config get-value project) --flatten=bindings[].members --format=table(bindings.role,bindings.members) --filter=bindings.members:service-*@gcp-sa-aiplatform.iam.gserviceaccount.com'\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects get-iam-policy $(gcloud config get-value project) --flatten=bindings[].members --format=table(bindings.role,bindings.members) --filter=bindings.members:service-*@gcp-sa-aiplatform.iam.gserviceaccount.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ddd756b8-35e5-4366-a475-bd665bdffb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `--flatten=bindings[].members'\n",
      "/bin/bash: -c: line 1: `(gcloud config get-value project) --flatten=bindings[].members --format=table(bindings.role,bindings.members) --filter=bindings.members:service-*@gcp-sa-aiplatform.iam.gserviceaccount.com'\n"
     ]
    }
   ],
   "source": [
    "!(gcloud config get-value project) --flatten=bindings[].members --format=table(bindings.role,bindings.members) --filter=bindings.members:service-*@gcp-sa-aiplatform.iam.gserviceaccount.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3285358-47c4-4e9b-97cb-9b5ad7acf60b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform.googleapis.com            Vertex AI API\n"
     ]
    }
   ],
   "source": [
    "! gcloud services list --enabled | grep aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e41a7eb9-1b5c-42e3-a0ae-2c38dae9c746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY NAME                            EMAIL                                                       DISABLED\n",
      "Compute Engine default service account  494586852359-compute@developer.gserviceaccount.com          False\n",
      "App Engine default service account      nine-quality-test@appspot.gserviceaccount.com               False\n",
      "WorkflowRunner                          workflowrunner@nine-quality-test.iam.gserviceaccount.com    False\n",
      "functionrunner                          functionrunner@nine-quality-test.iam.gserviceaccount.com    False\n",
      "Cloud Run to access Vertex AI APIs      vertex-ai-caller@nine-quality-test.iam.gserviceaccount.com  False\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ffcd076-e869-47d7-83ba-f4ca8f063a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY NAME                            EMAIL                                                       DISABLED\n",
      "Compute Engine default service account  494586852359-compute@developer.gserviceaccount.com          False\n",
      "App Engine default service account      nine-quality-test@appspot.gserviceaccount.com               False\n",
      "WorkflowRunner                          workflowrunner@nine-quality-test.iam.gserviceaccount.com    False\n",
      "functionrunner                          functionrunner@nine-quality-test.iam.gserviceaccount.com    False\n",
      "Cloud Run to access Vertex AI APIs      vertex-ai-caller@nine-quality-test.iam.gserviceaccount.com  False\n"
     ]
    }
   ],
   "source": [
    "! gcloud iam service-accounts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5f8a7157-6d49-4b50-aee3-ffe228c0f830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.iam.service-accounts.get-iam-policy) PERMISSION_DENIED: Permission 'iam.serviceAccounts.getIamPolicy' denied on resource (or it may not exist). This command is authenticated as 494586852359-compute@developer.gserviceaccount.com which is the active account specified by the [core/account] property.\n",
      "- '@type': type.googleapis.com/google.rpc.ErrorInfo\n",
      "  domain: iam.googleapis.com\n",
      "  metadata:\n",
      "    permission: iam.serviceAccounts.getIamPolicy\n",
      "  reason: IAM_PERMISSION_DENIED\n"
     ]
    }
   ],
   "source": [
    "! gcloud iam service-accounts get-iam-policy service-301679871630@gcp-sa-aiplatform.iam.gserviceaccount.com --format=json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "69909c9c-0b44-4fa4-8eb1-bf77ef372fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "from vertexai.vision_models import Image, Video, MultiModalEmbeddingModel\n",
    "\n",
    "text_embedding_model=\"textembedding-gecko\"\n",
    "multimodal_embedding_model=\"multimodalembedding@002\"\n",
    "\n",
    "def embed_text(query) -> list[list[float]]:\n",
    "    \"\"\"Embeds texts with a pre-trained, foundational model.\n",
    "    Args:\n",
    "    str query: the query that should be converted to text embedding\n",
    "    Returns:\n",
    "        A list of lists containing the embedding vectors for each input text\n",
    "    \"\"\"\n",
    "  \n",
    "    # The task type for embedding. Check the available tasks in the model's documentation.\n",
    "    task = \"RETRIEVAL_DOCUMENT\"\n",
    "\n",
    "    model = TextEmbeddingModel.from_pretrained(text_embedding_model)\n",
    "    inputs = [TextEmbeddingInput(query, task)]\n",
    "    #kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
    "    embeddings = model.get_embeddings(inputs)\n",
    " \n",
    "    # Example response:\n",
    "    # [[0.006135190837085247, -0.01462465338408947, 0.004978656303137541, ...], [0.1234434666, ...]],\n",
    "    return [embedding.values for embedding in embeddings]\n",
    "\n",
    "\n",
    "\n",
    "def embed_multimodal(query,image,video) -> list[list[float]]:\n",
    "    \"\"\"Embeds texts with a pre-trained, foundational model.\n",
    "    Args:\n",
    "    str query: the query that should be converted to text embedding\n",
    "    64bytestream image: the query in image format converted to 64bytestream or path -->talk to Mason\n",
    "    64bytestream video: the query in video format converted to 64bytestream or path-->talk to Mason\n",
    "    \n",
    "    Returns:\n",
    "        A list of lists containing the embedding vectors for each input text\n",
    "    \"\"\"\n",
    "     \n",
    "    model = MultiModalEmbeddingModel.from_pretrained(multimodal_embedding_model)\n",
    "    #if image path given\n",
    "    # image = Image.load_from_file(\n",
    "    # \"gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png\"\n",
    "    #  )\n",
    "    #if not, we have to convert the image to 64bytestream\n",
    "\n",
    "    \n",
    "     #if video path given\n",
    "     # video = Video.load_from_file(\n",
    "    # \"gs://cloud-samples-data/vertex-ai/llm/prompts/landmark1.png\"\n",
    "    #  )\n",
    "    #if not, we have to convert the video to 64bytestream\n",
    "    \n",
    "    if image: \n",
    "    embeddings = model.get_embeddings(\n",
    "    image=image,\n",
    "    #contextual_text=\"Colosseum\"#,\n",
    "    #dimension=embedding_dimension,\n",
    "    )\n",
    "    if video: \n",
    "    embeddings = model.get_embeddings(\n",
    "    video=video,\n",
    "    #contextual_text=\"Colosseum\"#,\n",
    "    #dimension=embedding_dimension,\n",
    "    )\n",
    "    if text: \n",
    "    embeddings = model.get_embeddings(\n",
    "    contextual_text=query,\n",
    "    #contextual_text=\"Colosseum\"#,\n",
    "    #dimension=embedding_dimension,\n",
    "    )\n",
    "  \n",
    "    return embeddings.image_embedding,embeddings.text_embedding,embeddings.video_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5011f-1928-464c-8d55-e03e995e60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_nearest_neighbors(query_embedding, table_name, top_k=50):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery.\"\"\"\n",
    " \n",
    "\n",
    "    # SQL for finding the nearest neighbors using cosine similarity\n",
    "    sql = f\"\"\"\n",
    "    WITH query_embedding AS (\n",
    "        SELECT {json.dumps(query_embedding)} AS embedding\n",
    "    ),\n",
    "    similarity_scores AS (\n",
    "        SELECT \n",
    "            text,\n",
    "            embedding,\n",
    "            (SUM(a * b) / (SQRT(SUM(POW(a, 2))) * SQRT(SUM(POW(b, 2))))) AS cosine_similarity\n",
    "        FROM\n",
    "            `your_project-id.your_dataset.{table_name}`,\n",
    "            UNNEST(embedding) AS a WITH OFFSET AS idx,\n",
    "            UNNEST(query_embedding.embedding) AS b WITH OFFSET AS query_idx\n",
    "        GROUP BY text, embedding\n",
    "    )\n",
    "    SELECT text, cosine_similarity\n",
    "    FROM similarity_scores\n",
    "    ORDER BY cosine_similarity DESC\n",
    "    LIMIT {top_k}\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "\n",
    "    return [(row.text, row.cosine_similarity) for row in results]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
