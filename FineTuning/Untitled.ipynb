{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab31c62-609f-4913-ad5c-9a79f902dfbc",
   "metadata": {},
   "source": [
    "### reference\n",
    "https://partner.cloudskillsboost.google/course_templates/948/video/485086\n",
    "\n",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings--> tune text embedding\n",
    "\n",
    "https://partner.cloudskillsboost.google/course_templates/948/video/485091--> tune text embedding\n",
    "\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python--> tune gemini\n",
    "\n",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning--> tunning gemini\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b0aa0-abf0-4336-9e78-c20546c04c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --user google-cloud-aiplatform umap-learn tqdm pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db04d67-94f0-4953-ac89-bb5778263971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part as GenerativeModelPart,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce33910-1baa-440f-873a-79ff2346f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('testdata.csv',encoding='latin1')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4310a25-66e8-45b5-b6eb-5f0b9eca48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import tempfile, shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from google.cloud import bigquery \n",
    "import os \n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "\n",
    "def upload_file(request_file : tempfile,dest_bucket_name:str =None,request_file_folder: str =None,request_file_prefix: str =None, version: int=0, request_file_post_fix : str=\"\"):\n",
    "\n",
    "    \"\"\"upload file into gcs\n",
    "   \n",
    "        Args:\n",
    "            tempfile request_file: request file\n",
    "            str dest_bucket_name:  name of destination bucket\n",
    "            str request_file_folder: name of the destination folder name to write files to\n",
    "            list request_file_prefix: prefix of request file name\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    temp=request_file\n",
    "    client = storage.Client()\n",
    "    # Extract name to the temp file\n",
    "    temp_file = \"\".join([str(temp.name)])\n",
    "    # Uploading the temp image file to the bucket\n",
    "    dest_filename = f\"{request_file_folder}/\"+request_file_prefix+'_'+request_file_post_fix+'_'+str(version)+\".json\" \n",
    "    dest_bucket = client.get_bucket(dest_bucket_name)\n",
    "    dest_blob = dest_bucket.blob(dest_filename)\n",
    "    dest_blob.upload_from_filename(temp_file)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a7ade-f64d-48fb-8cdf-8e76a74c0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "rf= open(request_file.name, \"a\") \n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    data_items=[ json.dumps( {  \"_id\": idx,\n",
    "                                \"title\":row['title'],\n",
    "                                 \"text\":row['content']                                          \n",
    "                               }\n",
    "                               )  +\"\\n\"\n",
    "                ]\n",
    "\n",
    "    rf.writelines(data_items)\n",
    "    rf.flush()\n",
    "upload_file(rf,dest_bucket_name='vlt_search_and_contet_output',request_file_folder='finetunning',request_file_prefix='fine_tuning_intelia_sample', version=0, request_file_post_fix ='0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d2d00-c38f-4f41-a0c0-6864670396b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(model=\"gemini-1.5-pro\", title=\"\", text=\"\"):\n",
    "\n",
    "        metric_prompt = (f'You are an expert who is tasked with extracting some questions from a given article.\\n'\n",
    "        f'Think throughly when reading this article including both title and content. Think of questions may people may ask to know more about a data consultancy called Intelia.\\n'\n",
    "        f'Suggest 1 to 2 short questions that are important to ask when people want to explore who Intelia is and what it does.\\n'\n",
    "        f' Also suggest 1-2 labels or short expressions that are related to this articleor highlight them.\\n'\n",
    "        f' These expressions or questions should be optimized for fine tuning a text embedding model.\\n'    \n",
    "        f'Output each response on a separate line divided by a newline.\\n'\n",
    "        f'Here is the article:\\n'\n",
    "        f'\"title\": {title}\\n'\n",
    "        f'\"content\":{text}')\n",
    "\n",
    "        # set evaluation metric schema\n",
    "        metric_response_schema = {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"question_expression\": {\n",
    "                    \"type\": \"ARRAY\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"properties\": {\n",
    "                            \"query_expression\": {\"type\": \"STRING\"},\n",
    "                            \"answer\": {\"type\": \"STRING\"}\n",
    "                        },\n",
    "                        \"required\": [\"query_expression\",\"answer\"]\n",
    "                    }\n",
    "                } \n",
    "            },\n",
    "            \"required\": [\"question_expression\"],\n",
    "        }\n",
    "    \n",
    "      \n",
    "        #define a generative model as an autorator\n",
    "  \n",
    "  \n",
    "        autorater = GenerativeModel(\n",
    "            model,\n",
    "            generation_config=GenerationConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=metric_response_schema,\n",
    "            ),\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "        #generate the rating metrics as per requested measures and metric in the prompt\n",
    "        response = autorater.generate_content([metric_prompt])\n",
    "\n",
    "        response_json = {}\n",
    "\n",
    "        if response.candidates and len(response.candidates) > 0:\n",
    "            candidate = response.candidates[0]\n",
    "            if (\n",
    "                candidate.content\n",
    "                and candidate.content.parts\n",
    "                and len(candidate.content.parts) > 0\n",
    "            ):\n",
    "                part = candidate.content.parts[0]\n",
    "                if part.text:\n",
    "                    response_json = json.loads(part.text)\n",
    "\n",
    "        return response_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c30cc03-17e7-4ab8-951d-6290e430436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autorater_response(llm_model: str=\"gemini-1.5-pro\", title=\"\", text=\"\", query=\"\") -> dict:\n",
    "        \n",
    "        \"\"\"Extract evaluation metric on a AI-generated content using a AI-as-judge approach\n",
    "        \n",
    "        Args:\n",
    "        list metric_prompt: the input metric prompt parameters\n",
    "        str llm_model: evaluation model\n",
    "\n",
    "        Returns:\n",
    "        dict response_json: the evaluated metric in json format\n",
    "        \"\"\"\n",
    "            \n",
    "        # set evaluation metric schema\n",
    "        metric_response_schema = {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"score\": {\"type\": \"NUMBER\"},\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"score\", \"explanation\"],\n",
    "        }     \n",
    "        #define a generative model as an autorator\n",
    "        autorater = GenerativeModel(\n",
    "            llm_model,\n",
    "            generation_config=GenerationConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=metric_response_schema,\n",
    "            ),\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        metric_prompt= f\"\"\"\n",
    "                        # Instruction \n",
    "                        You are an expert evaluator. Your task is to evaluate the amount of the relevancy between a given question and a given article. You should first read the question carefully for analyzing the task, then read the article. Then evaluate the how relevant the given question is to the given article based on the Criteria provided in the Evaluation section below. You will assign the response a rating following the Rating Rubric and Evaluation Steps. Only choose ratings from the Rating Rubric. \n",
    "                        # Evaluation\n",
    "                        ## Metric Definition \n",
    "                        You will be assessing Relevancy, which measures the ability to find a clear and thorough answer to the given question within given artcile.\n",
    "                        ## Criteria \n",
    "                        Relevancy: Measures the ability to find a clear and thorough answer to the given question within given artcile\n",
    "                        - Category - Detailed Description Of Events And Conversations - Brands, Company Names, and Logos -\n",
    "                        Key Locations And Scenes - Key Themes - People Appearing And Mentioned Thi\n",
    "                        s AI-generated responses will be used for data retrieval. So, it has to be able to capture all the details of a scene. \n",
    "                        ## Rating Rubric \n",
    "                         0: No relevancy. The document has no connection to the question. It lacks relevant keywords, topics, or context. \n",
    "                         1: Low Relevance. The document contains some keywords but does not provide meaningful information related to the question. The context is weak or unrelated. \\n\n",
    "                         2: Moderate Relevance. The document partially addresses the question but lacks completeness, clarity, or depth. It may require inference to extract an answer.\\n\n",
    "                         3\tHigh Relevance. The document directly and clearly answers the question with specific, well-aligned information. Strong contextual alignment.\\n\n",
    "                        \n",
    "                        ## Evaluation Steps \n",
    "                        STEP 1: Assess question: Carefully read the question to underestand what it is asking. \n",
    "                        STEP 2: Readt the artcile including title and content\n",
    "                        STEP 3: Evaluate relevancy: Evaluate how much the article could be relevant to the question and if the answer can be find in this article. \n",
    "                        STEP 4: Determine relevancy score: Based on the previous steps, assign a relevancy score using the\n",
    "                        0-3 rubric.\n",
    "\n",
    "                        Here is the question and article:\n",
    "                        ## Question:\\n\n",
    "                        {query}\\n                      \n",
    "                        ## Article:\\n                    \n",
    "                        \"title\": \n",
    "                                {title}\\n\n",
    "                        \"content\":\n",
    "                                 {text}\"\"\"\n",
    "\n",
    "   \n",
    "        #generate the rating metrics as per requested measures and metric in the prompt\n",
    "        response = autorater.generate_content([metric_prompt])\n",
    "\n",
    "        response_json = {}\n",
    "\n",
    "        if response.candidates and len(response.candidates) > 0:\n",
    "            candidate = response.candidates[0]\n",
    "            if (\n",
    "                candidate.content\n",
    "                and candidate.content.parts\n",
    "                and len(candidate.content.parts) > 0\n",
    "            ):\n",
    "                part = candidate.content.parts[0]\n",
    "                if part.text:\n",
    "                    response_json = json.loads(part.text)\n",
    "\n",
    "        return response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ed000-cd88-44f9-be97-66cffe40569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_queries=[]\n",
    "request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "rf= open(request_file.name, \"a\") \n",
    "all_queries=[]\n",
    "qid=0\n",
    "for idx,row in data.iterrows():\n",
    "    queries=generate_queries(title= row['title'],text= row['content']) \n",
    "    tmp=queries.get(\"question_expression\", \"\")\n",
    "    for exp in tmp:          \n",
    "          all_queries.append(['doc'+str(idx),'q'+str(qid),exp['query_expression'],exp['answer']])\n",
    "          qid=qid+1\n",
    "    \n",
    "    print(all_queries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f09953-d12a-47c9-9be8-af152128d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with column names\n",
    "df = pd.DataFrame(all_queries, columns=[ 'corpus-id' , 'query-id' ,'query','answer'])\n",
    "df.to_csv('all_queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f126c-2b3b-401e-93e9-ba04e4e09827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc1c0f-4bdf-4860-a79a-54275d7e418e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb27322-c689-41b8-a618-abae369e8535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92678420-225c-4353-b7df-0629acb1df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b59985-611b-4c8a-80d9-f82b20038058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69b7da-870a-46d5-b1fd-5ed244397cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "rf= open(request_file.name, \"a\") \n",
    "q1=[\"What is Intelia's core philosophy and how does it impact their approach to customer success?\",  'How does Intelia balance its focus on customer delight with the well-being of its employees?',  'What specific actions does Intelia take to foster a passionate and enjoyable work environment?',  'How does Intelia leverage diversity and collaboration to achieve its goals?',  'What sets Intelia apart from other data consultancies in the market?']\n",
    "\n",
    "q2=['1. **What specific types of data migrations does Intelia specialize in?**',  '2. **Does Intelia offer end-to-end data migration services, including planning, execution, and post-migration support?**',  '3. **What cloud platforms is Intelia most experienced with and certified in?**',  '4. **Can Intelia provide case studies or examples of successful data migration projects they have completed?**',  \"5. **What is Intelia's pricing model for data migration services?**\"]\n",
    "\n",
    "q3=[ '1. **What specific data-driven opportunities does Intelia help its clients unlock?** ', '2. **Beyond data analysis, what other services does Intelia provide to its clients?**', \"3. **Can you elaborate on Intelia's unique approach to data consultancy?**\", '4. **How does Intelia tailor its services to address the specific challenges of each client?**', '5. **What are some examples of successful projects Intelia has completed for its clients?** ']\n",
    "\n",
    "q4=['What types of businesses does Intelia typically work with?', 'What makes Intelia different from other cloud data platform providers?', \"How does Snowflake's cloud-first architecture benefit businesses?\", 'How can Intelia assist with planning a cloud or multi-cloud strategy? ', 'How does Intelia help clients focus on business value rather than data warehousing complexity']\n",
    "\n",
    "q5=[  '1. What specific data consultancy services does Intelia offer?', '2. What kind of impact does Intelia make for its customers?', '3. How does Intelia foster a culture of innovation and excellence within its team?', \"4. What are Intelia's plans for the future, especially in the context of the evolving AI and data landscape?\", \"5. What are some examples of projects or achievements that demonstrate Intelia's expertise and success? \"]\n",
    "\n",
    "q6=['1. What specific data governance services does Intelia offer to help businesses succeed with their machine learning solutions? ', '2. How does Intelia ensure compliance and facilitate collaboration when implementing data governance strategies?', '3. Can Intelia provide examples of how they have helped businesses improve their data management and avoid the need for significant changes down the line?', '4. What are the key benefits of partnering with Intelia for data governance, particularly in the context of Generative AI adoption?', '5. Does Intelia offer tailored solutions for different industries or specific data-driven challenges? ']\n",
    "\n",
    "q7=['- What services does Intelia provide?', \"- What are some of Intelia's major accomplishments in 2024?\", \"- What are some of Intelia's core values?\", \"- What are Intelia's goals for 2025?\", '- What is the meaning of the hashtag \"#onetribe\"?']\n",
    "\n",
    "queries=q1+q2+q3+q4+q5+q6+q7\n",
    "\n",
    "qid=0\n",
    "for query in queries:\n",
    "        if query!=\"\":\n",
    "            generated_queries=[ json.dumps( {  \"_id\": qid,  \n",
    "                                               \"text\":query                                      \n",
    "                                         }\n",
    "                                       )  +\"\\n\"\n",
    "                        ]\n",
    "        \n",
    "            rf.writelines(generated_queries)\n",
    "            rf.flush()\n",
    "            qid +=1\n",
    "\n",
    "upload_file(rf,dest_bucket_name='vlt_search_and_contet_output',request_file_folder='finetunning',request_file_prefix='fine_tuning_query_sample', version=0, request_file_post_fix ='0')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87522dca-2d38-4e25-af99-85bdddfcc091",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e820aee-f7f7-4e3e-b8b4-721a75bd1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries=[{\"_id\": 0, \"text\": \"What is Intelia's core philosophy and how does it impact their approach to customer success?\"},\n",
    "{\"_id\": 1, \"text\": \"How does Intelia balance its focus on customer delight with the well-being of its employees?\"},\n",
    "{\"_id\": 2, \"text\": \"What specific actions does Intelia take to foster a passionate and enjoyable work environment?\"},\n",
    "{\"_id\": 3, \"text\": \"How does Intelia leverage diversity and collaboration to achieve its goals?\"},\n",
    "{\"_id\": 4, \"text\": \"What sets Intelia apart from other data consultancies in the market?\"},\n",
    "{\"_id\": 5, \"text\": \"1. **What specific types of data migrations does Intelia specialize in?**\"},\n",
    "{\"_id\": 6, \"text\": \"2. **Does Intelia offer end-to-end data migration services, including planning, execution, and post-migration support?**\"},\n",
    "{\"_id\": 7, \"text\": \"3. **What cloud platforms is Intelia most experienced with and certified in?**\"},\n",
    "{\"_id\": 8, \"text\": \"4. **Can Intelia provide case studies or examples of successful data migration projects they have completed?**\"},\n",
    "{\"_id\": 9, \"text\": \"5. **What is Intelia's pricing model for data migration services?**\"},\n",
    "{\"_id\": 10, \"text\": \"1. **What specific data-driven opportunities does Intelia help its clients unlock?** \"},\n",
    "{\"_id\": 11, \"text\": \"2. **Beyond data analysis, what other services does Intelia provide to its clients?**\"},\n",
    "{\"_id\": 12, \"text\": \"3. **Can you elaborate on Intelia's unique approach to data consultancy?**\"},\n",
    "{\"_id\": 13, \"text\": \"4. **How does Intelia tailor its services to address the specific challenges of each client?**\"},\n",
    "{\"_id\": 14, \"text\": \"5. **What are some examples of successful projects Intelia has completed for its clients?** \"},\n",
    "{\"_id\": 15, \"text\": \"What types of businesses does Intelia typically work with?\"},\n",
    "{\"_id\": 16, \"text\": \"What makes Intelia different from other cloud data platform providers?\"},\n",
    "{\"_id\": 17, \"text\": \"How does Snowflake's cloud-first architecture benefit businesses?\"},\n",
    "{\"_id\": 18, \"text\": \"How can Intelia assist with planning a cloud or multi-cloud strategy? \"},\n",
    "{\"_id\": 19, \"text\": \"How does Intelia help clients focus on business value rather than data warehousing complexity\"},\n",
    "{\"_id\": 20, \"text\": \"1. What specific data consultancy services does Intelia offer?\"},\n",
    "{\"_id\": 21, \"text\": \"2. What kind of impact does Intelia make for its customers?\"},\n",
    "{\"_id\": 22, \"text\": \"3. How does Intelia foster a culture of innovation and excellence within its team?\"},\n",
    "{\"_id\": 23, \"text\": \"4. What are Intelia's plans for the future, especially in the context of the evolving AI and data landscape?\"},\n",
    "{\"_id\": 24, \"text\": \"5. What are some examples of projects or achievements that demonstrate Intelia's expertise and success? \"},\n",
    "{\"_id\": 25, \"text\": \"1. What specific data governance services does Intelia offer to help businesses succeed with their machine learning solutions? \"},\n",
    "{\"_id\": 26, \"text\": \"2. How does Intelia ensure compliance and facilitate collaboration when implementing data governance strategies?\"},\n",
    "{\"_id\": 27, \"text\": \"3. Can Intelia provide examples of how they have helped businesses improve their data management and avoid the need for significant changes down the line?\"},\n",
    "{\"_id\": 28, \"text\": \"4. What are the key benefits of partnering with Intelia for data governance, particularly in the context of Generative AI adoption?\"},\n",
    "{\"_id\": 29, \"text\": \"5. Does Intelia offer tailored solutions for different industries or specific data-driven challenges? \"},\n",
    "{\"_id\": 30, \"text\": \"- What services does Intelia provide?\"},\n",
    "{\"_id\": 31, \"text\": \"- What are some of Intelia's major accomplishments in 2024?\"},\n",
    "{\"_id\": 32, \"text\": \"- What are some of Intelia's core values?\"},\n",
    "{\"_id\": 33, \"text\": \"- What are Intelia's goals for 2025?\"},\n",
    "{\"_id\": 34, \"text\": \"- What is the meaning of the hashtag \\\"#onetribe\\\"?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfefb5-544f-40d9-bf42-dfe8ac858dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fb2d8-4c72-4e9f-a1bc-0dd9ad1a639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid=0\n",
    "relevancy=[]\n",
    "\n",
    "import time\n",
    "\n",
    "try:\n",
    "    for query in queries:\n",
    "        for idx,row in data.iterrows():\n",
    "            r=get_autorater_response(title=row['title'], text=row['content'], query=query['text']) \n",
    "            relevancy.append([query['_id'],idx,r.get(\"score\", \"\")])\n",
    "            print(relevancy)\n",
    "            time.sleep(2)\n",
    "            \n",
    "     \n",
    "except:\n",
    "    print(qid)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51e53f-f481-4c07-bf21-ed0c9d70335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with column names\n",
    "df = pd.DataFrame(relevancy, columns=['query-id' , 'corpus-id' ,  'score'])\n",
    "\n",
    "df.to_csv('relevancies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c36b9d06-bbc8-4f09-9bd6-d313e8fd4928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     query-id  corpus-id  score\n",
       "0           0          0    1.0\n",
       "1           0          1    0.0\n",
       "2           0          2    1.0\n",
       "3           0          3    0.0\n",
       "4           0          4    0.0\n",
       "..        ...        ...    ...\n",
       "240        34          2    0.0\n",
       "241        34          3    0.0\n",
       "242        34          4    2.0\n",
       "243        34          5    0.0\n",
       "244        34          6    2.0\n",
       "\n",
       "[245 rows x 3 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f3f74-897a-42f4-8655-60d797d2a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bceab1b4-6ce1-4c9d-9f12-e9e5b38d7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('relevancies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e75c6132-7044-44c8-a044-87f3cc2475f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     query-id  corpus-id  score\n",
       "0           0          0    1.0\n",
       "2           0          2    1.0\n",
       "5           0          5    1.0\n",
       "7           1          0    1.0\n",
       "11          1          4    1.0\n",
       "..        ...        ...    ...\n",
       "229        32          5    1.0\n",
       "230        32          6    1.0\n",
       "238        34          0    3.0\n",
       "242        34          4    2.0\n",
       "244        34          6    2.0\n",
       "\n",
       "[95 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[df['score']>0]\n",
    "df[['query-id' , 'corpus-id' ,  'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f8eb2-7dec-418d-abde-8903a49453fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['query-id']==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ac0f219b-6a0c-4ee8-a04a-c6b64c018ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train=df.reset_index(drop=True)[['query-id' , 'corpus-id' ,  'score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3a3aac82-1147-483f-8e7c-75816132abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to TSV file\n",
    "train.to_csv('train.tsv', sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "01e5d429-d6f1-4a57-b6d4-314b16441515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from google.cloud.aiplatform import initializer as aiplatform_init\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "\n",
    "def tune_embedding_model(\n",
    "    api_endpoint: str,\n",
    "    base_model_name: str =  'textembedding-gecko@003',\n",
    "    corpus_path: str = \"gs://vlt_search_and_contet_output/finetunning/fine_tuning_intelia_sample_0_0.json\",\n",
    "    queries_path: str = \"gs://vlt_search_and_contet_output/finetunning/fine_tuning_query_sample_0_0.json\",\n",
    "    train_label_path: str = \"gs://vlt_search_and_contet_output/finetunning/train.tsv\",\n",
    "    #test_label_path: str = \"gs://vlt_search_and_contet_output/finetunning/test.tsv\",\n",
    "):  # noqa: ANN201\n",
    "    \"\"\"Tune an embedding model using the specified parameters.\n",
    "    Args:\n",
    "        api_endpoint (str): The API endpoint for the Vertex AI service.\n",
    "        base_model_name (str): The name of the base model to use for tuning.\n",
    "        corpus_path (str): GCS URI of the JSONL file containing the corpus data.\n",
    "        queries_path (str): GCS URI of the JSONL file containing the queries data.\n",
    "        train_label_path (str): GCS URI of the TSV file containing the training labels.\n",
    "        test_label_path (str): GCS URI of the TSV file containing the test labels.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"^(\\w+-\\w+)\", api_endpoint)\n",
    "    location = match.group(1) if match else \"us-central1\"\n",
    "    base_model = TextEmbeddingModel.from_pretrained(base_model_name)\n",
    "    tuning_job = base_model.tune_model(\n",
    "        task_type=\"DEFAULT\",\n",
    "        corpus_data=corpus_path,\n",
    "        queries_data=queries_path,\n",
    "        training_data=train_label_path,\n",
    "        #test_data=test_label_path,\n",
    "        batch_size=128,  # The batch size to use for training.\n",
    "        train_steps=1000,  # The number of training steps.\n",
    "        tuned_model_location=location,\n",
    "        #output_dimensionality=768,  # The dimensionality of the output embeddings.\n",
    "        learning_rate_multiplier=1.0,  # The multiplier for the learning rate.\n",
    "    )\n",
    "    return tuning_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09e40c8e-4a94-4d7b-9b41-4db35f445370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/494586852359/locations/us-central1/pipelineJobs/tune-text-embedding-model-20250202050250\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/494586852359/locations/us-central1/pipelineJobs/tune-text-embedding-model-20250202050250')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tune-text-embedding-model-20250202050250?project=494586852359\n"
     ]
    }
   ],
   "source": [
    "tuningjob=tune_embedding_model('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64262f68-a191-4ca1-b944-f2f4d2542530",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tuningjob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3eddfca3-a88e-4ff5-86f7-1143bc66f6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PipelineState.PIPELINE_STATE_RUNNING: 3>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuningjob._status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3d69a6fc-f023-41d7-8948-41c248839612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': 'Honesty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': 'Go onlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': \"The size...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': \"When you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': \"Being a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': 'You\\'ll ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': \"It's bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': 'Neither ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': \"You will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>[{'role': 'user', 'parts': [{'text': \"You’re t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              contents\n",
       "0    [{'role': 'user', 'parts': [{'text': 'Honesty ...\n",
       "1    [{'role': 'user', 'parts': [{'text': 'Go onlin...\n",
       "2    [{'role': 'user', 'parts': [{'text': \"The size...\n",
       "3    [{'role': 'user', 'parts': [{'text': \"When you...\n",
       "4    [{'role': 'user', 'parts': [{'text': \"Being a ...\n",
       "..                                                 ...\n",
       "495  [{'role': 'user', 'parts': [{'text': 'You\\'ll ...\n",
       "496  [{'role': 'user', 'parts': [{'text': \"It's bes...\n",
       "497  [{'role': 'user', 'parts': [{'text': 'Neither ...\n",
       "498  [{'role': 'user', 'parts': [{'text': \"You will...\n",
       "499  [{'role': 'user', 'parts': [{'text': \"You’re t...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "# GCS file path\n",
    "gcs_uri = \"gs://cloud-samples-data/ai-platform/embedding/goog-10k-2024/r11/corpus.jsonl\"\n",
    "gcs_uri=\"gs://cloud-samples-data/ai-platform/generative_ai/gemini-1_5/text/sft_train_data.jsonl\"\n",
    "# Initialize GCS file system\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# Read JSONL file into DataFrame\n",
    "with fs.open(gcs_uri, 'r') as f:\n",
    "    df = pd.read_json(f, lines=True)\n",
    "\n",
    "# Display DataFram\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "41844ff5-64e4-4c9f-a515-0f5ee88c6ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'parts': [{'text': 'Honesty is usually the best policy. It is disrespectful to lie to someone. If you don\\'t want to date someone, you should say so.  Sometimes it is easy to be honest. For example, you might be able to truthfully say, \"No, thank you, I already have a date for that party.\" Other times, you might need to find a kinder way to be nice. Maybe you are not attracted to the person. Instead of bluntly saying that, try saying, \"No, thank you, I just don\\'t think we would be a good fit.\" Avoid making up a phony excuse. For instance, don\\'t tell someone you will be out of town this weekend if you won\\'t be. There\\'s a chance that you might then run into them at the movies, which would definitely cause hurt feelings. A compliment sandwich is a really effective way to provide feedback. Essentially, you \"sandwich\" your negative comment between two positive things. Try using this method when you need to reject someone.  An example of a compliment sandwich is to say something such as, \"You\\'re an awesome person. Unfortunately, I\\'m not interested in dating you. Someone else is going to be really lucky to date someone with such a great personality!\" You could also try, \"You are a really nice person. I\\'m only interested you as a friend. I like when we hang out in big groups together!\" Be sincere. If you offer false compliments, the other person will likely be able to tell and feel hurt. If you do not want to date someone, it is best to be upfront about your feelings. Do not beat around the bush. If your mind is made up, it is best to clearly state your response.  If someone asks you to date them and you don\\'t want to, you can be direct and kind at the same time. State your answer clearly. You can make your feelings clear without purposefully hurting someone else\\'s feelings. Try smiling and saying, \"That sounds fun, but no thank you. I\\'m not interested in dating you.\" Don\\'t beat around the bush. If you do not want to accept the date, there is no need to say, \"Let me think about it.\" It is best to get the rejection over with. You don\\'t want to give someone false hope. Avoid saying something like, \"Let me check my schedule and get back to you.\" Try to treat the person the way you would want to be treated. This means that you should choose your words carefully. Be thoughtful in your response.  It\\'s okay to pause before responding. You might be taken by surprise and need a moment to collect your thoughts. Say thank you. It is a compliment to be asked out. You can say, \"I\\'m flattered. Unfortunately, I can\\'t accept.\" Don\\'t laugh. Many people laugh nervously in awkward situations. Try to avoid giggling, as that is likely to result in hurt feelings. Sometimes it is not what you say, but how you say it. If you need to reject someone, think about factors other than your words. Non-verbal communication matters, too.  Use the right tone of voice. Try to sound gentle but firm. Make eye contact. This helps convey that you are being serious, and also shows respect for the other person. If you are in public, try not to speak too loudly. It is not necessary for everyone around you to know that you are turning down a date.\\n\\nProvide a summary of the article in two or three sentences:\\n\\n'}]},\n",
       " {'role': 'model',\n",
       "  'parts': [{'text': 'Tell the truth. Use a \"compliment sandwich\". Be direct. Treat the person with respect. Communicate effectively.'}]}]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c91d1fe9-babf-4cb3-8dd8-f2ceba062c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>query-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corpus/195</td>\n",
       "      <td>queries/251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corpus/225</td>\n",
       "      <td>queries/221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>corpus/177</td>\n",
       "      <td>queries/177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corpus/183</td>\n",
       "      <td>queries/185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>corpus/306</td>\n",
       "      <td>queries/307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>corpus/85</td>\n",
       "      <td>queries/86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>corpus/232</td>\n",
       "      <td>queries/221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>corpus/272</td>\n",
       "      <td>queries/273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>corpus/203</td>\n",
       "      <td>queries/201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>corpus/201</td>\n",
       "      <td>queries/200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      corpus-id     query-id  score\n",
       "0    corpus/195  queries/251      1\n",
       "1    corpus/225  queries/221      1\n",
       "2    corpus/177  queries/177      1\n",
       "3    corpus/183  queries/185      1\n",
       "4    corpus/306  queries/307      1\n",
       "..          ...          ...    ...\n",
       "150   corpus/85   queries/86      1\n",
       "151  corpus/232  queries/221      1\n",
       "152  corpus/272  queries/273      1\n",
       "153  corpus/203  queries/201      1\n",
       "154  corpus/201  queries/200      1\n",
       "\n",
       "[155 rows x 3 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "# GCS file path\n",
    "gcs_uri = \"gs://cloud-samples-data/ai-platform/embedding/goog-10k-2024/r11/test.tsv\"\n",
    "\n",
    "# Initialize GCS file system\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# Read JSONL file into DataFrame\n",
    "with fs.open(gcs_uri, 'r') as f:\n",
    "    df = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# Display DataFram\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9803f4f1-558a-412e-9fd6-1174a3c15a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from google.cloud.aiplatform import initializer as aiplatform_init\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "\n",
    "def tune_embedding_model(\n",
    "    api_endpoint: str,\n",
    "    base_model_name: str = \"text-embedding-005\",\n",
    "    corpus_path: str = \"gs://cloud-samples-data/ai-platform/embedding/goog-10k-2024/r11/corpus.jsonl\",\n",
    "    queries_path: str = \"gs://cloud-samples-data/ai-platform/embedding/goog-10k-2024/r11/queries.jsonl\",\n",
    "    train_label_path: str = \"gs://cloud-samples-data/ai-platform/embedding/goog-10k-2024/r11/train.tsv\",\n",
    "    test_label_path: str = \"gs://cloud-samples-data/ai-platform/embedding/goog-10k-2024/r11/test.tsv\",\n",
    "):  # noqa: ANN201\n",
    "    \"\"\"Tune an embedding model using the specified parameters.\n",
    "    Args:\n",
    "        api_endpoint (str): The API endpoint for the Vertex AI service.\n",
    "        base_model_name (str): The name of the base model to use for tuning.\n",
    "        corpus_path (str): GCS URI of the JSONL file containing the corpus data.\n",
    "        queries_path (str): GCS URI of the JSONL file containing the queries data.\n",
    "        train_label_path (str): GCS URI of the TSV file containing the training labels.\n",
    "        test_label_path (str): GCS URI of the TSV file containing the test labels.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"^(\\w+-\\w+)\", api_endpoint)\n",
    "    location = match.group(1) if match else \"us-central1\"\n",
    "    base_model = TextEmbeddingModel.from_pretrained(base_model_name)\n",
    "    tuning_job = base_model.tune_model(\n",
    "        task_type=\"DEFAULT\",\n",
    "        corpus_data=corpus_path,\n",
    "        queries_data=queries_path,\n",
    "        training_data=train_label_path,\n",
    "        test_data=test_label_path,\n",
    "        batch_size=128,  # The batch size to use for training.\n",
    "        train_steps=1000,  # The number of training steps.\n",
    "        tuned_model_location=location,\n",
    "        output_dimensionality=768,  # The dimensionality of the output embeddings.\n",
    "        learning_rate_multiplier=1.0,  # The multiplier for the learning rate.\n",
    "    )\n",
    "    return tuning_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fba7cb9c-e2a6-437b-9418-16e1fec4df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/494586852359/locations/us-central1/pipelineJobs/tune-text-embedding-model-20250202052109\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/494586852359/locations/us-central1/pipelineJobs/tune-text-embedding-model-20250202052109')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tune-text-embedding-model-20250202052109?project=494586852359\n"
     ]
    }
   ],
   "source": [
    "job=tune_embedding_model('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ef875-8f63-4906-a65a-1552da42f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SupervisedTuningJob\n",
      "SupervisedTuningJob created. Resource name: projects/494586852359/locations/us-central1/tuningJobs/4813491997497098240\n",
      "To use this SupervisedTuningJob in another session:\n",
      "tuning_job = sft.SupervisedTuningJob('projects/494586852359/locations/us-central1/tuningJobs/4813491997497098240')\n",
      "View Tuning Job:\n",
      "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/4813491997497098240?project=494586852359\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-a8e72602-b002-48d6-b0f7-19d0e0870454\" href=\"#view-view-vertex-resource-a8e72602-b002-48d6-b0f7-19d0e0870454\">\n",
       "          <span class=\"material-icons view-vertex-icon\">tune</span>\n",
       "          <span>View Tuning Job</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-a8e72602-b002-48d6-b0f7-19d0e0870454');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/4813491997497098240?project=494586852359');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/4813491997497098240?project=494586852359', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-46022738-55de-4644-97d8-f0655a8c7d32\" href=\"#view-view-vertex-resource-46022738-55de-4644-97d8-f0655a8c7d32\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-46022738-55de-4644-97d8-f0655a8c7d32');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20250201230250837477/runs?project=nine-quality-test');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20250201230250837477/runs?project=nine-quality-test', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=pd.read_csv('all_queries.csv')\n",
    "for idx,row in data.iterrows():\n",
    "    data_items.append(  {  \n",
    "                                \"input_text\":row['query'],\n",
    "                                 \"output\":row['answer']                                          \n",
    "                               }\n",
    "                              \n",
    "                     )\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import vertexai\n",
    "from vertexai.tuning import sft\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "\n",
    "sft_tuning_job = sft.train(\n",
    "    source_model=\"gemini-1.5-pro-002\",\n",
    "    train_dataset=\"gs://cloud-samples-data/ai-platform/generative_ai/gemini-1_5/text/sft_train_data.jsonl\",\n",
    ")\n",
    "\n",
    "# Polling for job completion\n",
    "while not sft_tuning_job.has_ended:\n",
    "    time.sleep(60)\n",
    "    sft_tuning_job.refresh()\n",
    "\n",
    "print(sft_tuning_job.tuned_model_name)\n",
    "print(sft_tuning_job.tuned_model_endpoint_name)\n",
    "print(sft_tuning_job.experiment)\n",
    "# Example response:\n",
    "# projects/123456789012/locations/us-central1/models/1234567890@1\n",
    "# projects/123456789012/locations/us-central1/endpoints/123456789012345\n",
    "# <google.cloud.aiplatform.metadata.experiment_resources.Experiment object at 0x7b5b4ae07af0>\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "import random\n",
    "\n",
    "# name = f'generate-num-{random.randint(0,10000)}'\n",
    "# operation = genai.create_tuned_model(\n",
    "#     # You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\n",
    "#     source_model='gemini-1.5-flash-001-tuning',\n",
    "#     training_data=data_items,\n",
    "#     id = 'test tara',\n",
    "#     epoch_count = 100,\n",
    "#     batch_size=4,\n",
    "#     learning_rate=0.001,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c571d7-ace2-4dd1-9c76-e4ffc64ff728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import storage\n",
    " \n",
    "# if 1==1:\n",
    "\n",
    "#     \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "#     # The ID of your GCS bucket\n",
    "#     # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "#     # The ID of your GCS object\n",
    "#     # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "#     # The path to which the file should be downloaded\n",
    "#     # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "#     bucket = storage_client.bucket('cloud-samples-data')\n",
    "\n",
    "#     # Construct a client side representation of a blob.\n",
    "#     # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "#     # any content from Google Cloud Storage. As we don't need additional data,\n",
    "#     # using `Bucket.blob` is preferred here.\n",
    "#     blob = bucket.blob('ai-platform/generative_ai/gemini-1_5/text/sft_train_data.jsonl')\n",
    "#     blob.download_to_filename('sft_train_data.jsonl')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efaf41-ef67-4b40-be3a-5e8900636c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e88c7f-ad5b-4605-9c04-ecd3d4c87cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41130866-0f1c-4b9d-ac54-dba343b5b6d6",
   "metadata": {},
   "source": [
    "### Gemini Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775896c5-8d3c-48cc-b3c0-1c1120df20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=True) \n",
    "rf= open(request_file.name, \"a\") \n",
    "\n",
    "data=pd.read_csv('all_queries.csv')\n",
    "for idx,row in data.iterrows():\n",
    "   request_list=[\n",
    "                                       json.dumps(\n",
    "                                                 {\n",
    "                                                  \"contents\": [\n",
    "                                                    {\n",
    "                                                      \"role\": \"user\",\n",
    "                                                      \"parts\": [\n",
    "                                                        {\n",
    "                                                          \"text\": row['query']\n",
    "                                                        }\n",
    "                                                      ]\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                      \"role\": \"model\",\n",
    "                                                      \"parts\": [\n",
    "                                                        {\n",
    "                                                          \"text\": row['answer']\n",
    "                                                        }\n",
    "                                                      ]\n",
    "                                                    }\n",
    "                                                  ]\n",
    "                                                }\n",
    "                                           \n",
    "                                       )  +\"\\n\"\n",
    "                                 ]\n",
    "\n",
    "   rf.writelines(request_list)\n",
    "   rf.flush()\n",
    " \n",
    "upload_file(rf,dest_bucket_name='vlt_search_and_contet_output',request_file_folder='finetunning',request_file_prefix='fine_tuning_intelia_sample_query_answer', version=0, request_file_post_fix ='0')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d146510-cc5f-49d1-8178-391be1bd0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import vertexai\n",
    "from vertexai.tuning import sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76cbaf7a-60b5-4c28-ba25-effb06533806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*, source_model: Union[str, vertexai.generative_models.GenerativeModel], train_dataset: str, validation_dataset: Optional[str] = None, tuned_model_display_name: Optional[str] = None, epochs: Optional[int] = None, learning_rate_multiplier: Optional[float] = None, adapter_size: Optional[Literal[1, 4, 8, 16]] = None, labels: Optional[Dict[str, str]] = None) -> 'SupervisedTuningJob'\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(sft.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80735b56-0e93-4174-b15e-8a7debc1cab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SupervisedTuningJob\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 Failed to get object metadata for gs://vlt_search_and_contet_output/finetunning/fine_tuning_intelia_sample_query_answer_0_0.jsonl. Please make sure the file exists and the service account service-494586852359@gcp-sa-vertex-tune.iam.gserviceaccount.com has permission to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1192\u001b[0m (\n\u001b[1;32m   1193\u001b[0m     state,\n\u001b[1;32m   1194\u001b[0m     call,\n\u001b[1;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1197\u001b[0m )\n\u001b[0;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Failed to get object metadata for gs://vlt_search_and_contet_output/finetunning/fine_tuning_intelia_sample_query_answer_0_0.jsonl. Please make sure the file exists and the service account service-494586852359@gcp-sa-vertex-tune.iam.gserviceaccount.com has permission to access it.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.126.95:443 {grpc_message:\"Failed to get object metadata for gs://vlt_search_and_contet_output/finetunning/fine_tuning_intelia_sample_query_answer_0_0.jsonl. Please make sure the file exists and the service account service-494586852359@gcp-sa-vertex-tune.iam.gserviceaccount.com has permission to access it.\", grpc_status:3, created_time:\"2025-02-03T00:35:37.372780947+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sft\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO(developer): Update and un-comment below line\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# PROJECT_ID = \"your-project-id\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m sft_tuning_job \u001b[38;5;241m=\u001b[39m \u001b[43msft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-1.5-pro-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://vlt_search_and_contet_output/finetunning/fine_tuning_intelia_sample_query_answer_0_0.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuned_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIntelia-finetuned-gemini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Polling for job completion\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sft_tuning_job\u001b[38;5;241m.\u001b[39mhas_ended:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/tuning/_supervised_tuning.py:92\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(source_model, train_dataset, validation_dataset, tuned_model_display_name, epochs, learning_rate_multiplier, adapter_size, labels)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source_model, generative_models\u001b[38;5;241m.\u001b[39mGenerativeModel):\n\u001b[1;32m     89\u001b[0m     source_model \u001b[38;5;241m=\u001b[39m source_model\u001b[38;5;241m.\u001b[39m_prediction_resource_name\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     91\u001b[0m supervised_tuning_job \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mSupervisedTuningJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtuning_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupervised_tuning_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtuned_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuned_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m _ipython_utils\u001b[38;5;241m.\u001b[39mdisplay_model_tuning_button(supervised_tuning_job)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m supervised_tuning_job\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/tuning/_tuning.py:227\u001b[0m, in \u001b[0;36mTuningJob._create\u001b[0;34m(cls, base_model, tuning_spec, tuned_model_display_name, description, labels, project, location, credentials)\u001b[0m\n\u001b[1;32m    216\u001b[0m tuning_job: TuningJob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_sdk_resource_from_gapic(\n\u001b[1;32m    217\u001b[0m     gapic_resource\u001b[38;5;241m=\u001b[39mgca_tuning_job,\n\u001b[1;32m    218\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[1;32m    219\u001b[0m     location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[1;32m    220\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    223\u001b[0m parent \u001b[38;5;241m=\u001b[39m aiplatform_initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mcommon_location_path(\n\u001b[1;32m    224\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject, location\u001b[38;5;241m=\u001b[39mlocation\n\u001b[1;32m    225\u001b[0m )\n\u001b[0;32m--> 227\u001b[0m created_gca_tuning_job \u001b[38;5;241m=\u001b[39m \u001b[43mtuning_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tuning_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuning_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgca_tuning_job\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m tuning_job\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m created_gca_tuning_job\n\u001b[1;32m    233\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_create_complete(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    235\u001b[0m     resource\u001b[38;5;241m=\u001b[39mcreated_gca_tuning_job,\n\u001b[1;32m    236\u001b[0m     variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuning_job\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    237\u001b[0m     module_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    238\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform_v1beta1/services/gen_ai_tuning_service/client.py:940\u001b[0m, in \u001b[0;36mGenAiTuningServiceClient.create_tuning_job\u001b[0;34m(self, request, parent, tuning_job, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 940\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Failed to get object metadata for gs://vlt_search_and_contet_output/finetunning/fine_tuning_intelia_sample_query_answer_0_0.jsonl. Please make sure the file exists and the service account service-494586852359@gcp-sa-vertex-tune.iam.gserviceaccount.com has permission to access it."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import vertexai\n",
    "from vertexai.tuning import sft\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "\n",
    "sft_tuning_job = sft.train(\n",
    "    source_model=\"gemini-1.5-pro-002\",\n",
    "    train_dataset=\"gs://vlt_search_and_contet_output/finetunning/fine_tuning_intelia_sample_query_answer_0_0.jsonl\",\n",
    "    tuned_model_display_name=\"Intelia-finetuned-gemini\"\n",
    ")\n",
    "\n",
    "# Polling for job completion\n",
    "while not sft_tuning_job.has_ended:\n",
    "    time.sleep(60)\n",
    "    sft_tuning_job.refresh()\n",
    "\n",
    "print(sft_tuning_job.tuned_model_name)\n",
    "print(sft_tuning_job.tuned_model_endpoint_name)\n",
    "print(sft_tuning_job.experiment)\n",
    "# Example response:\n",
    "# projects/123456789012/locations/us-central1/models/1234567890@1\n",
    "# projects/123456789012/locations/us-central1/endpoints/123456789012345\n",
    "# <google.cloud.aiplatform.metadata.experiment_resources.Experiment object at 0x7b5b4ae07af0>\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "import random\n",
    "\n",
    "# name = f'generate-num-{random.randint(0,10000)}'\n",
    "# operation = genai.create_tuned_model(\n",
    "#     # You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\n",
    "#     source_model='gemini-1.5-flash-001-tuning',\n",
    "#     training_data=data_items,\n",
    "#     id = 'test tara',\n",
    "#     epoch_count = 100,\n",
    "#     batch_size=4,\n",
    "#     learning_rate=0.001,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d57658-0123-492d-9e14-6a685beaf78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
