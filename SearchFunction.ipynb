{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c2be1844-9ec6-47f4-b56f-0b5f1a7f605f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import functions_framework\n",
    "import time\n",
    "import random\n",
    "from EmbeddingPredictionClient import EmbeddingPredictionClient  \n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "async def exponential_backoff_retries(client, text=None, image_file: bytes=None, max_retries=5, embedding_type=None):\n",
    "    \"\"\"\n",
    "    This function applies exponential backoff with retries to the API calls.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Try to get the embedding from the client\n",
    "            if embedding_type==\"multimodal_embedding\":                   \n",
    "                    return client.get_multimodal_embedding(\"\", image_file)            \n",
    "            elif embedding_type==\"text_embedding\":\n",
    "                    return client.get_text_embedding(text)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            backoff_delay = min(2 ** attempt + random.uniform(0, 1), 32)  # Exponential backoff with jitter\n",
    "            print(f\"Attempt {attempt} failed with error {e}. Retrying in {backoff_delay:.2f} seconds...\")\n",
    "            time.sleep(backoff_delay)  # Wait before retrying\n",
    "\n",
    "    raise Exception(\"Max retries reached. Could not complete the request.\")\n",
    "\n",
    "    \n",
    "async def generate_query_embedding(client,text=None,image_file=None, embedding_type=None):\n",
    "    try:\n",
    "        # Retry logic with exponential backoff to calculate query embeddings\n",
    "        result = exponential_backoff_retries(embedding_client, text, image_file, embedding_type)\n",
    "        \n",
    "        # Respond with the successful embedding response\n",
    "        return {\n",
    "            \"text_embedding\": result.text_embedding,\n",
    "            \"image_embedding\": result.image_embedding\n",
    "        }, 200\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle failure after max retries\n",
    "        return f\"Error: {str(e)}\", 500\n",
    "\n",
    "\n",
    "async def get_media_nearest_neighbors(query_embedding, table, dataset,source_embedding_column,project_id,top_k=50, filter_query=\"\"):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for multimodal embeddings.\"\"\"\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    options=\"\"\"'{\"fraction_lists_to_search\": 1}'\"\"\"\n",
    "    #options=\"\"\"'{\"use_brute_force\":true}' \"\"\"\n",
    "\n",
    "    sql = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.uri as fileUri,  \n",
    "              search_results.base.combined_multimodal_id as unique_id,\n",
    "              search_results.distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              search_results.base.asset_id ,\n",
    "              search_results.base.ml_generate_embedding_start_sec as startOffset_seconds,\n",
    "              search_results.base.ml_generate_embedding_end_sec as endOffset_seconds,  \n",
    "              search_results.base.content_type as asset_type,\n",
    "              ROW_NUMBER() OVER (PARTITION BY search_results.base.asset_id ORDER BY distance ASC) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                ( SELECT * FROM  `{dataset}.{table}` WHERE 1=1 {filter_query}), --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  }, -- Top k most similar matches based on distance\n",
    "                distance_type => 'COSINE',\n",
    "                options => {options}                \n",
    "              ) search_results\n",
    "              \n",
    "          ),   \n",
    "            -- Step 2: Find the minimum distance per asset_id\n",
    "             ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,        \n",
    "                    MIN(distance) AS min_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            )\n",
    "\n",
    "            -- Step 3: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id,  \n",
    "                sr.fileUri,  \n",
    "                sr.asset_type,\n",
    "                rd.min_distance,\n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id ORDER BY min_distance ASC) AS IDX,\n",
    "                STRING_AGG(CONCAT(\"\"\"+\"'{startOffset_seconds:', sr.startOffset_seconds, ',endOffset_seconds:', sr.endOffset_seconds, '}')\"\"\"+f\"\"\", \", \" ) \n",
    "                OVER (PARTITION BY sr.asset_id ORDER BY sr.startOffset_seconds) AS time_lines,\n",
    "                CASE WHEN LOWER(asset_type) like '%video%' then\n",
    "                 CONCAT(' ',\n",
    "                   CASE  \n",
    "                       WHEN UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(0)])  LIKE '%NNNT23%' THEN 'NINE NEWS 2023'  \n",
    "                       WHEN UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(0)]) LIKE '%MAAT2023%' THEN 'MARRIED AT FIRST SIGHT 2023'\n",
    "                       WHEN UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(0)]) LIKE '%60MI23%' THEN '60 MINUTES 2023'\n",
    "                   END ,\n",
    "                   ' EPISODE ' , UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(1)])\n",
    "                   )\n",
    "                 ELSE '' END AS headline\n",
    "    \n",
    "               -- sr.distance,\n",
    "               -- final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            ORDER BY min_distance ASC  \n",
    "            )\n",
    "            WHERE IDX=1\n",
    "    \"\"\"       \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "    \n",
    "    output=[]\n",
    "    for row in results:\n",
    "        output.append({'asset_id':row['asset_id'], 'fileUri':row['fileUri'], \"time_lines\":row['time_lines'], \"asset_type\":row[\"asset_type\"], \"distance\":row['min_distance'],\n",
    "                       \"headline\":row[\"headline\"]})\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return output\n",
    "\n",
    "async def get_content_nearest_neighbors(query_embedding, table, dataset,source_embedding_column,project_id,top_k=50,filter_query=\"\"):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for text embeddings.\"\"\"\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    options=\"\"\"'{\"fraction_lists_to_search\": 1}'\"\"\"\n",
    "    #options=\"\"\"'{\"use_brute_force\":true}' \"\"\"\n",
    "    \n",
    "    sql = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.content as content,  \n",
    "              search_results.base.combined_id as combined_id,\n",
    "              search_results.base.unique_id,\n",
    "              distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              search_results.base.asset_id,\n",
    "              search_results.base.headline,\n",
    "              ifnull(search_results.base.html_safe_text,search_results.base.description) as description,\n",
    "              search_results.base.startOffset_seconds,\n",
    "              search_results.base.endOffset_seconds,\n",
    "              search_results.base.fileUri,\n",
    "              search_results.base.asset_type,    \n",
    "              search_results.base.first_published_timestamp,\n",
    "              search_results.base.brand_type,\n",
    "              search_results.base.primary_category_name,\n",
    "             search_results.base.byline[SAFE_OFFSET(0)].author_name,\n",
    "              ROW_NUMBER() OVER (PARTITION BY  search_results.base.asset_id ORDER BY distance ASC) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                ( SELECT * FROM  `{dataset}.{table}` WHERE 1=1 {filter_query}), --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  }, -- Top k most similar matches based on distance\n",
    "                distance_type => 'COSINE',\n",
    "                options => {options}                   \n",
    "              ) search_results              \n",
    "          ),          \n",
    "\n",
    "             -- Step 2: Aggregate relevance per document (original_document_id)\n",
    "            ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,        \n",
    "                    MIN(distance) AS min_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            )\n",
    "\n",
    "            -- Step 4: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id, \n",
    "                CASE WHEN LOWER(asset_type) like '%video%' then\n",
    "                 CONCAT(' ',\n",
    "                   CASE  \n",
    "                       WHEN UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(0)])  LIKE '%NNNT23%' THEN 'NINE NEWS 2023'  \n",
    "                       WHEN UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(0)]) LIKE '%MAAT2023%' THEN 'MARRIED AT FIRST SIGHT 2023'\n",
    "                       WHEN UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(0)]) LIKE '%60MI23%' THEN '60 MINUTES 2023'\n",
    "                   END ,\n",
    "                   ' EPISODE ' , UPPER(SPLIT(REPLACE(sr.asset_id,'SYD-NINE_','') ,'_')[OFFSET(1)])\n",
    "                   )\n",
    "                 ELSE IFNULL(sr.headline,'') END AS headline,                 \n",
    "                sr.description,\n",
    "                sr.combined_id,\n",
    "                sr.unique_id,\n",
    "                sr.fileUri,\n",
    "                sr.asset_type,\n",
    "                rd.min_distance,\n",
    "                sr.first_published_timestamp,\n",
    "                sr.brand_type,\n",
    "                sr.primary_category_name,\n",
    "                sr.author_name,\n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id ORDER BY min_distance desc) AS IDX,\n",
    "                STRING_AGG(CONCAT(\"\"\"+\"'{startOffset_seconds:', sr.startOffset_seconds, ',endOffset_seconds:', sr.endOffset_seconds, '}')\"\"\"+f\"\"\", \", \" ) \n",
    "                OVER (PARTITION BY sr.asset_id ORDER BY sr.startOffset_seconds) AS time_lines               \n",
    "                --sr.distance,\n",
    "                --final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            ORDER BY min_distance ASC\n",
    "            )\n",
    "            WHERE IDX=1\n",
    "    \"\"\"       \n",
    " \n",
    "    print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()   \n",
    "    output=[]\n",
    "    for row in results:\n",
    "        output.append({'asset_id':row['asset_id'], 'headline':row['headline'],'description':row['description'],'fileUri':row['fileUri'], \"time_lines\":row['time_lines'], \"asset_type\":row[\"asset_type\"], \n",
    "                      \"distance\":row['min_distance'],\n",
    "                       \"first_published_timestamp\":row['first_published_timestamp'],\n",
    "                        \"brand_type\":row['brand_type'],\n",
    "                        \"primary_category_name\":row['primary_category_name'],\n",
    "                        \"author_name\":row['author_name']\n",
    "                      \n",
    "                      })\n",
    "  \n",
    "    end_time = time.time()\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return output\n",
    "\n",
    "def merge_result(combined_list):\n",
    "    # Step 2: Create a dictionary to merge by 'id'\n",
    "    merged_dict = {}\n",
    "\n",
    "    # Step 3: Iterate through the combined list and merge dictionaries by 'id'\n",
    "    for d in combined_list:\n",
    "        id_value = d['asset_id']\n",
    "\n",
    "        # If the id already exists in merged_dict, update it\n",
    "        if id_value in merged_dict:\n",
    "            merged_dict[id_value].update(d)\n",
    "        else:\n",
    "            # If the id doesn't exist, add the dictionary as it is\n",
    "            merged_dict[id_value] = d.copy()\n",
    "\n",
    "    # Step 4: Convert the merged dictionary back into a list\n",
    "    final_merged_list = list(merged_dict.values())\n",
    "    \n",
    "    return final_merged_list\n",
    "\n",
    "\n",
    "\n",
    "async def get_nearest_contet(request):\n",
    "    \"\"\"\n",
    "    Cloud Function entry point. This function handles the incoming request, \n",
    "    performs exponential backoff retries, and returns the embedding response.\n",
    "    \"\"\" \n",
    "    # Parse the incoming request to extract text or image file\n",
    "    request_json = request.get_json(silent=True)\n",
    "    text = request_json.get('search_query')\n",
    "    image_file = request_json.get('image_file')  # Assume it's the path or base64 string of the image\n",
    "    project_id = request_json.get('project')  \n",
    "    region = request_json.get('region')  \n",
    "    filter_image = request_json.get('filter_image') \n",
    "    filter_video = request_json.get('filter_video') \n",
    "    filter_article=request_json.get('filter_article')\n",
    "    \n",
    "    # Load configuration from config.json\n",
    "    with open('config.json') as config_file:\n",
    "         config = json.load(config_file)\n",
    "    \n",
    "    \n",
    "    top_k=int(config['top_k'])  \n",
    "    dataset= config['dataset']\n",
    "    content_table=config['content_table']\n",
    "    mm_table=config['mm_table']\n",
    "    content_source_embedding_column=config['content_source_embedding_column']\n",
    "    mm_source_embedding_column=config['mm_source_embedding_column'] \n",
    "    if image_file==\"\" or image_file==\"None\":\n",
    "        image_file=None\n",
    "        \n",
    "    article_filter_query=\"\"\n",
    "    if filter_article==\"True\" or filter_article==\"1\":\n",
    "        article_filter_query= article_filter_query+f\" AND lower(asset_type) like '%article%' \" \n",
    "        \n",
    "    image_filter_query=\"\"\n",
    "    if filter_image==\"True\" or filter_image==\"1\":\n",
    "        image_filter_query= image_filter_query+f\" AND lower(asset_type) like '%image%' \"  \n",
    "        \n",
    "    video_filter_query=\"\"\n",
    "    if filter_video==\"True\" or filter_video==\"1\":\n",
    "        video_filter_query= video_filter_query+f\" AND lower(asset_type) like '%video%' \"\n",
    "        \n",
    "#     project_id='nine-quality-test'\n",
    "#     region=\"us-central1\"\n",
    "#     text='curtis sittenfeld'\n",
    "#     image_file=None\n",
    "    \n",
    "#     top_k=50     \n",
    "#     dataset='vlt_media_embeddings_integration'\n",
    "#     content_table='vlt_all_media_content_text_embeddings'\n",
    "#     mm_table='vlt_imgvdo_multimodal_embeddings'\n",
    "#     content_source_embedding_column='text_embedding_result'\n",
    "#     mm_source_embedding_column='ml_generate_embedding_result'\n",
    "\n",
    "    # Initialize the EmbeddingPredictionClient outside the function for reuse\n",
    "    embedding_client = EmbeddingPredictionClient(project=project_id , location=region,api_regional_endpoint=region+\"-aiplatform.googleapis.com\")\n",
    "        \n",
    "    if not text and not image_file:\n",
    "        print('you are here')\n",
    "        return 'Error: At least one of \"text\" or \"image_file\" must be provided.', 400\n",
    "     \n",
    "    content_result_article=[]\n",
    "    content_result_image=[]\n",
    "    content_result_video=[]\n",
    "    #media_text_result=[]\n",
    "    media_image_result=[]\n",
    "    if text:\n",
    "        #if a text is given, calculate both multiomdal embedding and text embedding of the search query\n",
    "        txtembding_for_text_result =  await asyncio.create_task(exponential_backoff_retries(embedding_client, text, embedding_type='text_embedding'))\n",
    "        #mmembding_for_text_result =  await asyncio.create_task(exponential_backoff_retries(embedding_client, text, embedding_type='multimodal_embedding')) \n",
    "        txtembding_for_text_result=txtembding_for_text_result .text_embedding\n",
    "        #mmembding_for_text_result=mmembding_for_text_result.text_embedding\n",
    "        #find nearest neighbours both from text embedding and multimodal embedding\n",
    "        if article_filter_query!=\"\":\n",
    "            content_result_article = await asyncio.create_task(get_content_nearest_neighbors(txtembding_for_text_result, content_table, dataset,content_source_embedding_column,project_id,top_k=top_k, filter_query=article_filter_query))\n",
    "        if image_filter_query!=\"\":\n",
    "            content_result_image = await asyncio.create_task(get_content_nearest_neighbors(txtembding_for_text_result, content_table, dataset,content_source_embedding_column,project_id,top_k=top_k,filter_query=image_filter_query))\n",
    "        if video_filter_query!=\"\":\n",
    "            content_result_video = await asyncio.create_task(get_content_nearest_neighbors(txtembding_for_text_result, content_table, dataset,content_source_embedding_column,project_id,top_k=top_k, filter_query=video_filter_query))\n",
    "        \n",
    "        #media_text_result = await asyncio.create_task(get_media_nearest_neighbors(mmembding_for_text_result, mm_table, dataset,mm_source_embedding_column,project_id,top_k=top_k))\n",
    "               \n",
    "    if image_file:\n",
    "        \n",
    "        #if an image is given convert image to 64bytestring and extract embedding\n",
    "        mmembding_for_image_result = await asyncio.create_task(exponential_backoff_retries(embedding_client, text=\"\",image_file=image_file, embedding_type='multimodal_embedding'))\n",
    "        mmembding_for_image_result=mmembding_for_image_result.image_embedding\n",
    "        #find nearest neighbours both from multimodal embedding\n",
    "        media_image_result = await asyncio.create_task(get_media_nearest_neighbors(mmembding_for_image_result, mm_table, dataset,mm_source_embedding_column,project_id,top_k=top_k))\n",
    "        media_image_result=media_image_result   \n",
    " \n",
    "    final_merged_list=merge_result(content_result_article+content_result_image+content_result_video+media_image_result)\n",
    "    return final_merged_list \n",
    "\n",
    "\n",
    "@functions_framework.http\n",
    "async def search_content_function(request):\n",
    " \n",
    "    result = await get_nearest_contet(request) \n",
    "    return result#[0],result[1],result[2]\n",
    "\n",
    "# @functions_framework.http\n",
    "# def search_content_function(request):\n",
    "#     \"\"\"This is the entry point for the Cloud Function.\"\"\"\n",
    "#     try:\n",
    "#         loop = asyncio.get_event_loop()\n",
    "#     except RuntimeError as e:\n",
    "#         # If no event loop is running, create a new event loop for this thread\n",
    "#         loop = asyncio.new_event_loop()\n",
    "#         asyncio.set_event_loop(loop)\n",
    "#     result = loop.run_until_complete(get_nearest_contet(request))\n",
    "#     return result\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6536c67-0ab9-4df1-8fa0-c0cfbb94c69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Open an image file\n",
    "with open(\"data/51673.jpg\", \"rb\") as image_file:\n",
    "    image_byte_string = image_file.read()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a9276915-7d88-45ec-b268-edfe571db894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unittest.mock import Mock\n",
    "import json\n",
    "\n",
    "# Your input data as a dictionary\n",
    "data = {\"search_query\":\"curtis sittenfeld\",\"image_file\":image_byte_string,\"project\":\"nine-quality-test\",\"region\":\"us-central1\",\n",
    "        \"filter_image\":\"1\",\n",
    "         \"filter_video\":\"0\",\n",
    "        \"filter_article\":\"0\"\n",
    "       }\n",
    "\n",
    "# Simulating an HTTP request with the mock object\n",
    "mock_request = Mock()\n",
    "mock_request.get_json.return_value = data  # Mock the get_json method to return your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54cc2f6b-f674-4387-bda1-1e1f119ae8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "request_json = mock_request.get_json(silent=True)\n",
    "image_file = request_json.get('image_file') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5058338-391d-49c2-8c50-ee52c6cf497d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c71cc35d-da60-4a8d-97d2-c5eb437b5a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625638961791992\n",
      "3.0487136840820312\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "x= await search_content_function(mock_request)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7eb0097e-3e4a-4ab7-bc11-81c022eaac12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
