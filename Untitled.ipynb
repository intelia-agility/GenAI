{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712d3cb2-b281-47e1-936f-5a5d64266512",
   "metadata": {},
   "source": [
    "#this uses stuffing. stuffing is a bit slow for large docs and get context limit error when too long\n",
    "#https://github.com/google/generative-ai-docs/blob/main/examples/gemini/python/langchain/Gemini_LangChain_Summarization_WebLoad.ipynb\n",
    "#https://github.com/google/generative-ai-docs/blob/main/examples/gemini/python/langchain/Gemini_LangChain_Summarization_WebLoad.ipynb\n",
    "\n",
    "soloution:\n",
    "#using 2 prompts and refine method:\n",
    "#https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc04824b-b5e7-4615-bd0d-4124e4b8d57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_community import BigQueryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "#from langchain.llms import VertexAI as langchain_vertexai\n",
    "from langchain_google_vertexai import VertexAI as langchain_vertexai\n",
    "from langchain import PromptTemplate\n",
    "from pathlib import Path as p\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28305f6-be4b-472f-9250-ef6d518aef51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"nine-quality-test\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ae5ffecb-194d-4d70-95f2-a29a930dbca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question_prompt_template = \"\"\"\n",
    "                  Summarize the given text by high lighting most important information.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    " \n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = question_prompt_template\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "982da458-4cf9-42ca-806b-7c8ed8426076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_query_str='select unique_id,content, chunk from `nine-quality-test.vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` order by unique_id, chunk asc '\n",
    "metadata_columns=[\"unique_id\",\"chunk\"]\n",
    "page_content_columns=[\"content\"]\n",
    "loader = BigQueryLoader(\n",
    "        query=source_query_str, project=PROJECT_ID, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "    )\n",
    "    \n",
    "documents = []\n",
    "documents.extend(loader.load())\n",
    "\n",
    "docs=[[doc] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c33089cc-2a89-4aa5-ad77-38aaa1a8fbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs1=[doc for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3f36326-a9ed-4aa5-a1a4-850932960d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertex_llm_text = langchain_vertexai(model_name=\"gemini-1.5-pro-002\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cd810ceb-e263-4793-b673-74a94ddbbe56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "refine_chain = load_summarize_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028e08f-7131-4e04-b157-80f29df04d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outputs = refine_chain({\"input_documents\": documents})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "28b88632-882b-4f22-8415-f191ff0f9920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_docs(llm_chain,docs):\n",
    "    \"\"\"\n",
    "    function to summarize chunked documents\n",
    "    Args:\n",
    "        llm_chain: a langchain summarize chain\n",
    "        docs: chunked documents\n",
    "    Output:\n",
    "        summaries: list of summarized documents\n",
    "    \"\"\"\n",
    "    #summarize all chunks in one go\n",
    "    summary = llm_chain.batch(docs)\n",
    "\n",
    "    summaries=[]\n",
    "    #extract summaries\n",
    "    for summarized_doc in summary:\n",
    "        print(summarized_doc)\n",
    "        summaries.append(summarized_doc['output_text'])\n",
    "\n",
    "    return summaries\n",
    "\n",
    "#summarize splited complaints\n",
    "summary= summarize_docs(refine_chain,docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ce502cf7-45e5-4141-a964-a69496892499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Leeth Reynolds, a retired woman driving a modest Mazda CX-30, is unexpectedly working as a courier.  She was a close friend of Ridgeway, who was even a pallbearer at her husband's funeral.  The contrast between her current work and Ridgeway's flashy car is highlighted, implying a betrayal of trust.\\n\"]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summarize splited complaints\n",
    "summary= summarize_docs(refine_chain,[docs1])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0afc8ae8-aee1-4df5-b6e3-38854f0ec907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_mp_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m     final_refine_data\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     13\u001b[0m pdf_refine_summary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(final_refine_data)\n\u001b[0;32m---> 14\u001b[0m pdf_refine_summary \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_mp_summary\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(\n\u001b[1;32m     15\u001b[0m     by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m )  \u001b[38;5;66;03m# sorting the dataframe by filename and page_number\u001b[39;00m\n\u001b[1;32m     17\u001b[0m pdf_refine_summary\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m pdf_refine_summary\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf_mp_summary' is not defined"
     ]
    }
   ],
   "source": [
    "final_refine_data = []\n",
    "for doc, out in zip(\n",
    "    refine_outputs[\"input_documents\"], refine_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"combined_id\"] = p(doc.metadata[\"combined_id\"]).stem    \n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_refine_data.append(output)\n",
    "    \n",
    "    \n",
    "    \n",
    "pdf_refine_summary = pd.DataFrame.from_dict(final_refine_data)\n",
    "pdf_refine_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"combined_id\"]\n",
    ")  # sorting the dataframe by filename and page_number\n",
    "pdf_refine_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_refine_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0b12e2d-7c65-468c-9128-36dd7f9b6bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p5eie4', 'vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HBB_360p'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(pdf_refine_summary['combined_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c408ec5f-97b0-4583-821f-26accf8778c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_id</th>\n",
       "      <th>chunks</th>\n",
       "      <th>concise_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: The video begins with a clos...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* **Focus:** The 60 M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:00:35] Tara Brown explai...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* Chris Ridgeway, a k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:01:41] The voiceover sta...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* **Ridgeway confesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:02:18] The voiceover exp...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* **Ridgeway, a forme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:03:26] A man in a dark b...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* A man describes a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:04:14] Ridgeway is shown...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* **Ridgeway, a finan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:05:02] A bald man in a d...</td>\n",
       "      <td>* **Turner**, a bald man in a green hoodie, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:05:34] The voiceover exp...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* **Ridgeway acted as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...</td>\n",
       "      <td>content: Content: [00:07:06] A woman with shor...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* Leeth Reynolds, a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         combined_id  \\\n",
       "1  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "2  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "3  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "4  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "5  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "6  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "7  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "8  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "9  vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HB...   \n",
       "\n",
       "                                              chunks  \\\n",
       "1  content: Content: The video begins with a clos...   \n",
       "2  content: Content: [00:00:35] Tara Brown explai...   \n",
       "3  content: Content: [00:01:41] The voiceover sta...   \n",
       "4  content: Content: [00:02:18] The voiceover exp...   \n",
       "5  content: Content: [00:03:26] A man in a dark b...   \n",
       "6  content: Content: [00:04:14] Ridgeway is shown...   \n",
       "7  content: Content: [00:05:02] A bald man in a d...   \n",
       "8  content: Content: [00:05:34] The voiceover exp...   \n",
       "9  content: Content: [00:07:06] A woman with shor...   \n",
       "\n",
       "                                     concise_summary  \n",
       "1  BULLET POINT SUMMARY:\\n\\n* **Focus:** The 60 M...  \n",
       "2  BULLET POINT SUMMARY:\\n\\n* Chris Ridgeway, a k...  \n",
       "3  BULLET POINT SUMMARY:\\n\\n* **Ridgeway confesse...  \n",
       "4  BULLET POINT SUMMARY:\\n\\n* **Ridgeway, a forme...  \n",
       "5  BULLET POINT SUMMARY:\\n\\n* A man describes a c...  \n",
       "6  BULLET POINT SUMMARY:\\n\\n* **Ridgeway, a finan...  \n",
       "7  * **Turner**, a bald man in a green hoodie, is...  \n",
       "8  BULLET POINT SUMMARY:\\n\\n* **Ridgeway acted as...  \n",
       "9  BULLET POINT SUMMARY:\\n\\n* Leeth Reynolds, a w...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_refine_summary[pdf_refine_summary['combined_id']=='vlt_video_extract_SIXTY_MINUTES_60MI23_11_A_HBB_360p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a966358-ea80-4690-b7fc-2aa916777163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jupyter/GenAI/data/practitioners_guide_to_mlops_whitepaper.pdf',\n",
       " <http.client.HTTPMessage at 0x7efea10bcc10>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path as p\n",
    "import urllib\n",
    "import warnings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "data_folder = p.cwd() / \"data\"\n",
    "p(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_url = \"https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf\"\n",
    "pdf_file = str(p(data_folder, pdf_url.split(\"/\")[-1]))\n",
    "\n",
    "urllib.request.urlretrieve(pdf_url, pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30ac4fd8-ba02-44c5-8ad2-2a0859189dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "pages = pdf_loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6df977cb-6e85-4583-a4c9-c76b5e137f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7136482b-abe4-4b6b-aa6f-1e2bf2424fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "refine_chain = load_summarize_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36f37c28-db66-4a7e-a724-97895a437196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437cbed-6459-4783-9e92-9029eb63fe5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a2328-b640-4468-a7c0-546df37175ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e52cc-db8b-49b5-bfcf-f9dc32f77ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc6aa5-10d0-4886-9fa6-8d40efd982fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b26439c-d00c-4470-bb8e-efd1818e09d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "refine_outputs = refine_chain({\"input_documents\": pages})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5ffa15b-eedd-4e04-b40e-0b867fa3a9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_refine_data = []\n",
    "for doc, out in zip(\n",
    "    refine_outputs[\"input_documents\"], refine_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_refine_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "861e50e7-3c91-4c89-8f81-9c574b0b3053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>concise_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Practitioners guide to MLOps: \\nA framework fo...</td>\n",
       "      <td>This white paper, \"Practitioner's Guide to MLO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Table of Contents\\nExecutive summary 3\\nOvervi...</td>\n",
       "      <td>BULLET POINT SUMMARY:\\n\\n* **MLOps Lifecycle:*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Executive summary\\nAcross industries, DevOps a...</td>\n",
       "      <td>* **MLOps Defined:** MLOps addresses the need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>4\\nOrganizations can use the framework to iden...</td>\n",
       "      <td>* **Framework for ML Platform Adoption:** Orga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>5\\nprocesses in place is one of the differenti...</td>\n",
       "      <td>* **ML Engineering's Importance:**  High-perfo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file_name file_type  page_number  \\\n",
       "0  practitioners_guide_to_mlops_whitepaper      .pdf            0   \n",
       "1  practitioners_guide_to_mlops_whitepaper      .pdf            1   \n",
       "2  practitioners_guide_to_mlops_whitepaper      .pdf            2   \n",
       "3  practitioners_guide_to_mlops_whitepaper      .pdf            3   \n",
       "4  practitioners_guide_to_mlops_whitepaper      .pdf            4   \n",
       "\n",
       "                                              chunks  \\\n",
       "0  Practitioners guide to MLOps: \\nA framework fo...   \n",
       "1  Table of Contents\\nExecutive summary 3\\nOvervi...   \n",
       "2  Executive summary\\nAcross industries, DevOps a...   \n",
       "3  4\\nOrganizations can use the framework to iden...   \n",
       "4  5\\nprocesses in place is one of the differenti...   \n",
       "\n",
       "                                     concise_summary  \n",
       "0  This white paper, \"Practitioner's Guide to MLO...  \n",
       "1  BULLET POINT SUMMARY:\\n\\n* **MLOps Lifecycle:*...  \n",
       "2  * **MLOps Defined:** MLOps addresses the need ...  \n",
       "3  * **Framework for ML Platform Adoption:** Orga...  \n",
       "4  * **ML Engineering's Importance:**  High-perfo...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_refine_summary = pd.DataFrame.from_dict(final_refine_data)\n",
    "pdf_refine_summary = pdf_refine_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the dataframe by filename and page_number\n",
    "pdf_refine_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_refine_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5170b481-e645-4d2a-99dc-b49aaa871a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This white paper, \"Practitioner's Guide to MLOps,\" published in May 2021 by Khalid Salama, Jarek Kazmierczak, and Donna Schut, presents a framework for continuous delivery and automation of machine learning (MLOps).  It aims to guide practitioners in implementing MLOps practices to streamline the ML lifecycle, enabling faster and more reliable deployment of ML models.\n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **MLOps Lifecycle:** The document details the entire MLOps lifecycle, from building an ML-enabled system to continuous monitoring and management.\n",
      "* **Core Capabilities:** Key MLOps capabilities are outlined, including experimentation, data processing, model training, evaluation, serving, online experimentation, monitoring, pipelines, registry, dataset & feature repositories, and metadata tracking.\n",
      "* **MLOps Processes Deep Dive:** The document provides an in-depth explanation of the following MLOps processes:\n",
      "    * ML development\n",
      "    * Training operationalization\n",
      "    * Continuous training\n",
      "    * Model deployment\n",
      "    * Prediction serving\n",
      "    * Continuous monitoring\n",
      "* **Data and Model Management:**  Focuses on managing datasets, features, and models, including metadata tracking and model governance.  Feature and dataset management are specifically addressed within data management.  Similarly, ML metadata tracking and model governance are discussed under model management. \n",
      "\n",
      "\n",
      "* **MLOps Defined:** MLOps addresses the need for standardized processes and technologies to build, deploy, and operate ML systems efficiently and reliably, similar to DevOps and DataOps.\n",
      "* **Google Cloud AI Adoption Framework Context:** This document expands on the \"scale\" and \"automate\" themes within Google Cloud's AI Adoption Framework, focusing on operationalizing ML systems.\n",
      "* **Scale and Automate:**  \"Scale\" refers to leveraging cloud-managed ML services for handling large datasets and numerous jobs. \"Automate\" emphasizes efficient, frequent, and reliable deployment and operation of data processing and ML pipelines.\n",
      "* **MLOps Framework Introduced:** The document presents an MLOps framework with core processes and technical capabilities to guide organizations in establishing mature MLOps practices.\n",
      "* **Benefits of MLOps Adoption:**  Improved team collaboration, increased system reliability and scalability, shorter development cycles, and greater business value from ML investments.\n",
      "* **Target Audience:**  Technology leaders, enterprise architects, and teams seeking practical MLOps implementation details. Assumes basic ML and CI/CD knowledge.\n",
      "* **Two-Part Structure:** Part 1 provides a general MLOps lifecycle overview. Part 2 offers a deep dive into specific MLOps processes and capabilities like continuous training, model deployment, and performance monitoring.\n",
      "\n",
      "\n",
      "* **Framework for ML Platform Adoption:** Organizations can leverage a framework to pinpoint gaps in their current ML platforms and enhance scalability and automation, aligning with Google's AI Adoption Framework. Cost-benefit analysis is crucial for deciding the extent of framework adoption.\n",
      "\n",
      "* **MLOps Bottlenecks:**  Despite the importance of AI/ML, deploying and operating them effectively remains a major challenge.  Many organizations struggle to move beyond the pilot phase and deploy models into production.\n",
      "\n",
      "* **Reasons for Deployment Failures:** Several factors contribute to these struggles:\n",
      "    * Manual, one-off work and a lack of reusable components.\n",
      "    * Difficult handoffs between data scientists and IT teams.\n",
      "    * Shortages of skilled talent.\n",
      "    * Difficulties with deployment, scaling, and version control of models.\n",
      "    * Lack of robust change management and governance processes.\n",
      "\n",
      "* **Need for Structured Approach:**  Successful MLOps requires a move away from ad-hoc development and towards integrating ML with broader IT initiatives like DataOps and DevOps.  Sound software engineering practices are essential, acknowledging the unique challenges of operationalizing ML compared to other software types.\n",
      "\n",
      "* **Benefits of Automated MLOps:**  An automated ML process not only facilitates successful model deployment but also helps manage risks as the number of ML applications scales and ensures alignment with business objectives.  Standard frameworks and processes are key for successful scaling, as highlighted by McKinsey's research.\n",
      "\n",
      "\n",
      "* **ML Engineering's Importance:**  High-performing ML teams prioritize robust processes. ML engineering addresses the complex task of developing and deploying production-ready ML systems, going beyond traditional software engineering.\n",
      "\n",
      "* **ML Engineering Complexities:** These include data preparation and maintenance, performance monitoring, ongoing experimentation, model retraining, preventing training-serving skew, and addressing fairness and security concerns.\n",
      "\n",
      "* **MLOps Defined:**  MLOps is a methodology within ML engineering that combines ML system development with operations. It aims to formalize and automate key steps in building ML systems, enabling rapid and reliable deployment.\n",
      "\n",
      "* **MLOps vs. DevOps/DataOps:**  Similar to DevOps and DataOps, MLOps supports ML development and deployment.  However,  MLOps uniquely addresses challenges like data changes, model updates, and adversarial attacks.\n",
      "\n",
      "* **MLOps Benefits:** Implementing MLOps leads to shorter development cycles, improved team collaboration, increased system reliability and performance, streamlined operations, and better ROI for ML projects.\n",
      "\n",
      "\n",
      "* **ML-enabled systems require multifaceted development:**  Combining data engineering, machine learning (ML) engineering, and application engineering.\n",
      "* **Data engineering is foundational:**  It involves ingesting, integrating, curating, and refining data, and is crucial for successful analytics and ML initiatives.  Poor data engineering can derail downstream projects.\n",
      "* **ML models integrate into applications:** Deployed models are components of larger application systems (business intelligence, line of business apps, etc.) and don't operate in isolation.\n",
      "* **Integration focuses on effective use and monitoring:**  Ensuring the deployed model is used effectively within the application and monitoring its performance is critical.\n",
      "* **Business KPI monitoring is essential:** Tracking relevant business KPIs (e.g., click-through rate, revenue uplift) helps understand the model's business impact and allows for adaptation.\n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **ML Development:**  Focuses on creating a robust and reproducible model training pipeline, including data preparation, transformation, model training, and evaluation.\n",
      "* **Training Operationalization:** Automates the packaging, testing, and deployment of reliable training pipelines.\n",
      "* **Continuous Training:**  Enables repeated execution of the training pipeline based on new data, code changes, or scheduled runs.\n",
      "* **Model Deployment:**  Covers the packaging, testing, and deployment of trained models to a serving environment for experimentation and production use.\n",
      "\n",
      "\n",
      "* **Prediction Serving:** Focuses on deploying and using the trained ML model for inference in a production environment.\n",
      "* **Continuous Monitoring:** Tracks the deployed model's performance (effectiveness) and resource usage (efficiency) over time.\n",
      "* **Data and Model Management:**  A central function managing ML artifacts (data, models, etc.) to ensure auditability, traceability, and compliance.  It also promotes sharing, reuse, and discovery of these assets.\n",
      "* **MLOps Workflow:**  An iterative, not strictly sequential, process involving experimentation (ML development), leveraging managed data and models, and incorporating prediction serving and continuous monitoring.  Specific phases can be repeated or skipped as needed.\n",
      "\n",
      "\n",
      "* **Formalized Training Procedure:** The core output is a defined process encompassing data preprocessing, model architecture, and training settings.\n",
      "\n",
      "* **Continuous Training (Optional):** For ML systems requiring repeated retraining, a CI/CD pipeline automates the build, test, and deployment of the training process.  Retraining is triggered by new data or performance decay.  Resulting models and artifacts are tracked. Successful models become registered models.\n",
      "\n",
      "* **Model Management:** Registered models undergo annotation, review, and approval before deployment to production. This can range from automated processes in no-code solutions to custom CI/CD pipelines.\n",
      "\n",
      "* **Prediction Serving:** Deployed models serve predictions (online, batch, or streaming) and may also provide explanations and generate logs for monitoring.\n",
      "\n",
      "* **Continuous Monitoring:**  Monitors model effectiveness (detecting decay and drift) and service performance (latency, throughput, resource utilization, errors).\n",
      "\n",
      "* **MLOps Capabilities:** Implementing these processes requires core technical capabilities, potentially from an integrated platform, combined vendor tools, custom services, or a hybrid approach.\n",
      "\n",
      "* **Phased Deployment:** MLOps capabilities are usually adopted in stages, aligning with business priorities and technical maturity.  Initial focus is often on model development, deployment, and prediction serving. Continuous training and monitoring may be later additions.\n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **Foundational Capabilities:** Pre-existing IT infrastructure (compute, security, scaling) across multiple clouds or on-premises, ideally with ML accelerators.  These are leveraged for ML workflows.\n",
      "* **Configuration Management and CI/CD:** Standardized tools and processes for building, testing, releasing, and operating ML systems.\n",
      "* **Core MLOps Capabilities:**\n",
      "    * Experimentation\n",
      "    * Data processing\n",
      "    * Model training\n",
      "    * Model evaluation\n",
      "    * Model serving\n",
      "    * Online experimentation\n",
      "    * Model monitoring\n",
      "    * ML pipeline\n",
      "    * Model registry\n",
      "* **Cross-Cutting Capabilities:**\n",
      "    * ML metadata and artifact repository\n",
      "    * ML dataset and feature repository \n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **Experimentation:** Focuses on enabling data scientists and ML researchers to explore data, prototype models, and develop training routines.  Key features include:\n",
      "    * Version-controlled notebook environments.\n",
      "    * Experiment tracking for reproducibility.\n",
      "    * Data and model visualization and analysis tools.\n",
      "    * Dataset exploration and experiment review capabilities.\n",
      "    * Integration with other data and ML services.\n",
      "\n",
      "* **Data Processing:**  Centers on preparing and transforming data for ML at scale, both for development and production.  Key features include:\n",
      "    * Support for interactive and long-running data processing jobs.\n",
      "    * Diverse data connectors and encoding/decoding tools.\n",
      "    * Efficient data transformations and feature engineering for various data types.\n",
      "    * Scalable batch and stream processing.\n",
      "\n",
      "* **Model Training:**  Enables efficient and cost-effective training of ML models using powerful algorithms. (The provided text does not detail specific functionalities within this category).\n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **Model Training:**\n",
      "    * Scalable to large models and datasets.\n",
      "    * Supports common ML frameworks and custom runtimes.\n",
      "    * Enables large-scale distributed training across multiple GPUs and workers.\n",
      "    * Supports on-demand ML accelerators.\n",
      "    * Facilitates efficient hyperparameter tuning and optimization.\n",
      "    * Ideally includes AutoML features (feature selection/engineering, model architecture search).\n",
      "\n",
      "* **Model Evaluation:**\n",
      "    * Scalable batch scoring on evaluation datasets.\n",
      "    * Computes predefined or custom metrics on different data slices.\n",
      "    * Tracks model performance across continuous training runs.\n",
      "    * Visualizes and compares model performance.\n",
      "    * Provides tools for what-if analysis, bias detection, and fairness assessment.\n",
      "    * Enables model interpretation using explainable AI techniques.\n",
      "\n",
      "* **Model Serving:**\n",
      "    * Supports low-latency online prediction and high-throughput batch prediction.\n",
      "    * Supports common ML serving frameworks (TensorFlow Serving, TorchServe, Triton, etc.) and custom runtimes.\n",
      "    * Enables composite prediction routines with pre/post-processing.\n",
      "    * Allows efficient use of inference accelerators with autoscaling. \n",
      "\n",
      "\n",
      "* **Cost and Latency Optimization:**  Focuses on minimizing the cost and latency of model predictions.  Supports explaining model predictions using techniques like feature attribution.  Logs requests and responses for analysis.\n",
      "\n",
      "* **Online Experimentation:** Enables comparison of new models against existing ones in a production-like setting before full release. Supports canary/shadow deployments, A/B testing, and multi-armed bandit (MAB) tests. Integrates with the model registry to inform release decisions.\n",
      "\n",
      "* **Model Monitoring:** Tracks deployed model performance and identifies issues like staleness, data skew (schema anomalies, drifts, shifts), and resource utilization. Integrates with model evaluation when ground truth is available.\n",
      "\n",
      "* **ML Pipelines:**  Facilitates the automation and orchestration of complex ML training and prediction pipelines.  (The provided text cuts off before fully describing this capability.)\n",
      "\n",
      "\n",
      "* **ML Pipelines:**\n",
      "    * Triggered on demand, schedule, or events.\n",
      "    * Allow local debugging.\n",
      "    * Integrate with metadata tracking.\n",
      "    * Offer built-in and custom components.\n",
      "    * Run on various environments (local and cloud).\n",
      "    * May include GUI tools for design.\n",
      "\n",
      "* **Model Registry:**\n",
      "    * Centralized repository for managing model lifecycle.\n",
      "    * Registers, organizes, tracks, and versions models.\n",
      "    * Stores metadata and dependencies.\n",
      "    * Supports documentation (e.g., model cards).\n",
      "    * Integrates with evaluation and deployment tools.\n",
      "    * Governs model launching (review, approval, release, rollback).\n",
      "\n",
      "* **Dataset and Feature Repository:**\n",
      "    * Centralized storage for ML data assets.\n",
      "    * Promotes shareability, discoverability, and reusability.\n",
      "    * Ensures data consistency for training and inference.\n",
      "    * Reduces time spent on data preparation and feature engineering. \n",
      "\n",
      "\n",
      "* **ML Data Management:**  Focuses on enabling shareability, discoverability, reusability, and versioning of data assets. Supports real-time and batch ingestion, various data modalities (tabular, image, text), and feature versioning.  Management can be at the entity feature level (e.g., customer features) or the full dataset level (e.g., customer churn dataset).\n",
      "\n",
      "* **ML Metadata and Artifact Tracking:**  Manages information about ML artifacts (models, data schemas, evaluation results, etc.), including their location, type, properties, and associations. Enables traceability, lineage tracking, sharing of experiment parameters, and storage/access of artifacts.  Integrates with all other MLOps capabilities.\n",
      "\n",
      "* **Deep Dive of MLOps Processes:** Provides detailed descriptions of core MLOps processes, including key tasks, flow of control, artifacts created, and relationships between upstream and downstream processes.  Covers tasks like continuous training, model deployment, and performance monitoring.\n",
      "\n",
      "\n",
      "* **MLOps platforms:** Integrated platforms support both development and operational aspects of machine learning.\n",
      "* **Infrastructure management:**  Platforms are provisioned across multiple environments (development, test, staging, production) using tools like Terraform and IaC.  Each environment has its own resources and MLOps services.\n",
      "* **ML development core:** Experimentation is the central activity.\n",
      "* **Defined use case prerequisites:**  Experimentation begins with a well-defined use case including a clear task, measurable business impact, and defined evaluation metric. \n",
      "\n",
      "\n",
      "* **Goal:** Develop an effective prototype ML model and formalize the training procedure into an operational pipeline.\n",
      "* **Experimentation Steps:**\n",
      "    * Data discovery, selection, and exploration.\n",
      "    * Data preparation and feature engineering.\n",
      "    * Model prototyping and validation.\n",
      "* **Problem Refinement:** Experimentation can lead to adjustments in the problem definition (e.g., changing from regression to classification or choosing a different evaluation metric).\n",
      "* **Data Source:** Curated data assets from a dataset and feature repository.\n",
      "* **Key Success Factors:** Experiment tracking, reproducibility, and collaboration.  Leveraging past experiments can save time.\n",
      "* **Reproducibility Requirements:** Tracking experiment configurations including:\n",
      "    * Versioned training code.\n",
      "    * Model architecture and pre-trained modules.\n",
      "    * Hyperparameters and tuning information.\n",
      "    * Data splits (training, validation, testing).\n",
      "    * Evaluation metrics and validation procedures.\n",
      "* **Model Deployment:** If retraining isn't regularly required, the finalized model is submitted to a model registry for review, approval, and deployment. \n",
      "\n",
      "\n",
      "* **ML Development Outputs:**\n",
      "    * Notebooks for experimentation and visualization.\n",
      "    * Metadata and artifacts from experiments.\n",
      "    * Data schemas.\n",
      "    * Training data query scripts.\n",
      "    * Source code and configurations for data validation and transformation.\n",
      "    * Source code and configurations for model creation, training, and evaluation.\n",
      "    * Source code and configurations for the training pipeline workflow.\n",
      "    * Source code for unit and integration tests.\n",
      "\n",
      "* **Model Retraining and Continuous Training:**  ML models often require retraining with new data or code changes.  The focus shifts from deploying a single model to deploying a continuous training pipeline.  Version control (e.g., Git) is crucial for managing code, configurations, and applying software engineering best practices like code review, analysis, and automated testing. CI/CD workflows are used for pipeline deployment.\n",
      "\n",
      "* **Experimentation and Data Engineering:** Experimentation generates new features and datasets. Reusable data assets can be integrated into a feature/dataset repository via data engineering pipelines.  Experimentation often defines requirements for these upstream data engineering pipelines.\n",
      "\n",
      "* **Training Operationalization:** This involves building, testing, and deploying a repeatable ML training pipeline. Configurations specify the target environment, data sources, and service accounts for execution.\n",
      "\n",
      "* **Core MLOps Capabilities:** The text highlights key MLOps functionalities, including dataset & feature repositories, data processing, experimentation tracking, model training, a model registry, and ML metadata & artifact repositories.\n",
      "\n",
      "\n",
      "* ML pipelines undergo testing and staging before production deployment, the number of stages varying by organization.\n",
      "* Pipeline deployment processes depend on the implementation technology (no-code vs. code-first).\n",
      "* Code-first approaches utilize standard CI/CD processes and tools, offering more flexibility and control.\n",
      "* The CI/CD workflow includes:\n",
      "    * Unit testing the source code.\n",
      "    * Building and integration-testing the training pipeline.\n",
      "    * Storing build artifacts in a repository.\n",
      "* Key artifacts produced include:\n",
      "    * Executable components of the training pipeline (e.g., container images).\n",
      "    * Runtime representation of the training pipeline. \n",
      "* Core MLOps capability highlighted: ML pipelines. \n",
      "\n",
      "\n",
      "* **Continuous Delivery (CD):** Deploys tested training pipelines to a target environment for end-to-end testing before production release.  Pipelines are typically tested on a subset of production data in a non-production environment.  A fallback mechanism exists to revert to the previous model if the new one fails.\n",
      "* **Continuous Training:** Orchestrates and automates the execution of training pipelines. Retraining frequency depends on use case, business value, and cost.  Examples are given for image classification (infrequent retraining) vs. a recommendation system (frequent retraining).\n",
      "* **Training Pipeline Triggers:**\n",
      "    * Scheduled runs.\n",
      "    * Event-driven runs (e.g., new data, model decay).\n",
      "    * Manual invocation.\n",
      "* **Typical ML Training Pipeline Workflow:**\n",
      "    * **Data Ingestion:** Data is extracted based on criteria like date/time.\n",
      "    * **Data Validation:** Ensures data quality and prevents training on skewed or corrupted data.\n",
      "    * **Data Transformation:** Data is split (train/eval/test) and transformed/engineered.\n",
      "    * **Model Training and Tuning:** Model training and hyperparameter tuning are performed using training and evaluation data splits.\n",
      "\n",
      "\n",
      "* **Model Evaluation:**  Performance is assessed against test data using various metrics on different data partitions.\n",
      "* **Model Validation:** Evaluation results are checked to ensure they meet predefined performance criteria.\n",
      "* **Model Registration:** Validated models and their metadata are stored in a model registry.\n",
      "* **Continuous Training Pipeline:** Triggered retraining process extracts new data, executes the ML workflow (data processing, training, evaluation, validation), and registers the new model.  All runs and artifacts are tracked.\n",
      "* **Automated vs. Experimental Pipelines:** Automated pipelines emphasize data and model validation as gatekeepers due to their unattended nature. Data evolution can impact the pipeline's effectiveness over time.\n",
      "* **Diagram (Figure 7):**  A diagram (not included here) illustrates the continuous training process.\n",
      "\n",
      "\n",
      "* **Data Validation:** Detects anomalies in new training data by comparing it to the expected schema and reference statistics.  These anomalies can include new or missing features, changes in feature domains, and shifts in feature distributions.\n",
      "* **Model Validation:** Detects performance degradation or lack of improvement in new model candidates.  Uses various evaluation metrics, sensitivity analysis, calibration, and fairness indicators.\n",
      "* **Tracking and Lineage Analysis:**  Pipeline runs track metadata and artifacts for debugging, reproducibility, and lineage analysis.  This allows tracing a model back to its training dataset and related artifacts, including hyperparameters, evaluations, processed data, and data summaries.\n",
      "* **Model Registry:**  Upon successful validation, the pipeline registers the model candidate in a model registry.\n",
      "* **Automated Deployment:** Pipelines can automate deployment, creating an end-to-end training and deployment process.  Additional validations like model size, serving runtime, and latency are included for frequent deployments.\n",
      "* **Outputs:**  The process generates a trained and validated model stored in the model registry, as well as training metadata and artifacts (parameters, statistics, validation results, transformed data, evaluation metrics, checkpoints, and logs) stored in a repository.\n",
      "* **Core MLOps Capabilities Used:** Dataset & feature repository, ML metadata & artifact repository, data processing, model training, model evaluation, ML pipelines, and model registry. \n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **Pipeline Practicalities:** Creating a full machine learning pipeline (training to deployment) isn't always feasible due to organizational structures, with some organizations separating model training and deployment responsibilities.  Training pipelines often stop at model registration.\n",
      "* **Model Deployment Process:** After training, validation, and registration, models are deployed. This involves packaging, testing, deployment to a target environment, and potentially a model governance process.\n",
      "* **Simplified Deployment with No/Low-Code:** No/low-code solutions streamline deployment, often automating the process by referencing the model registry entry and its associated metadata and artifacts.\n",
      "* **Controlled Deployment:**  In some cases, more control over the deployment process may be desired, requiring a more hands-on approach than provided by no/low-code solutions.  The provided text excerpt cuts off mid-sentence before elaborating on this point.\n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **Complex CI/CD for Model Deployment:**  The text describes a sophisticated CI/CD process specifically designed for deploying machine learning models.\n",
      "* **Source Code and Model Integration:** The CI/CD system pulls source code for the model serving component from a repository and retrieves the trained model from a model registry.\n",
      "* **Standard CI Steps:**  The system performs standard CI tasks like integration, building, testing, and validation of the model serving service.\n",
      "* **CI Stage Testing:**  Testing during the CI stage includes:\n",
      "    * Model interface validation (input and output formats).\n",
      "    * Infrastructure compatibility checks (packages, accelerators).\n",
      "    * Model latency assessment.\n",
      "* **CD Stage and Progressive Delivery:** The CD stage uses progressive delivery strategies like canary, blue-green, and shadow deployments.\n",
      "* **Smoke Testing Focus:** Smoke tests concentrate on model service efficiency (latency, throughput) and error rates.\n",
      "* **Online Experimentation:**  Model effectiveness is evaluated in production by running online experiments. This involves gradually introducing the new model alongside the existing one and using live traffic for testing. \n",
      "\n",
      "\n",
      "* **Online Experimentation Crucial for ML Deployment:**  Deploying ML models is more complex than other software, requiring rigorous testing before full release.\n",
      "* **Progressive Delivery:** New models run alongside existing ones, serving a subset of users initially.  Performance determines eventual full deployment.\n",
      "* **A/B Testing and Multi-Armed Bandits (MAB):**  Used to measure the impact of new models against application goals.\n",
      "* **Canary/Shadow Deployment:** Facilitate online experiments.\n",
      "* **Prediction Serving:** Deployed models accept input (serving data) and return predictions.\n",
      "* **Key MLOps Capabilities:** Model serving, model registry, online experimentation, and an ML metadata & artifact repository.\n",
      "* **Model Deployment Assets:**  Include the model serving executable (e.g., container image) and online experiment metrics.\n",
      "\n",
      "\n",
      "* **Prediction Serving Methods:**  The serving engine supports online (REST, gRPC), streaming, offline batch, and embedded inference.\n",
      "* **Feature Retrieval:**  The serving engine can look up necessary feature values from a feature repository based on identifiers provided in the request.\n",
      "* **Explainability:** The system provides feature attributions to explain predictions and increase confidence in the ML system.\n",
      "* **Monitoring:** Inference logs and serving metrics are stored and monitored continuously to track model performance and detect potential decay.\n",
      "* **Core MLOps Capabilities:** Leverages a dataset & feature repository and model serving infrastructure.\n",
      "* **Typical Assets Produced:** Request-response payloads and feature attributions.\n",
      "\n",
      "\n",
      "* **Data Drift:** Models can perform poorly if the live data deviates from the training data (data drift). Upstream system changes can also cause bad predictions.\n",
      "* **Monitoring:** A monitoring engine uses inference logs to detect anomalies (skews and outliers).\n",
      "* **Monitoring Process:**\n",
      "    * Inference logs (request/response data) are sampled and stored.\n",
      "    * The engine periodically analyzes the logs, generates a schema, and calculates statistics.\n",
      "    * The engine compares the generated schema to a reference schema to identify schema skews. \n",
      "\n",
      "\n",
      "* **Model Monitoring Process:**  The system monitors production data (serving data) and compares it to the training data to identify anomalies and performance decay.\n",
      "* **Drift Detection:**  This involves checking for schema skew (differences in data structure) and distribution skew (changes in the distribution of feature values) between training and serving data. Other techniques like novelty detection, outlier detection, and feature attribution changes are also used.\n",
      "* **Performance Evaluation:** If ground truth (actual outcomes) is available for the serving data, the system evaluates the model's predictive accuracy in real-world scenarios.\n",
      "* **Alerting:** If anomalies or performance decay are detected, alerts are sent to trigger investigation or retraining.\n",
      "* **Data and Concept Drift:** The system aims to detect these. Data drift refers to differences between training and serving data, while concept drift indicates a changing relationship between input features and the target variable.\n",
      "* **Ground Truth Storage:**  In some cases, the system stores ground truth data from the production environment. This data can be used for continuous evaluation and future model training.\n",
      "* **Key Artifacts:** Anomalies detected during drift detection and evaluation metrics from continuous evaluation are key outputs of this process.\n",
      "* **Core MLOps Capabilities Used:**  Dataset & feature repository, model monitoring, and ML metadata & artifact repository.\n",
      "\n",
      "\n",
      "* **Model Serving Efficiency Monitoring:**  Focuses on resource utilization (CPU, GPU, memory), latency, throughput, and error rates.  This aids in performance maintenance/improvement and cost management.\n",
      "\n",
      "* **Data and Model Management:**  A core MLOps function enabling auditability, traceability, compliance, shareability, reusability, and discoverability of ML artifacts.\n",
      "\n",
      "* **Dataset and Feature Management:** Addresses challenges around creating, maintaining, and reusing high-quality data.  This helps avoid wasted time recreating datasets and ensures consistency.  It also mitigates training-serving skew, where differences between training and serving data impact model performance.\n",
      "\n",
      "* **Core MLOps Capabilities:** Include a dataset & feature repository, model registry, and ML metadata & artifact repository.\n",
      "\n",
      "\n",
      "* A unified repository stores machine learning (ML) features and datasets, enabling reuse across the MLOps environment.\n",
      "* This repository supports both batch serving (for experimentation, training, and batch prediction) and online serving (for real-time prediction).\n",
      "* Features, which are refined attributes of business entities (e.g., product, customer), are managed centrally.\n",
      "* This centralized feature management allows for standardized definition, storage, and access for training and serving.\n",
      "* The repository facilitates feature discoverability and reuse, preventing redundant feature creation.\n",
      "\n",
      "\n",
      "* **Feature Repositories Centralize Features:**  Establish a single source of truth for features used across multiple machine learning tasks. This facilitates sharing, consistency, and avoids training-serving skew.\n",
      "* **Benefits of Feature Repositories:**\n",
      "    * Consistent feature definitions across teams.\n",
      "    * Enables up-to-date feature values for training and serving.\n",
      "    * Streamlines the creation and sharing of new entities and features.\n",
      "    * Improves collaboration by providing a shared feature resource.\n",
      "* **Data Ingestion and Updates:** Feature repositories can be updated via batch ETL or streaming systems, with monitoring services providing feature statistics and metrics.\n",
      "* **Feature Repositories vs. Datasets:** Feature repositories store reusable feature values for various entities, while datasets are curated for specific ML tasks, including labeled data instances.  Features from the repository are combined to create datasets.\n",
      "* **Dataset Management Benefits:**\n",
      "    * Consistent dataset creation across different environments.\n",
      "    * Standardized dataset definitions and splits within a team.\n",
      "    * Facilitates metadata management and collaboration through annotation.\n",
      "    * Enables reproducibility and lineage tracking.\n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* **Scaling challenges:** Manually tracking numerous production models becomes difficult as organizations expand their ML efforts.\n",
      "* **Need for control:**  Robust model management is crucial for risk management, responsible AI implementation, and regulatory compliance.\n",
      "* **Model management as central to MLOps:** It encompasses both ML metadata tracking and model governance, influencing the entire ML lifecycle.\n",
      "* **Data quality and privacy:** Model management ensures data accuracy, addresses bias, and prevents privacy violations.\n",
      "* **Model evaluation and validation:** It includes assessing model effectiveness, fairness, and suitability for production deployment.\n",
      "* **Interpretability and explainability:** Model management promotes transparency by emphasizing model interpretability and explaining outcomes when necessary.\n",
      "* **Performance monitoring and reporting:**  Continuous evaluation, metric tracking, and reporting on deployed model performance are essential components.\n",
      "* **Traceability and reproducibility:** Model management enables tracing, debugging, and reproducing issues in training or prediction serving.\n",
      "* **ML metadata tracking is a key aspect:** Figure 13 likely illustrates the importance of metadata tracking within model management. \n",
      "\n",
      "\n",
      "* **ML Metadata Tracking:** Captures information about ML processes and artifacts, including execution details, environment configurations, and input parameters.  Stored artifacts include data splits, schemas, models, and evaluation metrics. Enables reproducibility, lineage tracing, searchability, and analysis of experiments.\n",
      "\n",
      "* **Benefits of ML Metadata Tracking:**  Facilitates experiment tracking for reproducibility, allows searching and discovering existing models and artifacts, supports adding annotations for improved discoverability, and provides tools for analyzing and comparing experiments to understand pipeline behavior and debug issues.\n",
      "\n",
      "* **Model Governance:**  Focuses on registering, reviewing, validating, and approving models for deployment. Processes can vary based on organizational needs and regulatory requirements, ranging from automated to manual. Includes reporting on the performance of deployed models.\n",
      "\n",
      "\n",
      "* **Model Governance Tasks:**  Model governance utilizes ML metadata and a model registry to perform several key tasks:\n",
      "    * **Store:**  Manage model versions, properties, and track changes for reproducibility.\n",
      "    * **Evaluate:** Compare new models against existing ones using metrics and business KPIs, including explainability analysis.\n",
      "    * **Check:** Review, request modifications, and approve models to mitigate various risks (business, legal, ethical, etc.).\n",
      "    * **Release:** Control model deployment strategies (e.g., canary, blue-green) and traffic allocation.\n",
      "    * **Report:**  Monitor and visualize model performance metrics gathered during continuous evaluation.\n",
      "\n",
      "* **Explainability and Accountability:**  Model governance emphasizes explainability, particularly for automated decisions, providing lineage and accountability for risk management and auditing.\n",
      "\n",
      "* **MLOps for Business Value:**  Building an effective ML system goes beyond creating a single model. It involves continuous operation, adaptation to changing business dynamics, and encompasses data management, training, evaluation, serving, monitoring, and metadata tracking.\n",
      "\n",
      "* **Streamlined Workflow:** A robust MLOps process helps organizations reduce time to market and improve the reliability, performance, scalability, and security of their ML systems.\n",
      "\n",
      "\n",
      "* The figure (Figure 15) depicts an end-to-end MLOps workflow.\n",
      "\n",
      "\n",
      "BULLET POINT SUMMARY:\n",
      "\n",
      "* The provided list points to further resources for learning and implementing MLOps on Google Cloud.\n",
      "* Resources include books, guides, courses, articles, and videos covering best practices, design patterns, and architectural examples.\n",
      "* Specific topics covered include introductory MLOps, continuous delivery and automation pipelines, using tools like TFX and Kubeflow Pipelines, and setting up MLOps environments.  A Coursera course on MLOps fundamentals is also listed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,c in pdf_refine_summary.iterrows():\n",
    "    print(c[\"concise_summary\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4317371-64a9-4b55-bc45-401eb1e1b582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
