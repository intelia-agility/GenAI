{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9a08143-9e46-4780-9ff9-26caf89436dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from google.cloud import functions_v2\n",
    "from google.cloud import aiplatform\n",
    "from EmbeddingPredictionClient import EmbeddingPredictionClient  \n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "import time\n",
    " \n",
    "\n",
    "# Initialize the EmbeddingPredictionClient outside the function for reuse\n",
    "embedding_client = EmbeddingPredictionClient(project='nine-quality-test' , location=\"us-central1\",api_regional_endpoint=\"us-central1-aiplatform.googleapis.com\")\n",
    "\n",
    "def exponential_backoff_retries(client, text=None, image_file=None, max_retries=5, embedding_type=None):\n",
    "    \"\"\"\n",
    "    This function applies exponential backoff with retries to the API calls.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Try to get the embedding from the client\n",
    "            if embedding_type==\"multimodal_embedding\":\n",
    "                    return client.get_multimodal_embedding(text, image_file)\n",
    "            elif embedding_type==\"text_embedding\":\n",
    "                    return client.get_text_embedding(text)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            backoff_delay = min(2 ** attempt + random.uniform(0, 1), 32)  # Exponential backoff with jitter\n",
    "            print(f\"Attempt {attempt} failed with error {e}. Retrying in {backoff_delay:.2f} seconds...\")\n",
    "            time.sleep(backoff_delay)  # Wait before retrying\n",
    "\n",
    "    raise Exception(\"Max retries reached. Could not complete the request.\")\n",
    "\n",
    "def search_content_function(request):\n",
    "    \"\"\"\n",
    "    Cloud Function entry point. This function handles the incoming request, \n",
    "    performs exponential backoff retries, and returns the embedding response.\n",
    "    \"\"\"\n",
    "    # Parse the incoming request to extract text or image file\n",
    "    request_json = request.get_json(silent=True)\n",
    "    text = request_json.get('text')\n",
    "    image_file = request_json.get('image_file')  # Assume it's the path or base64 string of the image\n",
    "\n",
    "    if not text and not image_file:\n",
    "        return 'Error: At least one of \"text\" or \"image_file\" must be provided.', 400\n",
    "\n",
    "    try:\n",
    "        # Retry logic with exponential backoff to calculate query embeddings\n",
    "        result = exponential_backoff_retries(embedding_client, text, image_file)\n",
    "        \n",
    "        # Respond with the successful embedding response\n",
    "        return {\n",
    "            \"text_embedding\": result.text_embedding,\n",
    "            \"image_embedding\": result.image_embedding\n",
    "        }, 200\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle failure after max retries\n",
    "        return f\"Error: {str(e)}\", 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ebbc3b-e248-4495-a7aa-7eccf2bf7e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response=exponential_backoff_retries(embedding_client, 'bedroom with gray carpet', embedding_type='text_embedding').text_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1d34e2-301c-4c5f-aee9-57321b21f094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response=exponential_backoff_retries(embedding_client, 'this is a test', embedding_type='multimodal_embedding').text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b51d1b8-ea7c-49b2-b136-bd0ce08f06fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_nearest_neighbors(query_embedding, table, dataset,source_embedding_column,project_id,top_k=50):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery.\"\"\"\n",
    "    \n",
    "    # Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "if 1==1: \n",
    "    # SQL for finding the nearest neighbors using cosine similarity\n",
    "#     sql = f\"\"\"  \n",
    "#          SELECT\n",
    "#           base.content as content,  \n",
    "#           base.combined_id as combined_id,\n",
    "#           distance  -- The computed distance (similarity score) between the embeddings\n",
    "#         FROM\n",
    "#           VECTOR_SEARCH(     \n",
    "#             TABLE `{dataset}.{table}`, --source embedding table\n",
    "#             '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "#             -- Use the query embedding computed in the previous step\n",
    "#              (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "#             -- Return top-k closest matches (adjust k as necessary)\n",
    "#             top_k =>{ top_k *10 } -- Top k*10 most similar matches based on distance\n",
    "#           )\n",
    "#           order by distance desc\n",
    " \n",
    "#     \"\"\"\n",
    "    \n",
    "    sql1 = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.content as content,  \n",
    "              search_results.base.combined_id as combined_id,\n",
    "              search_results.distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              b.asset_id,\n",
    "              b.headline,\n",
    "              b.description,\n",
    "              ROW_NUMBER() OVER (PARTITION BY b.asset_id ORDER BY distance) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                TABLE `{dataset}.{table}`, --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  } -- Top k most similar matches based on distance\n",
    "              ) search_results\n",
    "              --this part should be removed later\n",
    "              inner join   `nine-quality-test.vlt_media_content_prelanding.vlt_combined_media_content` b\n",
    "              on search_results.base.combined_id =b.combined_id  \n",
    "          )\n",
    "          -- Step 2: Aggregate relevance per document (original_document_id)\n",
    "            ,aggregated_results AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    COUNT(*) AS chunk_count,  -- The number of chunks for this document\n",
    "                    SUM(distance) AS total_distance,  -- Sum of the distances for this document's chunks\n",
    "                    AVG(distance) AS avg_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            ),\n",
    "\n",
    "            -- Step 3: Rank the documents by relevance (number of chunks and sum of distances)\n",
    "            ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    chunk_count,\n",
    "                    total_distance,\n",
    "                    avg_distance,\n",
    "                    ROW_NUMBER() OVER (ORDER BY chunk_count DESC, total_distance ASC) AS final_rank  -- Rank by chunk_count and then distance\n",
    "\n",
    "                FROM aggregated_results\n",
    "            )\n",
    "\n",
    "            -- Step 4: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id,  \n",
    "                sr.headline,\n",
    "                sr.description,\n",
    "                sr.combined_id,\n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id) AS IDX,\n",
    "               -- sr.distance,\n",
    "                final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            WHERE rd.final_rank <= 50 -- Return the top-k documents based on chunk relevance\n",
    "            ORDER BY rd.final_rank, sr.rank_within_document  -- Order by document relevance and chunk rank\n",
    "            )\n",
    "            WHERE IDX=1\n",
    " \n",
    "    \"\"\"       \n",
    "    \n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql1)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "    \n",
    "output=[]\n",
    "for row in results:\n",
    "    output.append({'asset_id':row['asset_id'], 'headline':row['headline'],'description':row['description']})\n",
    "    \n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6660480e-c2d5-4a07-af58-8bb558cc2faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a342f7b6-38a6-4315-9c03-ace3f38d1f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_embedding=response\n",
    "top_k=50\n",
    "project_id=PROJECT_ID\n",
    "dataset='langchain_dataset'\n",
    "table='vlt_media_content_text_test_for_search'\n",
    "source_embedding_column='ml_generate_embedding_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7481b97f-c335-4046-9043-e962922e9724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5602688789367676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
