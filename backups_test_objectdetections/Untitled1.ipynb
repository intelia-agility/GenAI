{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2258d2c5-fe57-45ea-abc7-e165cab3422b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import functions_framework\n",
    "import time\n",
    "import random\n",
    "from EmbeddingPredictionClient import EmbeddingPredictionClient  \n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "async def exponential_backoff_retries(client, text=None, image_file=None, max_retries=5, embedding_type=None):\n",
    "    \"\"\"\n",
    "    This function applies exponential backoff with retries to the API calls.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Try to get the embedding from the client\n",
    "            if embedding_type==\"multimodal_embedding\":\n",
    "                    return client.get_multimodal_embedding(text, image_file)\n",
    "            elif embedding_type==\"text_embedding\":\n",
    "                    return client.get_text_embedding(text)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            backoff_delay = min(2 ** attempt + random.uniform(0, 1), 32)  # Exponential backoff with jitter\n",
    "            print(f\"Attempt {attempt} failed with error {e}. Retrying in {backoff_delay:.2f} seconds...\")\n",
    "            time.sleep(backoff_delay)  # Wait before retrying\n",
    "\n",
    "    raise Exception(\"Max retries reached. Could not complete the request.\")\n",
    "\n",
    "    \n",
    "async def generate_query_embedding(client,text=None,image_file=None, embedding_type=None):\n",
    "    try:\n",
    "        # Retry logic with exponential backoff to calculate query embeddings\n",
    "        result = exponential_backoff_retries(embedding_client, text, image_file, embedding_type)\n",
    "        \n",
    "        # Respond with the successful embedding response\n",
    "        return {\n",
    "            \"text_embedding\": result.text_embedding,\n",
    "            \"image_embedding\": result.image_embedding\n",
    "        }, 200\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle failure after max retries\n",
    "        return f\"Error: {str(e)}\", 500\n",
    "\n",
    "\n",
    "async def get_media_nearest_neighbors(query_embedding, table, dataset,source_embedding_column,project_id,top_k=50):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for multimodal embeddings.\"\"\"\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    #option=\"\"\"'{\"fraction_lists_to_search\": 0.01}'\"\"\"\n",
    "    sql = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.uri as uri,  \n",
    "              search_results.base.combined_multimodal_id as combined_id,\n",
    "              search_results.distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              search_results.base.asset_id ,\n",
    "              ROW_NUMBER() OVER (PARTITION BY search_results.base.asset_id ORDER BY distance) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                TABLE `{dataset}.{table}`, --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  }, -- Top k most similar matches based on distance\n",
    "                distance_type => 'COSINE'                 \n",
    "              ) search_results\n",
    "              \n",
    "          )\n",
    "          -- Step 2: Aggregate relevance per document (original_document_id)\n",
    "            ,aggregated_results AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    COUNT(*) AS chunk_count,  -- The number of chunks for this document\n",
    "                    SUM(distance) AS total_distance,  -- Sum of the distances for this document's chunks\n",
    "                    AVG(distance) AS avg_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            ),\n",
    "\n",
    "            -- Step 3: Rank the documents by relevance (number of chunks and sum of distances)\n",
    "            ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    chunk_count,\n",
    "                    total_distance,\n",
    "                    avg_distance,\n",
    "                    ROW_NUMBER() OVER (ORDER BY chunk_count DESC, total_distance ASC) AS final_rank  -- Rank by chunk_count and then distance\n",
    "\n",
    "                FROM aggregated_results\n",
    "            )\n",
    "\n",
    "            -- Step 4: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id,  \n",
    "                sr.uri,              \n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id) AS IDX,\n",
    "               -- sr.distance,\n",
    "                final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            WHERE rd.final_rank <= {top_k} -- Return the top-k documents based on chunk relevance\n",
    "            ORDER BY rd.final_rank, sr.rank_within_document  -- Order by document relevance and chunk rank\n",
    "            )\n",
    "            WHERE IDX=1\n",
    "    \"\"\"       \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "    \n",
    "    output=[]\n",
    "    for row in results:\n",
    "        output.append({'asset_id':row['asset_id'], 'uri':row['uri']})\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return output\n",
    "\n",
    "async def get_content_nearest_neighbors(query_embedding, table, dataset,source_embedding_column,project_id,top_k=50):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for text embeddings.\"\"\"\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    #option=\"\"\"'{\"fraction_lists_to_search\": 0.01}'\"\"\"\n",
    "    sql = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.content as content,  \n",
    "              search_results.base.combined_id as combined_id,\n",
    "              search_results.distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              b.asset_id,\n",
    "              b.headline,\n",
    "              b.description,\n",
    "              ROW_NUMBER() OVER (PARTITION BY b.asset_id ORDER BY distance) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                TABLE `{dataset}.{table}`, --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  }, -- Top k most similar matches based on distance\n",
    "                distance_type => 'COSINE'                  \n",
    "              ) search_results\n",
    "              --this part should be removed later\n",
    "              inner join   `nine-quality-test.vlt_media_content_prelanding.vlt_combined_media_content` b\n",
    "              on search_results.base.combined_id =b.combined_id  \n",
    "          )\n",
    "          -- Step 2: Aggregate relevance per document (original_document_id)\n",
    "            ,aggregated_results AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    COUNT(*) AS chunk_count,  -- The number of chunks for this document\n",
    "                    SUM(distance) AS total_distance,  -- Sum of the distances for this document's chunks\n",
    "                    AVG(distance) AS avg_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            ),\n",
    "\n",
    "            -- Step 3: Rank the documents by relevance (number of chunks and sum of distances)\n",
    "            ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    chunk_count,\n",
    "                    total_distance,\n",
    "                    avg_distance,\n",
    "                    ROW_NUMBER() OVER (ORDER BY chunk_count DESC, total_distance ASC) AS final_rank  -- Rank by chunk_count and then distance\n",
    "\n",
    "                FROM aggregated_results\n",
    "            )\n",
    "\n",
    "            -- Step 4: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id,  \n",
    "                sr.headline,\n",
    "                sr.description,\n",
    "                sr.combined_id,\n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id) AS IDX,\n",
    "                sr.distance,\n",
    "                final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            WHERE rd.final_rank <= {top_k} -- Return the top-k documents based on chunk relevance      \n",
    "            --and sr.asset_id like '%00261507986b0faf31c775597d2d24beb4381e43%'\n",
    "            ORDER BY rd.final_rank, sr.rank_within_document  -- Order by document relevance and chunk rank\n",
    "            )\n",
    "            WHERE IDX=1\n",
    "    \"\"\"       \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "    \n",
    "    output=[]\n",
    "    for row in results:\n",
    "        output.append({'asset_id':row['asset_id'], 'headline':row['headline'],'description':row['description']})\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return output\n",
    "\n",
    "def merge_result(combined_list):\n",
    "    # Step 2: Create a dictionary to merge by 'id'\n",
    "    merged_dict = {}\n",
    "\n",
    "    # Step 3: Iterate through the combined list and merge dictionaries by 'id'\n",
    "    for d in combined_list:\n",
    "        id_value = d['asset_id']\n",
    "\n",
    "        # If the id already exists in merged_dict, update it\n",
    "        if id_value in merged_dict:\n",
    "            merged_dict[id_value].update(d)\n",
    "        else:\n",
    "            # If the id doesn't exist, add the dictionary as it is\n",
    "            merged_dict[id_value] = d.copy()\n",
    "\n",
    "    # Step 4: Convert the merged dictionary back into a list\n",
    "    final_merged_list = list(merged_dict.values())\n",
    "    \n",
    "    return final_merged_list\n",
    "\n",
    "\n",
    "\n",
    "async def get_nearest_contet(request):\n",
    "    \"\"\"\n",
    "    Cloud Function entry point. This function handles the incoming request, \n",
    "    performs exponential backoff retries, and returns the embedding response.\n",
    "    \"\"\"\n",
    "    # Parse the incoming request to extract text or image file\n",
    "    # request_json = request.get_json(silent=True)\n",
    "    # text = request_json.get('text')\n",
    "    # image_file = request_json.get('image_file')  # Assume it's the path or base64 string of the image\n",
    "    # project = request_json.get('project')  \n",
    "    # region = request_json.get('region')  \n",
    "\n",
    "    project='nine-quality-test'\n",
    "    region=\"us-central1\"\n",
    "    text='Curtis Sittenfeld'\n",
    "    image_file=None\n",
    "    \n",
    "    top_k=50     \n",
    "    dataset='langchain_dataset'\n",
    "    content_table='vlt_media_content_text_test_for_search'\n",
    "    mm_table='vlt_imgvdo_multimodal_embeddings'\n",
    "    source_embedding_column='ml_generate_embedding_result'\n",
    "\n",
    "    # Initialize the EmbeddingPredictionClient outside the function for reuse\n",
    "    embedding_client = EmbeddingPredictionClient(project=project , location=region,api_regional_endpoint=region+\"-aiplatform.googleapis.com\")\n",
    "        \n",
    "    if not text and not image_file:\n",
    "        print('you are here')\n",
    "        return 'Error: At least one of \"text\" or \"image_file\" must be provided.', 400\n",
    "     \n",
    "    content_result=[]\n",
    "    media_text_result=[]\n",
    "    media_image_result=[]\n",
    "    if text:\n",
    "        #if a text is given, calculate both multiomdal embedding and text embedding of the search query\n",
    "        txtembding_for_text_result =  await asyncio.create_task(exponential_backoff_retries(embedding_client, 'Curtis Sittenfeld', embedding_type='text_embedding'))\n",
    "        mmembding_for_text_result =  await asyncio.create_task(exponential_backoff_retries(embedding_client, 'Curtis sittenfeld', embedding_type='multimodal_embedding')) \n",
    "        txtembding_for_text_result=txtembding_for_text_result .text_embedding\n",
    "        mmembding_for_text_result=mmembding_for_text_result.text_embedding\n",
    "        #find nearest neighbours\n",
    "        content_result = await asyncio.create_task(get_content_nearest_neighbors(txtembding_for_text_result, content_table, dataset,source_embedding_column,project_id,top_k=top_k))\n",
    "        dataset='vlt_media_embeddings_integration'\n",
    "        media_text_result = await asyncio.create_task(get_media_nearest_neighbors(mmembding_for_text_result, mm_table, dataset,source_embedding_column,project_id,top_k=top_k))\n",
    "        print('search is done')\n",
    "            \n",
    "    if image_file:\n",
    "        #if an image is given convert image to 64bytestring and extract embedding\n",
    "        mmembding_for_image_result = await asyncio.create_task(exponential_backoff_retries(embedding_client, 'Curtis sittenfeld', embedding_type='multimodal_embedding').image_embedding)\n",
    "        dataset='vlt_media_embeddings_integration'\n",
    "        media_image_result = await asyncio.create_task(get_media_nearest_neighbors(mmembding_for_image_result, mm_table, dataset,source_embedding_column,project_id,top_k=top_k))\n",
    "        media_image_result=media_image_result\n",
    "        \n",
    "    final_merged_list=merge_result(content_result+media_text_result+media_image_result)\n",
    "    return final_merged_list#, content_result, media_text_result, media_image_result\n",
    "\n",
    "# @functions_framework.http\n",
    "# async def search_content_function(request):\n",
    " \n",
    "#     result = await get_nearest_contet(request) \n",
    "#     print(len(result))\n",
    "#     return result#[0],result[1],result[2]\n",
    "\n",
    " \n",
    "\n",
    "@functions_framework.http\n",
    "def search_content_function(request):\n",
    " \n",
    "    \"\"\"This is the entry point for the Cloud Function.\"\"\"\n",
    "    if asyncio.get_event_loop().is_running():  # Check if an event loop is running\n",
    "        result = asyncio.ensure_future(get_nearest_contet(request))  # Schedule the coroutine\n",
    "    \n",
    "    else:\n",
    "        result = asyncio.run(get_nearest_contet(request) )  # If no event loop, use asyncio.run()\n",
    " \n",
    "   \n",
    "    return result#[0],result[1],result[2]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fe3488f-a593-476c-ac7b-bb5f9f9f0b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task pending name='Task-13' coro=<get_nearest_contet() running at /var/tmp/ipykernel_704482/203890480.py:255>>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=search_content_function('')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47c63fba-5222-4315-b6a2-c3aa448ed863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'await' outside async function (1693313153.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    return await result\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'await' outside async function\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "async def search_content_function(request):\n",
    "    # Simulate some asynchronous work, e.g., querying a database, calling AI APIs, etc.\n",
    "    await asyncio.sleep(2)\n",
    "    return {\"message\": \"Asynchronous content search completed!\"}\n",
    "\n",
    "def gcp_function_entry(request):\n",
    "    \"\"\"This is the entry point for the Cloud Function.\"\"\"\n",
    "    if asyncio.get_event_loop().is_running():  # Check if an event loop is running\n",
    "        result = asyncio.ensure_future(search_content_function(request))  # Schedule the coroutine\n",
    "        return result\n",
    "    else:\n",
    "        result = asyncio.run(search_content_function(request))  # If no event loop, use asyncio.run()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462ee51b-4526-461b-ae23-2d5b2a3c9d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=gcp_function_entry('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff497af6-d352-4e48-9a60-407278cd6512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task finished name='Task-6' coro=<search_content_function() done, defined at /var/tmp/ipykernel_704482/2276924639.py:4> result={'message': 'Asynchronous...ch completed!'}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b65ad7ff-3d67-47bf-8fcc-37efb5f507bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Asynchronous content search completed!'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await gcp_function_entry('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e5440-df92-473a-9961-06678692f150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
