{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2be1844-9ec6-47f4-b56f-0b5f1a7f605f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import functions_framework\n",
    "import time\n",
    "import random\n",
    "from EmbeddingPredictionClient import EmbeddingPredictionClient  \n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "async def exponential_backoff_retries(client, text=None, image_file=None, max_retries=5, embedding_type=None):\n",
    "    \"\"\"\n",
    "    This function applies exponential backoff with retries to the API calls.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Try to get the embedding from the client\n",
    "            if embedding_type==\"multimodal_embedding\":\n",
    "                    return client.get_multimodal_embedding(text, image_file)\n",
    "            elif embedding_type==\"text_embedding\":\n",
    "                    return client.get_text_embedding(text)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            backoff_delay = min(2 ** attempt + random.uniform(0, 1), 32)  # Exponential backoff with jitter\n",
    "            print(f\"Attempt {attempt} failed with error {e}. Retrying in {backoff_delay:.2f} seconds...\")\n",
    "            time.sleep(backoff_delay)  # Wait before retrying\n",
    "\n",
    "    raise Exception(\"Max retries reached. Could not complete the request.\")\n",
    "\n",
    "    \n",
    "async def generate_query_embedding(client,text=None,image_file=None, embedding_type=None):\n",
    "    try:\n",
    "        # Retry logic with exponential backoff to calculate query embeddings\n",
    "        result = exponential_backoff_retries(embedding_client, text, image_file, embedding_type)\n",
    "        \n",
    "        # Respond with the successful embedding response\n",
    "        return {\n",
    "            \"text_embedding\": result.text_embedding,\n",
    "            \"image_embedding\": result.image_embedding\n",
    "        }, 200\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle failure after max retries\n",
    "        return f\"Error: {str(e)}\", 500\n",
    "\n",
    "\n",
    "async def get_media_nearest_neighbors(query_embedding, table, dataset,source_embedding_column,project_id,top_k=50):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for multimodal embeddings.\"\"\"\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    options=\"\"\"'{\"fraction_lists_to_search\": 1}'\"\"\"\n",
    "    #options=\"\"\"'{\"use_brute_force\":true}' \"\"\"\n",
    "\n",
    "    sql = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.uri as fileUri,  \n",
    "              search_results.base.combined_multimodal_id as unique_id,\n",
    "              search_results.distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              search_results.base.asset_id ,\n",
    "              search_results.base.ml_generate_embedding_start_sec as startOffset_seconds,\n",
    "              search_results.base.ml_generate_embedding_end_sec as endOffset_seconds,  \n",
    "              search_results.base.content_type as asset_type,\n",
    "              ROW_NUMBER() OVER (PARTITION BY search_results.base.asset_id ORDER BY distance) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                TABLE `{dataset}.{table}`, --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  }, -- Top k most similar matches based on distance\n",
    "                distance_type => 'COSINE',\n",
    "                options => {options}                \n",
    "              ) search_results\n",
    "              \n",
    "          )\n",
    "          -- Step 2: Aggregate relevance per document  \n",
    "            ,aggregated_results AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    COUNT(*) AS chunk_count,  -- The number of chunks for this document\n",
    "                    SUM(distance) AS total_distance,  -- Sum of the distances for this document's chunks\n",
    "                    AVG(distance) AS avg_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            ),\n",
    "\n",
    "            -- Step 3: Rank the documents by relevance (number of chunks and sum of distances)\n",
    "            ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    chunk_count,\n",
    "                    total_distance,\n",
    "                    avg_distance,\n",
    "                    ROW_NUMBER() OVER (ORDER BY chunk_count DESC, total_distance ASC) AS final_rank  -- Rank by chunk_count and then distance\n",
    "\n",
    "                FROM aggregated_results\n",
    "            )\n",
    "\n",
    "            -- Step 4: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id,  \n",
    "                sr.fileUri,  \n",
    "                sr.asset_type,\n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id) AS IDX,\n",
    "                STRING_AGG(CONCAT(\"\"\"+\"'{startOffset_seconds:', sr.startOffset_seconds, ',endOffset_seconds:', sr.endOffset_seconds, '}')\"\"\"+f\"\"\", \", \" ) \n",
    "                OVER (PARTITION BY sr.asset_id ORDER BY sr.startOffset_seconds) AS time_lines\n",
    "               -- sr.distance,\n",
    "               -- final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            WHERE rd.final_rank <= {top_k} -- Return the top-k documents based on chunk relevance\n",
    "            ORDER BY rd.final_rank, sr.rank_within_document  -- Order by document relevance and chunk rank\n",
    "            )\n",
    "            WHERE IDX=1\n",
    "    \"\"\"       \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "    \n",
    "    output=[]\n",
    "    for row in results:\n",
    "        output.append({'asset_id':row['asset_id'], 'fileUri':row['fileUri'], \"time_lines\":row['time_lines'], \"asset_type\":row[\"asset_type\"]})\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return output\n",
    "\n",
    "async def get_content_nearest_neighbors(query_embedding, table, dataset,source_embedding_column,project_id,top_k=50):\n",
    "    \"\"\"Query nearest neighbors using cosine similarity in BigQuery for text embeddings.\"\"\"\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    options=\"\"\"'{\"fraction_lists_to_search\": 0.5}'\"\"\"\n",
    "    #options=\"\"\"'{\"use_brute_force\":true}' \"\"\"\n",
    "\n",
    "    sql = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.content as content,  \n",
    "              search_results.base.combined_id as combined_id,\n",
    "              search_results.base.unique_id,\n",
    "              distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              search_results.base.asset_id,\n",
    "              search_results.base.headline,\n",
    "              ifnull(search_results.base.html_safe_text,search_results.base.description) as description,\n",
    "              search_results.base.startOffset_seconds,\n",
    "              search_results.base.endOffset_seconds,\n",
    "              search_results.base.fileUri,\n",
    "              search_results.base.asset_type,\n",
    "              ROW_NUMBER() OVER (PARTITION BY  search_results.base.asset_id ORDER BY distance) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                TABLE `{dataset}.{table}`, --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  }, -- Top k most similar matches based on distance\n",
    "                distance_type => 'COSINE',\n",
    "                options => {options}                   \n",
    "              ) search_results              \n",
    "          )\n",
    "          -- Step 2: Aggregate relevance per document (original_document_id)\n",
    "            ,aggregated_results AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    COUNT(*) AS chunk_count,  -- The number of chunks for this document\n",
    "                    SUM(distance) AS total_distance,  -- Sum of the distances for this document's chunks\n",
    "                    AVG(distance) AS avg_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            ),\n",
    "\n",
    "            -- Step 3: Rank the documents by relevance (number of chunks and sum of distances)\n",
    "            ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,\n",
    "                    chunk_count,\n",
    "                    total_distance,\n",
    "                    avg_distance,\n",
    "                    ROW_NUMBER() OVER (ORDER BY chunk_count DESC, total_distance ASC) AS final_rank  -- Rank by chunk_count and then distance\n",
    "\n",
    "                FROM aggregated_results\n",
    "            )\n",
    "\n",
    "            -- Step 4: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id,  \n",
    "                sr.headline,\n",
    "                sr.description,\n",
    "                sr.combined_id,\n",
    "                sr.unique_id,\n",
    "                sr.fileUri,\n",
    "                sr.asset_type,\n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id) AS IDX,\n",
    "                STRING_AGG(CONCAT(\"\"\"+\"'{startOffset_seconds:', sr.startOffset_seconds, ',endOffset_seconds:', sr.endOffset_seconds, '}')\"\"\"+f\"\"\", \", \" ) \n",
    "                OVER (PARTITION BY sr.asset_id ORDER BY sr.startOffset_seconds) AS time_lines\n",
    "                --sr.distance,\n",
    "                --final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            WHERE rd.final_rank <= {top_k} -- Return the top-k documents based on chunk relevance      \n",
    "            --and sr.asset_id like '%00261507986b0faf31c775597d2d24beb4381e43%'\n",
    "            ORDER BY rd.final_rank, sr.rank_within_document  -- Order by document relevance and chunk rank\n",
    "            )\n",
    "            WHERE IDX=1\n",
    "    \"\"\"       \n",
    "    #print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "    \n",
    "    output=[]\n",
    "    for row in results:\n",
    "        output.append({'asset_id':row['asset_id'], 'headline':row['headline'],'description':row['description'],'fileUri':row['fileUri'], \"time_lines\":row['time_lines'], \"asset_type\":row[\"asset_type\"]})\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return output\n",
    "\n",
    "def merge_result(combined_list):\n",
    "    # Step 2: Create a dictionary to merge by 'id'\n",
    "    merged_dict = {}\n",
    "\n",
    "    # Step 3: Iterate through the combined list and merge dictionaries by 'id'\n",
    "    for d in combined_list:\n",
    "        id_value = d['asset_id']\n",
    "\n",
    "        # If the id already exists in merged_dict, update it\n",
    "        if id_value in merged_dict:\n",
    "            merged_dict[id_value].update(d)\n",
    "        else:\n",
    "            # If the id doesn't exist, add the dictionary as it is\n",
    "            merged_dict[id_value] = d.copy()\n",
    "\n",
    "    # Step 4: Convert the merged dictionary back into a list\n",
    "    final_merged_list = list(merged_dict.values())\n",
    "    \n",
    "    return final_merged_list\n",
    "\n",
    "\n",
    "\n",
    "async def get_nearest_contet(request):\n",
    "    \"\"\"\n",
    "    Cloud Function entry point. This function handles the incoming request, \n",
    "    performs exponential backoff retries, and returns the embedding response.\n",
    "    \"\"\" \n",
    "    # Parse the incoming request to extract text or image file\n",
    "    request_json = request.get_json(silent=True)\n",
    "    text = request_json.get('search_query')\n",
    "    image_file = request_json.get('image_file')  # Assume it's the path or base64 string of the image\n",
    "    project_id = request_json.get('project')  \n",
    "    region = request_json.get('region')  \n",
    "    \n",
    "    # Load configuration from config.json\n",
    "    with open('config.json') as config_file:\n",
    "         config = json.load(config_file)\n",
    "    \n",
    "    \n",
    "    top_k=int(config['top_k'])  \n",
    "    dataset= config['dataset']\n",
    "    content_table=config['content_table']\n",
    "    mm_table=config['mm_table']\n",
    "    content_source_embedding_column=config['content_source_embedding_column']\n",
    "    mm_source_embedding_column=config['mm_source_embedding_column'] \n",
    "    if image_file==\"\" or image_file==\"None\":\n",
    "        image_file=None\n",
    "#     project_id='nine-quality-test'\n",
    "#     region=\"us-central1\"\n",
    "#     text='curtis sittenfeld'\n",
    "#     image_file=None\n",
    "    \n",
    "#     top_k=50     \n",
    "#     dataset='vlt_media_embeddings_integration'\n",
    "#     content_table='vlt_all_media_content_text_embeddings'\n",
    "#     mm_table='vlt_imgvdo_multimodal_embeddings'\n",
    "#     content_source_embedding_column='text_embedding_result'\n",
    "#     mm_source_embedding_column='ml_generate_embedding_result'\n",
    "\n",
    "    # Initialize the EmbeddingPredictionClient outside the function for reuse\n",
    "    embedding_client = EmbeddingPredictionClient(project=project_id , location=region,api_regional_endpoint=region+\"-aiplatform.googleapis.com\")\n",
    "        \n",
    "    if not text and not image_file:\n",
    "        print('you are here')\n",
    "        return 'Error: At least one of \"text\" or \"image_file\" must be provided.', 400\n",
    "     \n",
    "    content_result=[]\n",
    "    media_text_result=[]\n",
    "    media_image_result=[]\n",
    "    if text:\n",
    "        #if a text is given, calculate both multiomdal embedding and text embedding of the search query\n",
    "        txtembding_for_text_result =  await asyncio.create_task(exponential_backoff_retries(embedding_client, text, embedding_type='text_embedding'))\n",
    "        mmembding_for_text_result =  await asyncio.create_task(exponential_backoff_retries(embedding_client, text, embedding_type='multimodal_embedding')) \n",
    "        txtembding_for_text_result=txtembding_for_text_result .text_embedding\n",
    "        mmembding_for_text_result=mmembding_for_text_result.text_embedding\n",
    "        #find nearest neighbours both from text embedding and multimodal embedding\n",
    "        content_result = await asyncio.create_task(get_content_nearest_neighbors(txtembding_for_text_result, content_table, dataset,content_source_embedding_column,project_id,top_k=top_k))\n",
    "        media_text_result = await asyncio.create_task(get_media_nearest_neighbors(mmembding_for_text_result, mm_table, dataset,mm_source_embedding_column,project_id,top_k=top_k))\n",
    "               \n",
    "    if image_file:\n",
    "        #if an image is given convert image to 64bytestring and extract embedding\n",
    "        mmembding_for_image_result = await asyncio.create_task(exponential_backoff_retries(embedding_client, image_file, embedding_type='multimodal_embedding'))\n",
    "        mmembding_for_image_result=mmembding_for_text_result.image_embedding\n",
    "        #find nearest neighbours both from multimodal embedding\n",
    "        media_image_result = await asyncio.create_task(get_media_nearest_neighbors(mmembding_for_image_result, mm_table, dataset,mm_source_embedding_column,project_id,top_k=top_k))\n",
    "        media_image_result=media_image_result\n",
    "        \n",
    "    final_merged_list=merge_result(content_result+media_text_result+media_image_result)\n",
    "    return final_merged_list#, content_result, media_text_result, media_image_result\n",
    "\n",
    "\n",
    "# @functions_framework.http\n",
    "# async def search_content_function(request):\n",
    " \n",
    "#     result = await get_nearest_contet(request) \n",
    "#     return result#[0],result[1],result[2]\n",
    "\n",
    "@functions_framework.http\n",
    "def search_content_function(request):\n",
    "    \"\"\"This is the entry point for the Cloud Function.\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError as e:\n",
    "        # If no event loop is running, create a new event loop for this thread\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    result = loop.run_until_complete(get_nearest_contet(request))\n",
    "    return result\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9276915-7d88-45ec-b268-edfe571db894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unittest.mock import Mock\n",
    "import json\n",
    "\n",
    "# Your input data as a dictionary\n",
    "data = {\"search_query\":\"curtis sittenfeld\",\"image_file\":\"\",\"project\":\"nine-quality-test\",\"region\":\"us-central1\"}\n",
    "\n",
    "# Simulating an HTTP request with the mock object\n",
    "mock_request = Mock()\n",
    "mock_request.get_json.return_value = data  # Mock the get_json method to return your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c71cc35d-da60-4a8d-97d2-c5eb437b5a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731090784072876\n",
      "0.45113086700439453\n"
     ]
    }
   ],
   "source": [
    "x= await search_content_function(mock_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5eb2a4-8df7-42b9-9de5-eb9b49df6d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
