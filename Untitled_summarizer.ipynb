{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "026458de-69fd-44c8-8da1-8086a0214a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "#from langchain.llms import VertexAI as langchain_vertexai\n",
    "from langchain_google_vertexai import VertexAI as langchain_vertexai\n",
    "from langchain import PromptTemplate\n",
    "from pathlib import Path as p\n",
    "import pandas as pd\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "\n",
    "\n",
    "vertex_llm_text = langchain_vertexai(model_name=\"gemini-1.5-pro-002\")\n",
    "generative_multimodal_model= GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "\n",
    "def estimate_token_length(text, model=\"gpt2\"):\n",
    "    \"\"\"Estimates the token length of a given text using a specified model.\n",
    "\n",
    "      Args:\n",
    "        text: The input text.\n",
    "        model: The model to use for tokenization (default: \"gpt2\").\n",
    "\n",
    "      Returns:\n",
    "        The estimated number of tokens.\n",
    "      \"\"\"\n",
    "\n",
    "  \n",
    "    enc = tiktoken.get_encoding(model)  \n",
    "\n",
    "    # Tokenize the text and count tokens\n",
    "    tokens = enc.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    return token_count\n",
    "\n",
    "def get_data(source_query_str: str=None,metadata_columns: str=None,page_content_columns: str=None, project_id: str=None , return_text: bool=True):\n",
    "    \n",
    "    \"\"\"Load data from big query\n",
    "\n",
    "      Args:\n",
    "        str source_query_str:  The query string to fetch the data from bigquery\n",
    "        list[str] metadata_columns:  list of metadata column names\n",
    "        list[str] page_content_columns:  list of content column names  \n",
    "        str project_id: project id\n",
    "        bool return_text: returns the content columns description\n",
    "      Returns:\n",
    "          list[langchain_core.documents.base.Document] documents: langchain documents\n",
    "          \n",
    "      \"\"\"\n",
    "    \n",
    "    loader = BigQueryLoader(\n",
    "            query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "        )\n",
    "    documents = []\n",
    "    all_texts=[]\n",
    "    documents.extend(loader.load())\n",
    "    if return_text:  \n",
    "         all_texts=[doc.page_content.replace('description:',\"\",1) for doc in documents]\n",
    "        \n",
    "    return documents, '\\n'.join(all_texts)\n",
    "    \n",
    " \n",
    "def summarize_docs(documents: list[object],question_prompt_template: str=\"\", refine_prompt_template: str=\"\" ,is_token_limit_exceeded: bool=False ):\n",
    "    \n",
    "    \"\"\"summarizes the input documents\n",
    "\n",
    "      Args:\n",
    "        list[object] documents:  list of langchain documents\n",
    "        str question_prompt_template:  string question prompt template. \n",
    "        str refine_prompt_template:  string refine prompt template in the case that we need to use refine method\n",
    "        bool is_token_limit_exceeded:  boolean indicating wheather or not the token limit is exceeded.\n",
    "      Returns:\n",
    "         dict : summary result\n",
    "         \n",
    "      \"\"\"\n",
    "       \n",
    "    question_prompt = PromptTemplate(template=question_prompt_template, input_variables=[\"text\"]) \n",
    "    \n",
    "    if not is_token_limit_exceeded:        \n",
    "        #if the token limit is in the context window range, use a stuffing method for summary\n",
    "        chain = load_summarize_chain(vertex_llm_text, chain_type=\"stuff\", \n",
    "                                     prompt=question_prompt)\n",
    "        \n",
    "    else:     \n",
    "        #otherwise use a refine summarization method\n",
    "        refine_prompt = PromptTemplate(input_variables=[\"existing_answer\", \"text\"], template=refine_prompt_template)\n",
    "              \n",
    "        chain = load_summarize_chain(\n",
    "            vertex_llm_text,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=question_prompt,\n",
    "            refine_prompt=refine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "          )\n",
    "        \n",
    "    return chain.invoke(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c848aa6f-839e-440b-9927-0e67ae92e070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def  get_query_string (assets: str=\"\"):\n",
    "    \"\"\"set query string \n",
    "      Args:         \n",
    "        str assets:  comma separated string of all requested assets     \n",
    "      Returns:\n",
    "         str source_query_str : string query for loading data from biquery\n",
    "         \n",
    "      \"\"\"\n",
    "     #source_query_str=f\"select distinct combined_id,unique_id,content, chunk, trim(concat(ifnull(headline,''), CHR(10),  description)) as description from `nine-quality-test.vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in {assets} order by unique_id, chunk asc \"\n",
    "    source_query_str= f\"\"\"SELECT          asset_id,                  \n",
    "                    STRING_AGG(description, '\\\\n' ) \n",
    "                    OVER (PARTITION BY asset_id ORDER BY ifnull(startOffset_seconds,0) ASC , chunk ASC) AS full_description,\n",
    "                    IDX\n",
    "              FROM (\n",
    "                    SELECT  asset_id,startOffset_seconds, CHUNK, \n",
    "                    CASE WHEN chunk=0 \n",
    "                         THEN TRIM(CONCAT(IFNULL(headline,''), CHR(10),  description))  \n",
    "                         ELSE description \n",
    "                    END AS description,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY asset_id ORDER BY startOffset_seconds desc) AS IDX,\n",
    "                    FROM `vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in ({assets})\n",
    "             )\n",
    "           WHERE IDX=1\n",
    "        \"\"\"\n",
    "    return source_query_str\n",
    "    \n",
    "def get_prompt(action_type: str=\"\",platform: str=\"\",persona_text: str=\"\", input_text:str=\"\", Language:str=\"\"):\n",
    "    \n",
    "    \"\"\"set prompt according to the requested action\n",
    "      Args:         \n",
    "        str action_type:  the type of action needs to be done\n",
    "        str platform: platform name for off platform posts \n",
    "        str persona_text: for persona based summaries \n",
    "      Returns:\n",
    "         str question_prompt_template : the main prompt for the given action \n",
    "         str refine_prompt_template:  the second level prompt for refinement, in the case that the context is too long, we have to use refinement method.\n",
    "         \n",
    "      \"\"\"\n",
    "   \n",
    "    question_prompt_template=\"\"\n",
    "    refine_prompt_template=\"\"\n",
    "    \n",
    "    if action_type==\"Summary\" or action_type==\"Summary_Persona\":\n",
    "            #this is the main prompt for summary\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a summary of the following text\"\"\"+persona_text+\"\"\". Your result must be detailed and at least 2 paragraphs. \n",
    "                When summarizing, directly dive into the narrative or descriptions from the text without using introductory phrases like 'In this passage'. \n",
    "                Directly address the main events, characters, and themes, encapsulating the essence and significant details from the text in a flowing narrative. \n",
    "                The goal is to present a unified view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\n",
    "\n",
    "                TEXT: {text}\n",
    "                SUMMARY:\n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final summary. Your task is to combine and refine these summaries into a final, comprehensive summary that covers all key events, characters, themes, and details.\\n\"\n",
    "                \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing summary\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original summary\"\n",
    "                \"If the context isn't useful, return the original summary.\"\n",
    "            )\n",
    "    elif action_type==\"HeadLine\":  \n",
    "\n",
    "        #this is the main prompt for headline\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a one line headline of the following text. \n",
    "\n",
    "                TEXT: {text}\n",
    "                HEADLINE:\n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final headline. Your task is to combine and refine these headlines into a final, comprehensive headline that covers all details.\\n\"\n",
    "                \"We have provided an existing headline up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing headline\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original headline\"\n",
    "                \"If the context isn't useful, return the original headline.\"\n",
    "            )\n",
    "    elif action_type==\"OffPlatformPost\"  and Platform=='Twitter':\n",
    "\n",
    "            #this is the main prompt for social media post\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a tweet that thatâ€™s catchy, concise, and fits within 280 characters. Make sure to highlight the key message, and encourage engagement with a question or call to action.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Tweet: \n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final tweet. Your task is to combine and refine these tweets into a final, comprehensive tweet that covers all details, is catchy, concise, fits within 280 characters, and encourage engagement with a question or call to action.\\n\"\n",
    "                \"We have provided an existing tweet up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing tweet\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original tweet\"\n",
    "                \"If the context isn't useful, return the original tweet.\"\n",
    "            )\n",
    "    elif action_type==\"OffPlatformPost\" and Platform=='Instagram':\n",
    "\n",
    "            #this is the main prompt for social media post\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide  into an engaging Instagram post. Craft a short, attention-grabbing caption that highlights the main point. Use emojis to make it lively, and end with a question or call to action to spark conversation in the comments.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Instagram Post: \n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final tweet. Your task is to combine and refine these Instagram posts into a final, comprehensive post that covers all details, crafts a short, attention-grabbing caption that highlights the main point. Use emojis to make it lively, and end with a question or call to action to spark conversation in the comments.\\n\"\n",
    "                \"We have provided an existing post up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing post\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original post\"\n",
    "                \"If the context isn't useful, return the original post.\"\n",
    "            )\n",
    "    elif action_type==\"Translation\":\n",
    "         #this is the main prompt for social media post\n",
    "            question_prompt_template = f\"\"\"\n",
    "                Translate the following text into {Language}.  Make sure to preserve the meaning, tone, and style of the original text, while ensuring it is natural and fluent in {Language}.\n",
    "\n",
    "            \"\"\"\n",
    "      \n",
    "            \n",
    "    return question_prompt_template,refine_prompt_template\n",
    "\n",
    "def get_summary(assets:str=\"\",action_type:str=\"\",platform:str=\"\",persona_text:str=\"\",project_id:str=\"\",context_window_limit: int=2000000):\n",
    "    \n",
    "    \"\"\"get summary according to the action type requested\n",
    "      Args:         \n",
    "        str assets:  comma separate string including all assets\n",
    "        str action_type: requested action\n",
    "        str persona_text: for persona based summaries \n",
    "        str platform: for off platform based posts\n",
    "        str project_id: project id\n",
    "        int context_window_limit: context window limit for the llm model\n",
    "      Returns:\n",
    "         str :output summary \n",
    "      \"\"\"\n",
    "    \n",
    "    #set query string\n",
    "    source_query_str= get_query_string(assets)\n",
    "    \n",
    "    #set prompts\n",
    "    question_prompt_template, refine_prompt_template=get_prompt(action_type=action_type,platform=platform,persona_text=persona_text)\n",
    "   \n",
    "    #set metadata and content columns\n",
    "    metadata_columns=[\"asset_id\"]\n",
    "    page_content_columns=[\"full_description\"]\n",
    "    \n",
    "    #load data from biqquery\n",
    "    documents,all_texts=get_data(source_query_str=source_query_str,metadata_columns=metadata_columns,page_content_columns=page_content_columns, project_id=project_id,return_text=True)\n",
    "\n",
    "    # Estimate the token length\n",
    "    estimated_token_length = estimate_token_length(all_texts,'cl100k_base') #cl100k_base\n",
    "    \n",
    "    message=\"\"\n",
    "    is_token_limit_exceeded=False\n",
    "    if estimated_token_length > context_window_limit:\n",
    "      message=\"Your text is too long for the Gemini 1.5 Pro context window. We are trying to chunk and return the result.\"\n",
    "      is_token_limit_exceeded=True\n",
    "      summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,refine_prompt_template=refine_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded )\n",
    "\n",
    "    else:\n",
    "      message=\"Your text fits within the Gemini 1.5 Pro context window.\"\n",
    "      summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded )\n",
    "        \n",
    "    return summary[\"output_text\"]\n",
    "\n",
    "def get_translation(input_text: str=\"\",Language:str=\"\"):\n",
    "    \n",
    "    \"\"\" get translation according to the requested language\n",
    "      Args:         \n",
    "        str input_text:  text to be translated\n",
    "        str Language: destination language\n",
    "      \n",
    "      Returns:\n",
    "         str : translated document \n",
    "      \"\"\"\n",
    " \n",
    "    \n",
    "    #set prompts\n",
    "    question_prompt_template, _=get_prompt(action_type=action_type,input_text=input_text,Language=Language)\n",
    "    generation_config= GenerationConfig(temperature=0.2, max_output_tokens=8192) \n",
    "    safety_settings=  {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "\n",
    "    model_input=[question_prompt_template, input_text]\n",
    "        \n",
    "    response = generative_multimodal_model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings, )\n",
    "     \n",
    "    print(response)\n",
    " \n",
    "    return response\n",
    " \n",
    "        \n",
    "def func_generate_content(request):\n",
    "    \n",
    "    # Set the Gemini 1.5 Pro context window limit\n",
    "    context_window_limit = 2000000\n",
    "    project_id = \"nine-quality-test\"  # @param {type:\"string\"}\n",
    "    REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "    assets=\"p5d2tw,p5e9zq,p5e49l\" #comma separated asset_ids  \n",
    "    \n",
    "    assets= ','.join([ \"'\"+ id.strip()+\"'\" for id in assets.split(',')])\n",
    "    action_type=\"Summary\" # could be Summary, Summary_Persona, HeadLine, OffPlatformPost, Translation\n",
    "    persona=\"10-year-old\"\n",
    "    text=\"text to translate to Chaineese\"\n",
    "    Language=\"Chineese\"\n",
    "    \n",
    "    persona_text=\"\"\n",
    "    if action_type==\"Summary_Persona\" and persona==\"\":\n",
    "        return \"Error- Please set the persona\"    \n",
    "    else:\n",
    "         persona_text=f\" so that a {persona} can understand it. Use simple words and short sentences\"\n",
    "    \n",
    "    platform=\"Twitter\" # could be Twitter, Instagram or \"\" if OffPlatformPost is not selected\n",
    "    if action_type==\"OffPlatformPost\" and platform==\"\":\n",
    "         return \"Error- Please set the platform\"\n",
    "        \n",
    "    if action_type==\"Translation\" and (text==\"\" or Language==\"\"):\n",
    "        return \"Error- Please set the input text to translate and destination language\"\n",
    "        \n",
    "  \n",
    "    #get summary\n",
    "    if action_type!=\"Translation\":\n",
    "        \n",
    "        result=get_summary(assets =assets,action_type=action_type,platform=platform,\n",
    "                           persona_text=persona_text,project_id=project_id,context_window_limit=context_window_limit)\n",
    "    else:\n",
    "        result=get_translation(input_text=text,Language=Language )\n",
    "        \n",
    "      \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34736324-932f-4aec-953a-3f4d1e16e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=func_generate_content('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d4765-fe0a-496d-8301-f56c003ee8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1-->s1\n",
    "doc2-->s2\n",
    "doc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "46c35f1e-a804-4de8-b99e-f0f6bb539d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"TEXT: {text} is missing.  I need the text to summarize it.  Please provide the text you would like me to summarize.\\n\\nSUMMARY:\\nSince there is no text provided, I cannot offer a summary.  I am ready to create a detailed, flowing summary once you give me the text. Please provide the text so I can fulfill your request.\\n\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.06754685938358307\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.13568978011608124\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.04603387415409088\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.09534948319196701\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.20817901194095612\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.12940247356891632\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.14706426858901978\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.06560491770505905\n",
      "  }\n",
      "  avg_logprobs: -0.1854966314215409\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 132\n",
      "  candidates_token_count: 76\n",
      "  total_token_count: 208\n",
      "}\n",
      "model_version: \"gemini-1.5-pro-002\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=get_translation(input_text=\"how are you\",Language='Persian' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ad8d0fa-8ddf-41f9-9256-4f7ea37fef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disney+ offers a diverse range of adult programming, from comedies like *Abbott Elementary*, *Arrested Development*, and *Bob's Burgers* to adventures like *How I Met Your Mother* and *The Muppet Show*.  For more intense viewing, the platform boasts thrillers such as *The Americans*, *Andor*, and *Buffy the Vampire Slayer*,  mysteries like *Lost* and *Only Murders in the Building*, and suspenseful dramas like *Homeland*.  Disney+ also delves into real-world issues with shows like *Mrs. America*, *The People v. O.J. Simpson*, and *Pose*. While the platform offers lighter fare like the novel-based show \"Romantic Comedy,\" exploring relationships and the entertainment industry,  it's worth noting that the realities of that industry, particularly regarding toxic work environments and abusive power dynamics, are explored in Maureen Ryan's book \"Burn It Down,\" which exposes systemic issues behind the scenes of shows like *Lost* and even *Saturday Night Live*, highlighting a stark contrast to the romanticized version often portrayed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd70c31-5ea8-4531-aa37-7c0744a1162b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
