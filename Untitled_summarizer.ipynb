{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "026458de-69fd-44c8-8da1-8086a0214a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    " \n",
    "\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "#from langchain.llms import VertexAI as langchain_vertexai\n",
    "from langchain_google_vertexai import VertexAI as langchain_vertexai\n",
    "from langchain import PromptTemplate\n",
    "from pathlib import Path as p\n",
    "import pandas as pd\n",
    "\n",
    "def estimate_token_length(text, model=\"gpt2\"):\n",
    "    \"\"\"Estimates the token length of a given text using a specified model.\n",
    "\n",
    "      Args:\n",
    "        text: The input text.\n",
    "        model: The model to use for tokenization (default: \"gpt2\").\n",
    "\n",
    "      Returns:\n",
    "        The estimated number of tokens.\n",
    "      \"\"\"\n",
    "\n",
    "  \n",
    "    enc = tiktoken.get_encoding(model)  \n",
    "\n",
    "    # Tokenize the text and count tokens\n",
    "    tokens = enc.encode(text)\n",
    "    token_count = len(tokens)\n",
    "\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    return token_count\n",
    "\n",
    "def get_data(source_query_str: str=None,metadata_columns: str=None,page_content_columns: str=None, project_id: str=None, return_only_text:bool=False):\n",
    "    \n",
    "    loader = BigQueryLoader(\n",
    "            query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "        )\n",
    "    documents = []\n",
    "    all_texts=[]\n",
    "    documents.extend(loader.load())\n",
    "    if return_only_text:  \n",
    "        all_texts=[doc.page_content.replace('description:',\"\",1) for doc in documents]\n",
    " \n",
    "        \n",
    "    return documents, '\\n'.join(all_texts)\n",
    "\n",
    "\n",
    "def summarize_docs(documents,prompt ,is_token_limit_exceeded ):\n",
    "    \n",
    "    if not is_token_limit_exceeded:\n",
    "        stuffing_\n",
    "    else:\n",
    "        chunkand summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c848aa6f-839e-440b-9927-0e67ae92e070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Gemini 1.5 Pro context window limit\n",
    "context_window_limit = 2000000\n",
    "PROJECT_ID = \"nine-quality-test\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "source_query_str=\"select distinct combined_id,unique_id,content, chunk, trim(concat(ifnull(headline,''), CHR(10),  description)) as description from `nine-quality-test.vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` order by unique_id, chunk asc \"\n",
    "\n",
    "\n",
    "metadata_columns=[\"combined_id\"]\n",
    "page_content_columns=[\"description\"]\n",
    "documents, all_texts=get_data(source_query_str=source_query_str,metadata_columns=metadata_columns,page_content_columns=page_content_columns, project_id=PROJECT_ID, return_only_text=True)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93345630-2fc6-4fbb-866a-c5bf3feb61ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 18027\n",
      "18027\n",
      "Your text fits within the Gemini 1.5 Pro context window.\n"
     ]
    }
   ],
   "source": [
    "#documents,all_texts=get_data(source_query_str,metadata_columns,page_content_columns,PROJECT_ID,return_only_text=True)\n",
    "# Estimate the token length\n",
    "estimated_token_length = estimate_token_length(all_texts,'cl100k_base') #cl100k_base\n",
    "print(estimated_token_length)\n",
    "if estimated_token_length > context_window_limit:\n",
    "  print(\"Your text is too long for the Gemini 1.5 Pro context window. We are trying to chunk and return the result.\")\n",
    "  is_token_limit_exceeded=\n",
    "else:\n",
    "  print(\"Your text fits within the Gemini 1.5 Pro context window.\")\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481ec0d-4955-4548-b227-024ba604bf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
