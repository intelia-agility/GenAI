{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f2fea-6ba4-463d-8850-8518efa6b03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI as langchain_vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026458de-69fd-44c8-8da1-8086a0214a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "#from langchain.llms import VertexAI as langchain_vertexai\n",
    "from langchain_google_vertexai import VertexAI as langchain_vertexai\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pathlib import Path as p\n",
    "import pandas as pd\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "\n",
    "\n",
    "vertex_llm_text = langchain_vertexai(model_name=\"gemini-1.5-pro-002\")\n",
    "generative_multimodal_model= GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "\n",
    "def estimate_token_length(text, model=\"gpt2\"):\n",
    "    \"\"\"Estimates the token length of a given text using a specified model.\n",
    "\n",
    "      Args:\n",
    "        text: The input text.\n",
    "        model: The model to use for tokenization (default: \"gpt2\").\n",
    "\n",
    "      Returns:\n",
    "        The estimated number of tokens.\n",
    "      \"\"\"\n",
    "\n",
    "  \n",
    "    enc = tiktoken.get_encoding(model)  \n",
    "\n",
    "    # Tokenize the text and count tokens\n",
    "    tokens = enc.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    return token_count\n",
    "\n",
    "def get_data(source_query_str: str=None,metadata_columns: str=None,page_content_columns: str=None, project_id: str=None , return_text: bool=True):\n",
    "    \n",
    "    \"\"\"Load data from big query\n",
    "\n",
    "      Args:\n",
    "        str source_query_str:  The query string to fetch the data from bigquery\n",
    "        list[str] metadata_columns:  list of metadata column names\n",
    "        list[str] page_content_columns:  list of content column names  \n",
    "        str project_id: project id\n",
    "        bool return_text: returns the content columns description\n",
    "      Returns:\n",
    "          list[langchain_core.documents.base.Document] documents: langchain documents\n",
    "          \n",
    "      \"\"\"\n",
    "    \n",
    "    loader = BigQueryLoader(\n",
    "            query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "        )\n",
    "    documents = []\n",
    "    all_texts=[]\n",
    "    documents.extend(loader.load())\n",
    "    if return_text:  \n",
    "         all_texts=[doc.page_content.replace('description:',\"\",1) for doc in documents]\n",
    "        \n",
    "    return documents, '\\n'.join(all_texts)\n",
    "    \n",
    " \n",
    "def summarize_docs(documents: list[object],question_prompt_template: str=\"\", refine_prompt_template: str=\"\" ,is_token_limit_exceeded: bool=False ):\n",
    "    \n",
    "    \"\"\"summarizes the input documents\n",
    "\n",
    "      Args:\n",
    "        list[object] documents:  list of langchain documents\n",
    "        str question_prompt_template:  string question prompt template. \n",
    "        str refine_prompt_template:  string refine prompt template in the case that we need to use refine method\n",
    "        bool is_token_limit_exceeded:  boolean indicating wheather or not the token limit is exceeded.\n",
    "      Returns:\n",
    "         dict : summary result\n",
    "         \n",
    "      \"\"\"\n",
    "       \n",
    "    question_prompt = PromptTemplate(template=question_prompt_template, input_variables=[\"text\"]) \n",
    "    \n",
    "    if not is_token_limit_exceeded:        \n",
    "        #if the token limit is in the context window range, use a stuffing method for summary\n",
    "        chain = load_summarize_chain(vertex_llm_text, chain_type=\"stuff\", \n",
    "                                     prompt=question_prompt)\n",
    "        \n",
    "    else:     \n",
    "        #otherwise use a refine summarization method\n",
    "        refine_prompt = PromptTemplate(input_variables=[\"existing_answer\", \"text\"], template=refine_prompt_template)\n",
    "              \n",
    "        chain = load_summarize_chain(\n",
    "            vertex_llm_text,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=question_prompt,\n",
    "            refine_prompt=refine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "          )\n",
    "        \n",
    "    return chain.invoke(documents)\n",
    "\n",
    "def  get_query_string (assets: str=\"\"):\n",
    "    \"\"\"set query string \n",
    "      Args:         \n",
    "        str assets:  comma separated string of all requested assets     \n",
    "      Returns:\n",
    "         str source_query_str : string query for loading data from biquery\n",
    "         \n",
    "      \"\"\"\n",
    "     #source_query_str=f\"select distinct combined_id,unique_id,content, chunk, trim(concat(ifnull(headline,''), CHR(10),  description)) as description from `nine-quality-test.vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in {assets} order by unique_id, chunk asc \"\n",
    "    source_query_str= f\"\"\"SELECT          asset_id,                  \n",
    "                    STRING_AGG(description, '\\\\n' ) \n",
    "                    OVER (PARTITION BY asset_id ORDER BY ifnull(startOffset_seconds,0) ASC , chunk ASC) AS full_description,\n",
    "                    IDX\n",
    "              FROM (\n",
    "                    SELECT  asset_id,startOffset_seconds, CHUNK, \n",
    "                    \n",
    "                    CASE WHEN chunk=0 \n",
    "                         THEN TRIM(CONCAT(IFNULL(headline,''), CHR(10),  description))  \n",
    "                         ELSE description \n",
    "                    END AS description,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY asset_id ORDER BY startOffset_seconds desc, chunk desc) AS IDX,\n",
    "                    FROM `vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in ({assets})\n",
    "             )\n",
    "           WHERE IDX=1\n",
    "        \"\"\"\n",
    "    return source_query_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848aa6f-839e-440b-9927-0e67ae92e070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def  get_query_string (assets: str=\"\"):\n",
    "    \"\"\"set query string \n",
    "      Args:         \n",
    "        str assets:  comma separated string of all requested assets     \n",
    "      Returns:\n",
    "         str source_query_str : string query for loading data from biquery\n",
    "         \n",
    "      \"\"\"\n",
    "     #source_query_str=f\"select distinct combined_id,unique_id,content, chunk, trim(concat(ifnull(headline,''), CHR(10),  description)) as description from `nine-quality-test.vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in {assets} order by unique_id, chunk asc \"\n",
    "    source_query_str= f\"\"\"SELECT          asset_id,                  \n",
    "                    STRING_AGG(description, '\\\\n' ) \n",
    "                    OVER (PARTITION BY asset_id ORDER BY ifnull(startOffset_seconds,0) ASC , chunk ASC) AS full_description,\n",
    "                    IDX\n",
    "              FROM (\n",
    "                    SELECT  asset_id,startOffset_seconds, CHUNK, \n",
    "                    CASE WHEN chunk=0 \n",
    "                         THEN TRIM(CONCAT(IFNULL(headline,''), CHR(10),  description))  \n",
    "                         ELSE description \n",
    "                    END AS description,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY asset_id ORDER BY startOffset_seconds desc) AS IDX,\n",
    "                    FROM `vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in ({assets})\n",
    "             )\n",
    "           WHERE IDX=1\n",
    "        \"\"\"\n",
    "    return source_query_str\n",
    "    \n",
    "def get_prompt(action_type: str=\"\",platform: str=\"\",persona_text: str=\"\", input_text:str=\"\", Language:str=\"\"):\n",
    "    \n",
    "    \"\"\"set prompt according to the requested action\n",
    "      Args:         \n",
    "        str action_type:  the type of action needs to be done\n",
    "        str platform: platform name for off platform posts \n",
    "        str persona_text: for persona based summaries \n",
    "      Returns:\n",
    "         str question_prompt_template : the main prompt for the given action \n",
    "         str refine_prompt_template:  the second level prompt for refinement, in the case that the context is too long, we have to use refinement method.\n",
    "         \n",
    "      \"\"\"\n",
    " \n",
    "    question_prompt_template=\"\"\n",
    "    refine_prompt_template=\"\"\n",
    "    \n",
    "    if action_type==\"Summary\" or action_type==\"Summary_Persona\":\n",
    "            #this is the main prompt for summary\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a summary of the following text\"\"\"+persona_text+\"\"\". Your result must be detailed and at least 2 paragraphs. \n",
    "                When summarizing, directly dive into the narrative or descriptions from the text without using introductory phrases like 'In this passage'. \n",
    "                Directly address the main events, characters, and themes, encapsulating the essence and significant details from the text in a flowing narrative. \n",
    "                The goal is to present a unified view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\n",
    "\n",
    "                TEXT: {text}\n",
    "                SUMMARY:\n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final summary. Your task is to combine and refine these summaries into a final, comprehensive summary that covers all key events, characters, themes, and details.\\n\"\n",
    "                \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing summary\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original summary\"\n",
    "                \"If the context isn't useful, return the original summary.\"\n",
    "            )\n",
    "    elif action_type==\"HeadLine\":  \n",
    "\n",
    "        #this is the main prompt for headline\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a one line headline of the following text. \n",
    "\n",
    "                TEXT: {text}\n",
    "                HEADLINE:\n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final headline. Your task is to combine and refine these headlines into a final, comprehensive headline that covers all details.\\n\"\n",
    "                \"We have provided an existing headline up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing headline\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original headline\"\n",
    "                \"If the context isn't useful, return the original headline.\"\n",
    "            )\n",
    "    elif action_type==\"OffPlatformPost\"  and platform=='Twitter':\n",
    "               #this is the main prompt for social media post\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a tweet that that’s catchy, concise, and fits within 280 characters. Make sure to highlight the key message, and encourage engagement with a question or call to action.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Tweet: \n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final tweet. Your task is to combine and refine these tweets into a final, comprehensive tweet that covers all details, is catchy, concise, fits within 280 characters, and encourage engagement with a question or call to action.\\n\"\n",
    "                \"We have provided an existing tweet up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing tweet\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original tweet\"\n",
    "                \"If the context isn't useful, return the original tweet.\"\n",
    "            )\n",
    "    elif action_type==\"OffPlatformPost\" and platform=='Instagram':         \n",
    "            #this is the main prompt for social media post\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide  into an engaging Instagram post. Craft a short, attention-grabbing caption that highlights the main point. Use emojis to make it lively, and end with a question or call to action to spark conversation in the comments.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Instagram Post: \n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final tweet. Your task is to combine and refine these Instagram posts into a final, comprehensive post that covers all details, crafts a short, attention-grabbing caption that highlights the main point. Use emojis to make it lively, and end with a question or call to action to spark conversation in the comments.\\n\"\n",
    "                \"We have provided an existing post up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing post\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original post\"\n",
    "                \"If the context isn't useful, return the original post.\"\n",
    "            )\n",
    "    elif action_type==\"Translation\":\n",
    "            #this is the main prompt for social media post\n",
    "            question_prompt_template = f\"\"\"Translate the following text into {Language}.  Make sure to preserve the meaning, tone, and style of the original text, while ensuring it is natural and fluent in {Language}.\n",
    "            Text:\\n \n",
    "              {input_text}\\n\n",
    "            Only provide a single response.\n",
    "            \"\"\"\n",
    "      \n",
    "            \n",
    "    return question_prompt_template,refine_prompt_template\n",
    "\n",
    "def get_summary(assets:str=\"\",action_type:str=\"\",platform:str=\"\",persona_text:str=\"\",project_id:str=\"\",context_window_limit: int=2000000):\n",
    "    \n",
    "    \"\"\"get summary according to the action type requested\n",
    "      Args:         \n",
    "        str assets:  comma separate string including all assets\n",
    "        str action_type: requested action\n",
    "        str persona_text: for persona based summaries \n",
    "        str platform: for off platform based posts\n",
    "        str project_id: project id\n",
    "        int context_window_limit: context window limit for the llm model\n",
    "      Returns:\n",
    "         str :output summary \n",
    "      \"\"\"\n",
    "    \n",
    "    #set query string\n",
    "    source_query_str= get_query_string(assets)\n",
    "    \n",
    "    #set prompts\n",
    "    question_prompt_template, refine_prompt_template=get_prompt(action_type=action_type,platform=platform,persona_text=persona_text)\n",
    "    \n",
    "    #set metadata and content columns\n",
    "    metadata_columns=[\"asset_id\"]\n",
    "    page_content_columns=[\"full_description\"]\n",
    "    \n",
    "    #load data from biqquery\n",
    "    documents,all_texts=get_data(source_query_str=source_query_str,metadata_columns=metadata_columns,page_content_columns=page_content_columns, project_id=project_id,return_text=True)\n",
    "\n",
    "    # Estimate the token length\n",
    "    estimated_token_length = estimate_token_length(all_texts,'cl100k_base') #cl100k_base\n",
    "    \n",
    "    message=\"\"\n",
    "    is_token_limit_exceeded=False\n",
    "    if estimated_token_length > context_window_limit:\n",
    "      message=\"Your text is too long for the Gemini 1.5 Pro context window. We are trying to chunk and return the result.\"\n",
    "      is_token_limit_exceeded=True\n",
    "      summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,refine_prompt_template=refine_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded )\n",
    "\n",
    "    else:\n",
    "      message=\"Your text fits within the Gemini 1.5 Pro context window.\"\n",
    "      summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded )\n",
    "        \n",
    "    return summary[\"output_text\"]\n",
    "\n",
    "def get_translation(action_type: str=\"\",input_text: str=\"\",Language:str=\"\"):\n",
    "    \n",
    "    \"\"\" get translation according to the requested language\n",
    "      Args:         \n",
    "        str input_text:  text to be translated\n",
    "        str Language: destination language\n",
    "      \n",
    "      Returns:\n",
    "         str : translated document \n",
    "      \"\"\"\n",
    " \n",
    "    \n",
    "    #set prompts\n",
    "    question_prompt_template, _=get_prompt(action_type=action_type,input_text=input_text,Language=Language)\n",
    "     \n",
    "    generation_config= GenerationConfig(temperature=1, max_output_tokens=8192) \n",
    "    safety_settings=  {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "\n",
    "    model_input=[question_prompt_template]\n",
    "        \n",
    "    response = generative_multimodal_model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings, \n",
    "        \n",
    "    )\n",
    "    \n",
    "    result=\"\"\n",
    "    try:\n",
    "        result=response.text\n",
    "    except:\n",
    "        result=\"No translation can be provided.\"\n",
    "        \n",
    " \n",
    "    return result\n",
    " \n",
    "        \n",
    "def func_generate_content(request):\n",
    "    \n",
    "    # Set the Gemini 1.5 Pro context window limit\n",
    "    context_window_limit = 2000000\n",
    "    project_id = \"nine-quality-test\"  # @param {type:\"string\"}\n",
    "    REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "    assets=\"p5d2tw,p5e9zq,p5e49l\" #comma separated asset_ids  \n",
    "    \n",
    "    assets= ','.join([ \"'\"+ id.strip()+\"'\" for id in assets.split(',')])\n",
    "    action_type=\"Translation\" # could be Summary, Summary_Persona, HeadLine, OffPlatformPost, Translation\n",
    "    persona=\"10-year-old\"\n",
    "    text=\"\"\"\n",
    "    Disney+ has lots of cool shows!  There are funny shows like *Abbott Elementary*, which is about teachers at a school in Philadelphia.  There's also *The Americans*, a show about Russian spies pretending to be a regular family.  *Andor* is like *Star Wars* but for grown-ups.  *Arrested Development* is a hilarious show about a crazy family. *Atlanta* is about two cousins, one a rapper and the other his manager, and what it's like to be Black in America. *The Bear* is about a fancy chef who takes over his family's sandwich shop.  *Bob's Burgers* is a cartoon about a family who runs a burger restaurant. And for something really fun, there's *Buffy the Vampire Slayer*, about a girl who fights vampires! *Desperate Housewives* is about a group of women and all the secrets they keep. *Homeland* is about a CIA agent with mental health challenges.\n",
    "\n",
    "There are also shows with lots of episodes like *How I Met Your Mother*, which is about a group of friends. *Loki* from the Marvel movies has his own show. *Lost* is about people trapped on a strange island.  *Mrs. America* tells the true story of the fight for women's rights. *The Muppet Show* is a classic with puppets! *NYPD Blue* is about police officers in New York City.  *One Mississippi* is a sad but funny show about a woman who goes home to take care of her sick mom.  *Only Murders in the Building* is about three friends who investigate a murder. *The People v. O.J. Simpson* tells the story of a famous trial. *Pose* shows the lives of drag queens in the 1980s.  There are so many shows to watch on Disney+!\n",
    "\n",
    "\n",
    "Curtis Sittenfeld's new book, *Romantic Comedy*, is about Sally, a writer for a comedy show like *Saturday Night Live*.  Sally makes fun of how average-looking guys often date beautiful women. She writes a joke skit about it called \"The Danny Horst Rule\". Then Sally meets a handsome pop star named Noah, and she starts to like him.  But Sally doesn't think Noah could ever like her back.  The book is about whether they'll end up together. Sally is a feminist who wants to write romantic comedies that are smart and funny. She and Noah bond over their work, but Sally hides her true feelings.  She's afraid to be vulnerable because a coworker once hurt her feelings. The book also shows what it's like to work at a comedy show.\n",
    "    \"\"\"\n",
    "    Language=\"Persian\"\n",
    "    #text='How are you?'\n",
    "    \n",
    "    persona_text=\"\"\n",
    "    if action_type==\"Summary_Persona\" and persona==\"\":\n",
    "        return \"Error- Please set the persona\"    \n",
    "    else:\n",
    "         persona_text=f\" so that a {persona} can understand it. Use simple words and short sentences\"\n",
    "    \n",
    "    platform=\"Twitter\" # could be Twitter, Instagram or \"\" if OffPlatformPost is not selected\n",
    "    if action_type==\"OffPlatformPost\" and platform==\"\":\n",
    "         return \"Error- Please set the platform\"\n",
    "        \n",
    "    if action_type==\"Translation\" and (text==\"\" or Language==\"\"):\n",
    "        return \"Error- Please set the input text to translate and destination language\"\n",
    "        \n",
    "  \n",
    "    #get summary\n",
    "    if action_type!=\"Translation\":\n",
    "        if action_type==\"OffPlatformPost\":\n",
    "            result=\"Instagram Post:\\n\"+get_summary(assets =assets,action_type=action_type,platform='Instagram',\n",
    "                           persona_text=persona_text,project_id=project_id,context_window_limit=context_window_limit)\n",
    "\n",
    "            \n",
    "            result=result+\"\\nTwitter Post:\\n\"+get_summary(assets =assets,action_type=action_type,platform='Twitter',\n",
    "                           persona_text=persona_text,project_id=project_id,context_window_limit=context_window_limit)\n",
    "           \n",
    "        else:\n",
    "            result=get_summary(assets =assets,action_type=action_type,platform=\"\",\n",
    "                           persona_text=persona_text,project_id=project_id,context_window_limit=context_window_limit)\n",
    "    else:\n",
    "        result=get_translation(action_type=action_type,input_text=text,Language=Language )\n",
    "        \n",
    "      \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34736324-932f-4aec-953a-3f4d1e16e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=func_generate_content('')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2f1c6-7d5d-490a-ad6c-b82226747c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
    " \n",
    " \n",
    "def generate():\n",
    "    vertexai.init(project=\"nine-quality-test\", location=\"us-central1\")\n",
    "    model = GenerativeModel(\n",
    "        \"gemini-1.5-pro-002\",\n",
    "    )\n",
    "    responses = model.generate_content(\n",
    "        [text1],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "    )\n",
    "    result=\"\"\n",
    "    for response in responses:\n",
    "        result=result+response.text\n",
    "    return result\n",
    " \n",
    "text1 = \"\"\"Translate the following text into Persian. Make sure to preserve the meaning, tone, and style of the original text, while ensuring it is natural and fluent in Persian.\n",
    "Text:how are you\n",
    " \n",
    "Only provide a single response\"\"\"\n",
    " \n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    " \n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "]\n",
    " \n",
    "r=generate()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c35f1e-a804-4de8-b99e-f0f6bb539d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=get_translation(action_type='Translation',input_text=\"how are you\",Language='Persian' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1cd70c31-5ea8-4531-aa37-7c0744a1162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import tiktoken\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import pandas as pd\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from langchain_google_vertexai import VertexAI as langchain_vertexai\n",
    "from vertexai.generative_models._generative_models import SafetySettingsType\n",
    "import vertexai\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "def estimate_token_length(text, model=\"gpt2\"):\n",
    "    \"\"\"Estimates the token length of a given text using a specified model.\n",
    "\n",
    "      Args:\n",
    "        text: The input text.\n",
    "        model: The model to use for tokenization (default: \"gpt2\").\n",
    "\n",
    "      Returns:\n",
    "        The estimated number of tokens.\n",
    "      \"\"\"\n",
    "\n",
    "  \n",
    "    enc = tiktoken.get_encoding(model)  \n",
    "\n",
    "    # Tokenize the text and count tokens\n",
    "    tokens = enc.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    return token_count\n",
    "\n",
    "def get_data(source_query_str: str=None,metadata_columns: str=None,page_content_columns: str=None, project_id: str=None , return_text: bool=True):\n",
    "    \n",
    "    \"\"\"Load data from big query\n",
    "\n",
    "      Args:\n",
    "        str source_query_str:  The query string to fetch the data from bigquery\n",
    "        list[str] metadata_columns:  list of metadata column names\n",
    "        list[str] page_content_columns:  list of content column names  \n",
    "        str project_id: project id\n",
    "        bool return_text: returns the content columns description\n",
    "      Returns:\n",
    "          list[langchain_core.documents.base.Document] documents: langchain documents\n",
    "          \n",
    "      \"\"\"\n",
    "    \n",
    "    loader = BigQueryLoader(\n",
    "            query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "        )\n",
    "    documents = []\n",
    "    all_texts=[]\n",
    "    documents.extend(loader.load())\n",
    "    if return_text:  \n",
    "         all_texts=[doc.page_content.replace('full_description:',\"\",1) for doc in documents]\n",
    " \n",
    "        \n",
    "    return documents, '\\n'.join(all_texts)\n",
    "    \n",
    " \n",
    "def summarize_docs(documents: list[object],question_prompt_template: str=\"\", refine_prompt_template: str=\"\" ,is_token_limit_exceeded: bool=False ,model: object=None):\n",
    "    \n",
    "    \"\"\"summarizes the input documents\n",
    "\n",
    "      Args:\n",
    "        list[object] documents:  list of langchain documents\n",
    "        str question_prompt_template:  string question prompt template. \n",
    "        str refine_prompt_template:  string refine prompt template in the case that we need to use refine method\n",
    "        bool is_token_limit_exceeded:  boolean indicating wheather or not the token limit is exceeded.\n",
    "      Returns:\n",
    "         dict : summary result\n",
    "         \n",
    "      \"\"\"\n",
    "       \n",
    "    \n",
    "    question_prompt = PromptTemplate(template=question_prompt_template, input_variables=[\"text\"]) \n",
    "\n",
    "    if not is_token_limit_exceeded:        \n",
    "        #if the token limit is in the context window range, use a stuffing method for summary\n",
    "        chain = load_summarize_chain(model, chain_type=\"stuff\", \n",
    "                                     prompt=question_prompt)\n",
    "        \n",
    "    else:     \n",
    "        #otherwise use a refine summarization method\n",
    "        refine_prompt = PromptTemplate(input_variables=[\"existing_answer\", \"text\"], template=refine_prompt_template)\n",
    "              \n",
    "        chain = load_summarize_chain(\n",
    "            model,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=question_prompt,\n",
    "            refine_prompt=refine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "          )\n",
    "        \n",
    "    return chain.invoke(documents)\n",
    "\n",
    "def  get_query_string (assets: str=\"\"):\n",
    "    \"\"\"set query string \n",
    "      Args:         \n",
    "        str assets:  comma separated string of all requested assets     \n",
    "      Returns:\n",
    "         str source_query_str : string query for loading data from biquery\n",
    "         \n",
    "      \"\"\"\n",
    "    # source_query_str= f\"\"\"SELECT          asset_id,                  \n",
    "    #                 STRING_AGG(description, '\\\\n' ) \n",
    "    #                 OVER (PARTITION BY asset_id ORDER BY ifnull(startOffset_seconds,0) ASC , chunk ASC) AS full_description,\n",
    "    #                 IDX\n",
    "    #           FROM (\n",
    "    #                 SELECT  asset_id,startOffset_seconds, CHUNK, \n",
    "    #                 CASE WHEN chunk=0 \n",
    "    #                      THEN TRIM(CONCAT(IFNULL(headline,''), CHR(10),  description))  \n",
    "    #                      ELSE description \n",
    "    #                 END AS description,\n",
    "    #                 ROW_NUMBER() OVER (PARTITION BY asset_id ORDER BY startOffset_seconds desc, chunk desc) AS IDX,\n",
    "    #                 FROM `vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in ({assets})\n",
    "    #          )\n",
    "    #        WHERE IDX=1\n",
    "    #     \"\"\"\n",
    "    source_query_str= f\"\"\"\n",
    "     SELECT * FROM (\n",
    "        SELECT          asset_id,                  \n",
    "                    STRING_AGG(description, '\\\\n' ) \n",
    "                     OVER (PARTITION BY asset_id ORDER BY ifnull(startOffset_seconds,0) ASC , IFNULL(endOffset_seconds,0) ASC ) AS full_description,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY asset_id ORDER BY startOffset_seconds desc,endOffset_seconds desc) AS IDX,\n",
    "              FROM (\n",
    "                    SELECT * FROM \n",
    "                    (\n",
    "                        SELECT  distinct asset_id,startOffset_seconds,endOffset_seconds,\n",
    "                        TRIM(CONCAT(IFNULL(headline,''), CHR(10),  description))  description,\n",
    "                        ROW_NUMBER() OVER (PARTITION BY asset_id,startOffset_seconds ORDER BY startOffset_seconds desc, chunk desc) AS IDX,\n",
    "                        FROM `vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` where asset_id in ({assets})\n",
    "                      )\n",
    "                      WHERE IDX=1\n",
    "             )\n",
    "          )\n",
    "           WHERE IDX=1\n",
    "        \"\"\"\n",
    "    #print(source_query_str)\n",
    "    return source_query_str\n",
    "\n",
    "def get_prompt(action_type: str=\"\",platform: str=\"\",persona_text: str=\"\", input_text:str=\"\", Language:str=\"\"):\n",
    "    \n",
    "    \"\"\"set prompt according to the requested action\n",
    "      Args:         \n",
    "        str action_type:  the type of action needs to be done\n",
    "        str platform: platform name for off platform posts \n",
    "        str persona_text: for persona based summaries \n",
    "      Returns:\n",
    "         str question_prompt_template : the main prompt for the given action \n",
    "         str refine_prompt_template:  the second level prompt for refinement, in the case that the context is too long, we have to use refinement method.\n",
    "         \n",
    "      \"\"\"\n",
    " \n",
    "    question_prompt_template=\"\"\n",
    "    refine_prompt_template=\"\"\n",
    "    \n",
    "    if action_type==\"Summary\" or action_type==\"Summary_Persona\":\n",
    "            #this is the main prompt for summary\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a summary of the following text\"\"\"+persona_text+\"\"\". Your result must be detailed and at least 2 paragraphs. \n",
    "                When summarizing, directly dive into the narrative or descriptions from the text without using introductory phrases like 'In this passage'. \n",
    "                Directly address the main events, characters, and themes, encapsulating the essence and significant details from the text in a flowing narrative. \n",
    "                The goal is to present a unified view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\n",
    "                If different parts of texts look unrelevant, give a symmary of each text in 1 paragraph separately.\n",
    "\n",
    "                TEXT: {text}\n",
    "                SUMMARY:\n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final summary. Your task is to combine and refine these summaries into a final, comprehensive summary that covers all key events, characters, themes, and details.\\n\"\n",
    "                \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing summary\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original summary\"\n",
    "                \"If the context isn't useful, return the original summary and add the summary of the new context in a separate paragraph.\"\n",
    "            )\n",
    "    elif action_type==\"HeadLine\":  \n",
    "\n",
    "        #this is the main prompt for headline\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a one line headline of the following text. \n",
    "\n",
    "                TEXT: {text}\n",
    "                HEADLINE:\n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final headline. Your task is to combine and refine these headlines into a final, comprehensive headline that covers all details.\\n\"\n",
    "                \"We have provided an existing headline up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing headline\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original headline\"\n",
    "                \"If the context isn't useful, return the original headline and add the headline of the new context in a separate line.\"\n",
    "            )\n",
    "    elif action_type==\"OffPlatformPost\"  and platform=='Twitter':\n",
    "               #this is the main prompt for social media post\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide a tweet that that’s catchy, concise, and fits within 280 characters. Make sure to highlight the key message, and encourage engagement with a question or call to action.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Tweet: \n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final tweet. Your task is to combine and refine these tweets into a final, comprehensive tweet that covers all details, is catchy, concise, fits within 280 characters, and encourage engagement with a question or call to action.\\n\"\n",
    "                \"We have provided an existing tweet up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing tweet\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original tweet\"\n",
    "                \"If the context isn't useful, return the original tweet.\"\n",
    "            )\n",
    "    elif action_type==\"OffPlatformPost\" and platform=='Instagram':         \n",
    "            #this is the main prompt for social media post\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given different parts of texts. Provide  into an engaging Instagram post. Craft a short, attention-grabbing caption that highlights the main point. Use emojis to make it lively, and end with a question or call to action to spark conversation in the comments.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Instagram Post: \n",
    "            \"\"\"\n",
    "\n",
    "            refine_prompt_template = (\n",
    "                \"Your job is to produce a final tweet. Your task is to combine and refine these Instagram posts into a final, comprehensive post that covers all details, crafts a short, attention-grabbing caption that highlights the main point. Use emojis to make it lively, and end with a question or call to action to spark conversation in the comments.\\n\"\n",
    "                \"We have provided an existing post up to a certain point: {existing_answer}\\n\"\n",
    "                \"We have the opportunity to refine the existing post\"\n",
    "                \"(only if needed) with some more context below.\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"{text}\\n\"\n",
    "                \"------------\\n\"\n",
    "                \"Given the new context, refine the original post\"\n",
    "                \"If the context isn't useful, return the original post.\"\n",
    "            )\n",
    "    elif action_type==\"Translation\":\n",
    "            #this is the main prompt for social media post\n",
    "            question_prompt_template = f\"\"\"Translate the following text into {Language}.  Make sure to preserve the meaning, tone, and style of the original text, while ensuring it is natural and fluent in {Language}.\n",
    "            Text:\\n \n",
    "              {input_text}\\n\n",
    "            Only provide a single response.\n",
    "            \"\"\"   \n",
    "    elif action_type==\"TrailerScript\":\n",
    "        \n",
    "            #this is the main prompt for summary\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given a summary of episode. Provide a trailer script that will run for between 2 and 3 minutes.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Trailer Script:\n",
    "            \"\"\"\n",
    " \n",
    "            refine_prompt_template = \"\"\n",
    "    \n",
    "    elif action_type==\"KeyClips\":\n",
    "        \n",
    "            #this is the main prompt for summary\n",
    "            question_prompt_template = \"\"\"\n",
    "                You will be given a summary of episode. Identify the key clips in this episode and provide a title and summary of each clip along with the time line.\n",
    "\n",
    "                TEXT: {text}\n",
    "                Key Clips:\n",
    "            \"\"\"\n",
    " \n",
    "            refine_prompt_template = \"\"\n",
    "\n",
    "    return question_prompt_template,refine_prompt_template\n",
    "\n",
    "def get_summary(assets:str=\"\",action_type:str=\"\",platform:str=\"\",persona_text:str=\"\",project_id:str=\"\",context_window_limit: int=2000000, model: object=None):\n",
    "    \n",
    "    \"\"\"get summary according to the action type requested\n",
    "      Args:         \n",
    "        str assets:  comma separate string including all assets\n",
    "        str action_type: requested action\n",
    "        str persona_text: for persona based summaries \n",
    "        str platform: for off platform based posts\n",
    "        str project_id: project id\n",
    "        int context_window_limit: context window limit for the llm model\n",
    "      Returns:\n",
    "         str :output summary \n",
    "      \"\"\"\n",
    "    \n",
    "    #set query string\n",
    "    source_query_str= get_query_string(assets)\n",
    "    \n",
    "    #set prompts\n",
    "    question_prompt_template, refine_prompt_template=get_prompt(action_type=action_type,platform=platform,persona_text=persona_text)\n",
    "    \n",
    "    #set metadata and content columns\n",
    "    metadata_columns=[\"asset_id\"]\n",
    "    page_content_columns=[\"full_description\"]\n",
    "    \n",
    "    #load data from biqquery\n",
    "    documents,all_texts=get_data(source_query_str=source_query_str,metadata_columns=metadata_columns,page_content_columns=page_content_columns, project_id=project_id,return_text=True)\n",
    "\n",
    "    #print(all_texts)\n",
    "    # Estimate the token length\n",
    "    estimated_token_length = estimate_token_length(all_texts,'cl100k_base') #cl100k_base\n",
    "    \n",
    "    message=\"\"\n",
    "    is_token_limit_exceeded=False\n",
    "    if estimated_token_length > context_window_limit:\n",
    "      message=\"Your text is too long for the Gemini 1.5 Pro context window. We are trying to chunk and return the result.\"\n",
    "      is_token_limit_exceeded=True\n",
    "      summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,refine_prompt_template=refine_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded ,model=model)\n",
    "\n",
    "    else:\n",
    "      message=\"Your text fits within the Gemini 1.5 Pro context window.\"\n",
    "      summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded,model=model )\n",
    " \n",
    "  \n",
    "    return summary[\"output_text\"]\n",
    "\n",
    "def get_translation(action_type: str=\"\",input_text: str=\"\",Language:str=\"\", model: object=None):\n",
    "    \n",
    "    \"\"\" get translation according to the requested language\n",
    "      Args:         \n",
    "        str input_text:  text to be translated\n",
    "        str Language: destination language\n",
    "      \n",
    "      Returns:\n",
    "         str : translated document \n",
    "      \"\"\"\n",
    " \n",
    "    \n",
    "    #set prompts\n",
    "    question_prompt_template, _=get_prompt(action_type=action_type,input_text=input_text,Language=Language)\n",
    "    generation_config= GenerationConfig(temperature=0.2, max_output_tokens=8192) \n",
    "    safety_settings=  {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "\n",
    "    model_input=[question_prompt_template]\n",
    "        \n",
    "    response = model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings, )\n",
    "    \n",
    "    result=\"\"\n",
    "    try:\n",
    "        result=response.text\n",
    "    except Exception as e:\n",
    "        result=str(e)\n",
    "\n",
    "    return result\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "def log_data(result,error,request,elapsed_time,project_id):\n",
    "    \"\"\"\n",
    "      Log the search result into bigquery\n",
    "    Args:\n",
    "       List[dict]  result: the result of search\n",
    "       str error: the error message\n",
    "       dict request: the request sent\n",
    "       float elapsed_time: the time taken for the search result to be generated\n",
    "       str project_id: project id\n",
    "    \"\"\"\n",
    "    rows_to_insert=[] \n",
    "    rows_to_insert.append(\n",
    "                                {  \"search_date\":  datetime.now().isoformat() ,\n",
    "                                    \"request\":request,\n",
    "                                    \"response\":   result  , \n",
    "                                    \"error\":  error,\n",
    "                                    \"elapsed_time\":elapsed_time ,\n",
    "                                    \"API\": \"content_generation\"\n",
    "                                  \n",
    "                                    }\n",
    "                                            )   \n",
    "\n",
    "    #create table new if does not exist\n",
    "    # Load configuration from config.json\n",
    "    with open('config.json') as config_file:\n",
    "         config = json.load(config_file)\n",
    "\n",
    "    table=config['log_table']\n",
    "    dataset_id=config['log_dataset']\n",
    "    #push the data into the table\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table}\"\n",
    "    client = bigquery.Client(project_id)\n",
    "    dataset  = client.dataset(dataset_id)\n",
    "    table = dataset.table(table)\n",
    "\n",
    "    #send a big query streaming insert job- dont need to wait for the job to finish\n",
    "    job = client.load_table_from_json(rows_to_insert, table) \n",
    "\n",
    "def func_generate_content(request):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cloud Function entry point. This function handles the incoming request, \n",
    "    performs content generation according to the given action\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the incoming request to extract text or image file\n",
    "    request_json = request.get_json(silent=True)   \n",
    "    project_id = request_json.get('project')  \n",
    "    location = request_json.get('region')  \n",
    "    action_type = request_json.get('action_type') # could be Summary, Summary_Persona, HeadLine, OffPlatformPost, Translation\n",
    "    \n",
    "\n",
    "    if \"asset_ids\" in request_json:\n",
    "        assets = request_json.get('asset_ids') #comma separated assets\n",
    "    else:\n",
    "        assets=\"\"\n",
    "\n",
    "        \n",
    "    if \"persona\" in request_json:\n",
    "       persona = request_json.get('persona')   #persona filter\n",
    "    else:\n",
    "        persona=\"\"\n",
    "    \n",
    "    if action_type==\"Summary_Persona\" and persona==\"\":\n",
    "        return \"Error- Persona must be set\"  \n",
    "\n",
    "    if \"input_text\" in request_json: \n",
    "       input_text = request_json.get('input_text')   #input text for translation\n",
    "    else:\n",
    "        input_text=\"\"\n",
    "\n",
    "    if action_type==\"Translation\" and input_text==\"\":\n",
    "        return \"Error- Provide input text for translation\"\n",
    "\n",
    "    if \"language\" in request_json: \n",
    "       Language = request_json.get('language')   #input text for translation\n",
    "    else:\n",
    "        Language=\"Chineese\"\n",
    "\n",
    "    if action_type==\"Translation\" and Language==\"\":\n",
    "        return \"Error- Provide destination language for translation\"\n",
    "        \n",
    "          # Load configuration from config.json\n",
    "    with open('config.json') as config_file:\n",
    "         config = json.load(config_file)\n",
    "\n",
    "   # Set the Gemini 1.5 Pro context window limit\n",
    "    context_window_limit=int(config['context_window_limit']) \n",
    "    model_name=config['model_name'] \n",
    "\n",
    "    #Init vertex ai\n",
    "    vertexai.init(project=project_id, location=location )\n",
    "   \n",
    "\n",
    "    #set assets for search\n",
    "    if assets.strip() !=\"\":\n",
    "        assets= ','.join([ \"'\"+ id.strip()+\"'\" for id in assets.split(',')])\n",
    "    else:\n",
    "        assets=\"\"\n",
    "\n",
    "   #Set persona text    \n",
    "    persona_text=\"\"\n",
    "    if action_type==\"Summary_Persona\" and persona==\"\":\n",
    "        return \"Error- Please set the persona\"    \n",
    "    else:\n",
    "         persona_text=f\" so that a {persona} can understand it. Use simple words and short sentences\"\n",
    "\n",
    "    error=\"\" \n",
    "    start_time = time.time()     \n",
    "    #generate content according to the requested action\n",
    "    try:\n",
    "        if action_type!=\"Translation\":\n",
    "            #generate other content according to the requested action\n",
    "            vertex_llm_text = langchain_vertexai(model_name=model_name)\n",
    "           \n",
    "            if action_type==\"OffPlatformPost\": #for OffPlatformPost, create both Twitter and Instagram posts\n",
    "                result=\"Instagram Post:\\n\"+get_summary(assets =assets,action_type=action_type,platform='Instagram',\n",
    "                            persona_text=persona_text,project_id=project_id,context_window_limit=context_window_limit,model=vertex_llm_text)\n",
    "\n",
    "                \n",
    "                result=result+\"\\nTwitter Post:\\n\"+get_summary(assets =assets,action_type=action_type,platform='Twitter',\n",
    "                            persona_text=persona_text,project_id=project_id,context_window_limit=context_window_limit,model=vertex_llm_text)\n",
    "            \n",
    "            else:           \n",
    "                result=get_summary(assets =assets,action_type=action_type,platform=\"\",\n",
    "                            persona_text=persona_text,project_id=project_id,context_window_limit=context_window_limit,model=vertex_llm_text)\n",
    "        else:\n",
    "            #generate trannlation\n",
    "            generative_multimodal_model= GenerativeModel(model_name)\n",
    "            result=get_translation(action_type=action_type,input_text=input_text,Language=Language, model= generative_multimodal_model)\n",
    "    \n",
    "    except Exception as e:\n",
    "        result=\"Error- Service is not available or content generation has faced an issue:\\n\"+str(e)\n",
    "        error=result\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    #record the search log\n",
    "    log_data(result,error,request_json,elapsed_time,project_id)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ba953ede-84f9-468a-bc8a-8e713f25a5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unittest.mock import Mock\n",
    "import json\n",
    "\n",
    "\n",
    "#vlt_video_extract_SIXTY_MINUTES_60MI23_14_A_HBB.mp4\n",
    "data={\"asset_ids\":\"vlt_video_extract_NINE_NEWS_SYD-NINE_NNNT23_101_A.mp4\", \n",
    "\"project\":\"nine-quality-test\",\n",
    "\"region\":\"us-central1\",\n",
    "\"action_type\":\"TrailerScript\"  \n",
    "       }\n",
    " \n",
    "# Simulating an HTTP request with the mock object\n",
    "mock_request = Mock()\n",
    "mock_request.get_json.return_value = data  # Mock the get_json method to return your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87795546-ba28-4464-bef7-b2dc308a7835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    #set query string\n",
    "    assets=\"'p5d1x4','p5dx0h','p5duot','a4e57c915b48502be148d6fcb08944efa22d2107.jpeg','3466295218ab26efc1739b775d21453a5f1a819b.jpeg','03245688ac9a7dd651797170992af7d8421ae2c2.jpeg','vlt_video_extract_SIXTY_MINUTES_60MI23_10_A_HBB.mp4','vlt_video_extract_NINE_NEWS_SYD-NINE_NNNT23_101_A.mp4'\"\n",
    "    source_query_str= get_query_string(assets)\n",
    "    \n",
    "    #set prompts\n",
    "    question_prompt_template, refine_prompt_template=get_prompt(action_type='HeadLine',platform='',persona_text='')\n",
    "    \n",
    "    #set metadata and content columns\n",
    "    metadata_columns=[\"asset_id\"]\n",
    "    page_content_columns=[\"full_description\"]\n",
    "    \n",
    "    #load data from biqquery\n",
    "    documents,all_texts=get_data(source_query_str=source_query_str,metadata_columns=metadata_columns,page_content_columns=page_content_columns, project_id='nine-quality-test',return_text=True)\n",
    "    print(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694f635-e5bf-4d14-b8a5-2fd612b253f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_token_length = estimate_token_length(all_texts,'cl100k_base') #cl100k_base\n",
    "estimated_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babf95e-c97e-4b65-bb24-064e5acf0ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004da188-1173-454f-b815-f5d27d0e5174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    question_prompt_template, _=get_prompt(action_type=\"OffPlatformPost\",platform=\"Instagram\",input_text=all_texts,Language='')\n",
    "    generation_config= GenerationConfig(temperature=0.2, max_output_tokens=8192) \n",
    "    safety_settings=  {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "\n",
    "    p=f\"\"\" You will be given different parts of texts. Provide a summary of the following text. Your result must be detailed and at least 2 paragraphs. \n",
    "                When summarizing, directly dive into the narrative or descriptions from the text without using introductory phrases like 'In this passage'. \n",
    "                Directly address the main events, characters, and themes, encapsulating the essence and significant details from the text in a flowing narrative. \n",
    "                The goal is to present a unified view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\n",
    "                If different parts of texts look unrelevant, give a symmary of each text in 1 paragraph separately.\"\"\" \n",
    "   \n",
    "    model_input=[p+\" Texts are separated by ------------------------------\\n\"+all_texts]\n",
    "        \n",
    "    model= GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "    response = model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings, )\n",
    "    \n",
    "    result=\"\"\n",
    "    try:\n",
    "        result=response.text\n",
    "    except Exception as e:\n",
    "        result=str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cddaea-f80f-4f28-b464-f60ee67827c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff84648-9c06-434e-ab5b-c443e25523cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "799056f1-2f11-40f0-84c9-e56ebb238b25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**(Scene: Opens with rapid cuts of flashing police lights, crime scene tape, a burnt-out car, a tense close-up of a detective's face, and a grieving family member.)**\n",
      "\n",
      "**(Voiceover, urgent and dramatic):** In a city gripped by fear, a wave of violence explodes onto the streets...\n",
      "\n",
      "**(Scene: Aerial shot of Liverpool, then cuts to Bianca Balzer reporting from the scene of the shooting.)**\n",
      "\n",
      "**(Balzer):** A man, shot dead in broad daylight.  Multiple gunshot wounds...including to the head.\n",
      "\n",
      "**(Voiceover):** A shocking execution…two burnt-out cars…are they connected?\n",
      "\n",
      "**(Scene: Quick cuts of police swarming a crime scene, investigators examining evidence.)**\n",
      "\n",
      "**(Voiceover):**  Detectives race against time to untangle a web of deadly secrets...\n",
      "\n",
      "**(Scene: Karen Webb, Police Commissioner, in a press conference, looking stressed.)**\n",
      "\n",
      "**(Reporter's voice, sharp and accusatory):** How can you justify not watching the bodycam footage?\n",
      "\n",
      "**(Scene: Flashing image of a taser, then a hospital bed with a distraught family.)**\n",
      "\n",
      "**(Voiceover):**  A 95-year-old grandmother, tasered by police. The city cries out for justice.\n",
      "\n",
      "**(Scene: Majestic shot of Mount Everest, transitioning to footage of Jason Kennison climbing.)**\n",
      "\n",
      "**(Voiceover):** He defied the odds, conquering the world's highest peak…\n",
      "\n",
      "**(Scene: Kennison smiling at the summit, then a somber shot of his climbing gear.)**\n",
      "\n",
      "**(Voiceover):** ...but tragedy struck on the descent.\n",
      "\n",
      "**(Scene: Grainy security camera footage of a police officer aggressively arresting a teenager.)**\n",
      "\n",
      "**(Magistrate's voice, firm and disapproving):**  I simply do not see what the defendant says occurred.\n",
      "\n",
      "**(Voiceover):**  Abuse of power…corruption…who will protect us from those who are sworn to protect?\n",
      "\n",
      "**(Scene: Montage of chaotic scenes – trains delayed, a bus crash, a stabbing, interspersed with frustrated commuters.)**\n",
      "\n",
      "**(Voiceover):** Sydney's infrastructure crumbles.  Is the city on the brink of collapse?\n",
      "\n",
      "**(Scene: Brad Fittler, looking determined, in a pre-game interview.)**\n",
      "\n",
      "**(Fittler):** We're taking a gamble…but it's a gamble we have to take.\n",
      "\n",
      "**(Scene: Fast-paced action shots of a rugby game, culminating in a dramatic try.)**\n",
      "\n",
      "**(Voiceover):**  One city. Countless stories.  From the highest peaks to the darkest depths, Nine News brings you the truth.\n",
      "\n",
      "**(Scene:  Nine News logo, with the tagline: \"Tonight at 6.\")**\n",
      "\n",
      "**(Sound:  Dramatic sting, fading out.)**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=func_generate_content(mock_request)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b486b37-152d-4db9-b8e2-5ca5289b0ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
