{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "026458de-69fd-44c8-8da1-8086a0214a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_google_community import BigQueryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "#from langchain.llms import VertexAI as langchain_vertexai\n",
    "from langchain_google_vertexai import VertexAI as langchain_vertexai\n",
    "from langchain import PromptTemplate\n",
    "from pathlib import Path as p\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "\n",
    "vertex_llm_text = langchain_vertexai(model_name=\"gemini-1.5-pro-002\")\n",
    "\n",
    "\n",
    "def estimate_token_length(text, model=\"gpt2\"):\n",
    "    \"\"\"Estimates the token length of a given text using a specified model.\n",
    "\n",
    "      Args:\n",
    "        text: The input text.\n",
    "        model: The model to use for tokenization (default: \"gpt2\").\n",
    "\n",
    "      Returns:\n",
    "        The estimated number of tokens.\n",
    "      \"\"\"\n",
    "\n",
    "  \n",
    "    enc = tiktoken.get_encoding(model)  \n",
    "\n",
    "    # Tokenize the text and count tokens\n",
    "    tokens = enc.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    return token_count\n",
    "\n",
    "def get_data(source_query_str: str=None,metadata_columns: str=None,page_content_columns: str=None, project_id: str=None, return_only_text:bool=False):\n",
    "    \n",
    "    loader = BigQueryLoader(\n",
    "            query=source_query_str, project=project_id, metadata_columns=metadata_columns, page_content_columns=page_content_columns\n",
    "        )\n",
    "    documents = []\n",
    "    all_texts=[]\n",
    "    documents.extend(loader.load())\n",
    "    if return_only_text:  \n",
    "        all_texts=[doc.page_content.replace('description:',\"\",1) for doc in documents]\n",
    "        \n",
    "    return documents, '\\n'.join(all_texts)\n",
    "\n",
    " \n",
    "def summarize_docs(documents: list[object],question_prompt_template: str=\"\", refine_prompt_template: str=\"\" ,is_token_limit_exceeded: bool=False ):\n",
    "    \n",
    "    print(question_prompt_template)\n",
    "    print(refine_prompt_template)\n",
    "    question_prompt = PromptTemplate(template=question_prompt_template, input_variables=[\"text\"])    \n",
    "    if not is_token_limit_exceeded:        \n",
    "        #if the token limit is in the context window range, use a stuffing method for summary\n",
    "        chain = load_summarize_chain(vertex_llm_text, chain_type=\"stuff\", \n",
    "                                     prompt=question_prompt)\n",
    "\n",
    "    else:\n",
    "        print('you are here')   \n",
    "        refine_prompt = PromptTemplate(input_variables=[\"existing_answer\", \"text\"], template=refine_prompt_template)\n",
    "        print(refine_prompt_template)\n",
    "        print('*****')  \n",
    "        print(refine_prompt)\n",
    "        \n",
    "        # chain = load_summarize_chain(\n",
    "        #     vertex_llm_text,\n",
    "        #     chain_type=\"refine\",\n",
    "        #     question_prompt=question_prompt,\n",
    "        #     refine_prompt=refine_prompt,\n",
    "        #     return_intermediate_steps=False,\n",
    "        #   )\n",
    "        \n",
    "    return chain.invoke(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c848aa6f-839e-440b-9927-0e67ae92e070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Gemini 1.5 Pro context window limit\n",
    "context_window_limit = 200#2000000\n",
    "PROJECT_ID = \"nine-quality-test\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "source_query_str=\"select distinct combined_id,unique_id,content, chunk, trim(concat(ifnull(headline,''), CHR(10),  description)) as description from `nine-quality-test.vlt_media_embeddings_integration.vlt_all_media_content_text_embeddings` order by unique_id, chunk asc \"\n",
    "\n",
    "#this is the main prompt for summary\n",
    "question_prompt_template = \"\"\"\n",
    "    You will be given different parts of texts. Provide a summary of the following text. Your result must be detailed and at least 2 paragraphs. \n",
    "    When summarizing, directly dive into the narrative or descriptions from the text without using introductory phrases like 'In this passage'. \n",
    "    Directly address the main events, characters, and themes, encapsulating the essence and significant details from the text in a flowing narrative. \n",
    "    The goal is to present a unified view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\n",
    "\n",
    "    TEXT: {text}\n",
    "    SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "refine_prompt_template = (\n",
    "    \"Your job is to produce a final summary. Your task is to combine and refine these summaries into a final, comprehensive summary that covers all key events, characters, themes, and details.\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "metadata_columns=[\"combined_id\"]\n",
    "page_content_columns=[\"description\"]\n",
    "documents, all_texts=get_data(source_query_str=source_query_str,metadata_columns=metadata_columns,page_content_columns=page_content_columns, project_id=PROJECT_ID, return_only_text=True)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "93345630-2fc6-4fbb-866a-c5bf3feb61ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18027\n",
      "\n",
      "    You will be given different parts of texts. Provide a summary of the following text. Your result must be detailed and at least 2 paragraphs. \n",
      "    When summarizing, directly dive into the narrative or descriptions from the text without using introductory phrases like 'In this passage'. \n",
      "    Directly address the main events, characters, and themes, encapsulating the essence and significant details from the text in a flowing narrative. \n",
      "    The goal is to present a unified view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\n",
      "\n",
      "    TEXT: {text}\n",
      "    SUMMARY:\n",
      "\n",
      "\n",
      "you are here\n",
      "\n",
      "*****\n",
      "input_variables=[] input_types={} partial_variables={} template=''\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'chain' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m   message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour text is too long for the Gemini 1.5 Pro context window. We are trying to chunk and return the result.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m   is_token_limit_exceeded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m   summary\u001b[38;5;241m=\u001b[39m\u001b[43msummarize_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquestion_prompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_prompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43mis_token_limit_exceeded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_token_limit_exceeded\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m   message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour text fits within the Gemini 1.5 Pro context window.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[102], line 74\u001b[0m, in \u001b[0;36msummarize_docs\u001b[0;34m(documents, question_prompt_template, refine_prompt_template, is_token_limit_exceeded)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(refine_prompt)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# chain = load_summarize_chain(\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#     vertex_llm_text,\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m#     chain_type=\"refine\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#     return_intermediate_steps=False,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m#   )\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(documents)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'chain' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#documents,all_texts=get_data(source_query_str,metadata_columns,page_content_columns,PROJECT_ID,return_only_text=True)\n",
    "# Estimate the token length\n",
    "estimated_token_length = estimate_token_length(all_texts,'cl100k_base') #cl100k_base\n",
    "print(estimated_token_length)\n",
    "message=\"\"\n",
    "is_token_limit_exceeded=False\n",
    "if estimated_token_length > context_window_limit:\n",
    "  message=\"Your text is too long for the Gemini 1.5 Pro context window. We are trying to chunk and return the result.\"\n",
    "  is_token_limit_exceeded=True\n",
    "  summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded )\n",
    "else:\n",
    "  message=\"Your text fits within the Gemini 1.5 Pro context window.\"\n",
    "  summary=summarize_docs(documents=documents,question_prompt_template=question_prompt_template,refine_prompt_template=refine_prompt_template,is_token_limit_exceeded=is_token_limit_exceeded )\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32f3c618-2a05-42c1-bc70-57f21e02b666",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your job is to produce a final summary. Your task is to combine and refine these summaries into a final, comprehensive summary that covers all key events, characters, themes, and details.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary(only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summaryIf the context isn't useful, return the original summary.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4413bbd-1147-4a11-b1f8-954612291da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_prompt = PromptTemplate(template=question_prompt_template, input_variables=[\"text\"])    \n",
    "refine_prompt = PromptTemplate(input_variables=[\"existing_answer\", \"text\"], template=refine_prompt_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481ec0d-4955-4548-b227-024ba604bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1c415-608e-4f8e-8eac-9bafb5e4e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " sql = f\"\"\"  \n",
    "         WITH search_results AS\n",
    "         (\n",
    "              SELECT\n",
    "              search_results.base.content as content,  \n",
    "              search_results.base.combined_id as combined_id,\n",
    "              search_results.base.unique_id,\n",
    "              distance,  -- The computed distance (similarity score) between the embeddings\n",
    "              search_results.base.asset_id,\n",
    "              search_results.base.headline,\n",
    "              ifnull(search_results.base.html_safe_text,search_results.base.description) as description,\n",
    "              search_results.base.startOffset_seconds,\n",
    "              search_results.base.endOffset_seconds,\n",
    "              search_results.base.fileUri,\n",
    "              search_results.base.asset_type,\n",
    "              ROW_NUMBER() OVER (PARTITION BY  search_results.base.asset_id ORDER BY distance ASC) AS rank_within_document  -- Rank by distance within each document\n",
    "              \n",
    "            FROM\n",
    "              VECTOR_SEARCH(     \n",
    "                TABLE `{dataset}.{table}`, --source embedding table\n",
    "                '{source_embedding_column}',  -- Column with the embedding vectors in the base table\n",
    "\n",
    "                -- Use the query embedding computed in the previous step\n",
    "                 (SELECT {json.dumps(query_embedding)} query_embedding),  -- The query embedding from the CTE (query_embedding)\n",
    "\n",
    "                -- Return top-k closest matches (adjust k as necessary)\n",
    "                top_k =>{ top_k  }, -- Top k most similar matches based on distance\n",
    "                distance_type => 'COSINE',\n",
    "                options => {options}                   \n",
    "              ) search_results              \n",
    "          ),          \n",
    "\n",
    "             -- Step 2: Aggregate relevance per document (original_document_id)\n",
    "            ranked_documents AS (\n",
    "                SELECT\n",
    "                    asset_id,        \n",
    "                    MIN(distance) AS min_distance  -- Alternatively, you can use the average distance\n",
    "                FROM search_results\n",
    "                GROUP BY asset_id\n",
    "            )\n",
    "\n",
    "            -- Step 4: Retrieve the top-k ranked documents based on relevance\n",
    "            SELECT * FROM (\n",
    "              SELECT  \n",
    "                sr.asset_id,  \n",
    "                sr.headline,\n",
    "                sr.description,\n",
    "                sr.combined_id,\n",
    "                sr.unique_id,\n",
    "                sr.fileUri,\n",
    "                sr.asset_type,\n",
    "                sr.min_distance,\n",
    "                ROW_NUMBER() OVER (PARTITION BY SR.asset_id ORDER BY min_distance ASC) AS IDX,\n",
    "                STRING_AGG(CONCAT(\"\"\"+\"'{startOffset_seconds:', sr.startOffset_seconds, ',endOffset_seconds:', sr.endOffset_seconds, '}')\"\"\"+f\"\"\", \", \" ) \n",
    "                OVER (PARTITION BY sr.asset_id ORDER BY sr.startOffset_seconds) AS time_lines\n",
    "                --sr.distance,\n",
    "                --final_rank--,\n",
    "               -- rank_within_document\n",
    "            FROM search_results sr\n",
    "            JOIN ranked_documents rd ON sr.asset_id = rd.asset_id\n",
    "            WHERE rd.final_rank <= {top_k} -- Return the top-k documents based on chunk relevance      \n",
    "            --and sr.asset_id like '%00261507986b0faf31c775597d2d24beb4381e43%'\n",
    "            ORDER BY rd.final_rank, sr.rank_within_document  -- Order by document relevance and chunk rank\n",
    "            )\n",
    "            WHERE IDX=1\n",
    "    \"\"\"       \n",
    "    print(sql)\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "  \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(sql)\n",
    "\n",
    "    # Fetch results\n",
    "    results = query_job.result()\n",
    "    \n",
    "    output=[]\n",
    "    for row in results:\n",
    "        output.append({'asset_id':row['asset_id'], 'headline':row['headline'],'description':row['description'],'fileUri':row['fileUri'], \"time_lines\":row['time_lines'], \"asset_type\":row[\"asset_type\"]})\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
