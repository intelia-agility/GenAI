{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2d6134-e252-4faf-a9f4-d07da15224dd",
   "metadata": {},
   "source": [
    "### OnlineServing\n",
    "This is developed according to \n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/online_feature_serving_and_vector_retrieval_bigquery_data_with_feature_store.ipynb\n",
    "\n",
    "We use FeatureStore as it is the best approach for profduction ready development and online servings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522d76bd-6dbe-4fde-a946-7dfa7ede41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 09:01:10.870650: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-20 09:01:12.608335: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-08-20 09:01:12.608513: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-08-20 09:01:12.608525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform_v1.types import NearestNeighborQuery\n",
    "from vertexai.resources.preview import (FeatureOnlineStore, FeatureView,\n",
    "                                        FeatureViewBigQuerySource)\n",
    "from vertexai.resources.preview.feature_store import utils\n",
    "\n",
    "#set project info\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-central1\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fbac03-c22e-4966-b88e-507e9c1b8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_ONLINE_STORE_ID = \"nine_quality_test_multimodal_featurestore\"  # @param {type: \"string\"}\n",
    "FEATURE_VIEW_ID = \"feature_view_nine_quality_test\"  # @param {type: \"string\"}\n",
    "\n",
    "nine_fs=FeatureOnlineStore(FEATURE_ONLINE_STORE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6355f92-5e2c-47b2-a4d4-ef0fb5981c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify that the FeatureView instance is created by getting the feature view.\n",
    "nine_fv=FeatureView(\n",
    "    FEATURE_VIEW_ID, feature_online_store_id=FEATURE_ONLINE_STORE_ID\n",
    ") \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b40ee2f-f396-4abc-a120-57299d5744a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public endpoint for the optimized online store nine_quality_test_multimodal_featurestore is 6871349539274489856.us-central1-494586852359.featurestore.vertexai.goog\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DIMENSIONS = 1408\n",
    "EMBEDDINGS = [1] * DIMENSIONS\n",
    "\n",
    "result=nine_fv.search(\n",
    "    embedding_value=EMBEDDINGS,\n",
    "    neighbor_count=10,\n",
    "    #string_filters=[country_filter],#for multimodal embedding this can be set to None, unless having a description column\n",
    "    return_full_entity=True,  # returning entities with metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb92ff2b-b014-4cfb-941b-d1c966d6177a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result=result.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd817cf3-b7d4-4b61-98bb-ab7b73660971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nearest_neighbours=[]\n",
    "for neighbour in result['neighbors']:\n",
    "    nearest_neighbour={}\n",
    "    nearest_neighbour['entity_id']=neighbour['entity_id']\n",
    "    nearest_neighbour['distance']=neighbour['distance']\n",
    "  \n",
    "    for feature in neighbour['entity_key_values']['key_values']['features']:\n",
    "        if 'value' in feature:\n",
    "            if type(list(feature['value'].values())[0]) is dict:\n",
    "                nearest_neighbour[feature['name']]=[]#list(list(feature['value'].values())[0].values())[0]             \n",
    "            else:\n",
    "                nearest_neighbour[feature['name']]=list(feature['value'].values())[0]             \n",
    "        else :\n",
    "            nearest_neighbour[feature['name']]=None\n",
    "   \n",
    "    nearest_neighbours.append(nearest_neighbour)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c1d1d0-ada4-4161-8d38-102a600e08a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_id': 'lf2aujik.png',\n",
       "  'distance': 2.895124912261963,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/lf2aujik.png'},\n",
       " {'entity_id': 'pnll2ztl.png',\n",
       "  'distance': 2.9542393684387207,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/pnll2ztl.png'},\n",
       " {'entity_id': 'sample3.png',\n",
       "  'distance': 2.998533248901367,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/sample3.png'},\n",
       " {'entity_id': 'sample2.png',\n",
       "  'distance': 3.0093295574188232,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/sample2.png'},\n",
       " {'entity_id': '816f62dz.png',\n",
       "  'distance': 3.010995626449585,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/816f62dz.png'},\n",
       " {'entity_id': '0c7mq0hk.png',\n",
       "  'distance': 3.0795702934265137,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/0c7mq0hk.png'},\n",
       " {'entity_id': 'ka7ggrz8.png',\n",
       "  'distance': 3.1104907989501953,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/ka7ggrz8.png'},\n",
       " {'entity_id': 'v5c0kpyo.png',\n",
       "  'distance': 3.1119742393493652,\n",
       "  'end_offset_sec_embedding': None,\n",
       "  'media_type': 'image',\n",
       "  'start_offset_sec_embedding': None,\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'SampleImage/v5c0kpyo.png'},\n",
       " {'entity_id': '60MI24_1_A_HBB.mp4|2088|2104',\n",
       "  'distance': 3.12129282951355,\n",
       "  'end_offset_sec_embedding': '2104',\n",
       "  'media_type': 'video',\n",
       "  'start_offset_sec_embedding': '2088',\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'gs://raw_nine_files/60MI24_1_A_HBB.mp4'},\n",
       " {'entity_id': '60MI24_1_A_HBB.mp4|2376|2392',\n",
       "  'distance': 3.1779329776763916,\n",
       "  'end_offset_sec_embedding': '2392',\n",
       "  'media_type': 'video',\n",
       "  'start_offset_sec_embedding': '2376',\n",
       "  'multimodal_embedding': [],\n",
       "  'path': 'gs://raw_nine_files/60MI24_1_A_HBB.mp4'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e1c86-65e5-47cc-9c0e-b6909d1a51dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
